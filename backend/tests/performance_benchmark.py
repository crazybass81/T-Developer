#!/usr/bin/env python3
"""
Performance Benchmark System for T-Developer MVP
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ìµœì í™” ì‹œìŠ¤í…œ
"""

import asyncio
import httpx
import time
import statistics
import json
import logging
import tempfile
import zipfile
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import psutil
import threading
import sys
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í…ŒìŠ¤íŠ¸ ì„¤ì •
BASE_URL = "http://localhost:8000"
TIMEOUT = 120  # 2ë¶„ íƒ€ì„ì•„ì›ƒ


@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""

    generation_time: float
    response_time: float
    download_time: float
    total_time: float
    memory_usage: Dict[str, float]
    cpu_usage: float
    file_count: int
    zip_size_mb: float
    success: bool
    error_message: Optional[str] = None


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""

    scenario: str
    metrics: List[PerformanceMetrics]
    average_generation_time: float
    median_generation_time: float
    success_rate: float
    performance_target_met: bool
    recommendations: List[str]


class SystemMonitor:
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""

    def __init__(self):
        self.monitoring = False
        self.metrics = []
        self.monitor_thread = None

    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitoring = True
        self.metrics = []
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()

    def stop_monitoring(self) -> Dict[str, float]:
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ ë° ê²°ê³¼ ë°˜í™˜"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()

        if not self.metrics:
            return {"memory_mb": 0, "cpu_percent": 0}

        memory_values = [m["memory_mb"] for m in self.metrics]
        cpu_values = [m["cpu_percent"] for m in self.metrics]

        return {
            "memory_mb": {
                "avg": statistics.mean(memory_values),
                "max": max(memory_values),
                "min": min(memory_values),
            },
            "cpu_percent": {
                "avg": statistics.mean(cpu_values),
                "max": max(cpu_values),
                "min": min(cpu_values),
            },
        }

    def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring:
            try:
                # í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                process = psutil.Process(os.getpid())
                memory_info = process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024

                # CPU ì‚¬ìš©ë¥ 
                cpu_percent = process.cpu_percent()

                self.metrics.append(
                    {
                        "timestamp": time.time(),
                        "memory_mb": memory_mb,
                        "cpu_percent": cpu_percent,
                    }
                )

                time.sleep(1)  # 1ì´ˆë§ˆë‹¤ ì¸¡ì •
            except Exception as e:
                logger.warning(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(1)


class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ê¸°"""

    def __init__(self, base_url: str = BASE_URL):
        self.base_url = base_url
        self.client = httpx.AsyncClient(timeout=TIMEOUT)
        self.system_monitor = SystemMonitor()

    async def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""

        print("ğŸš€ T-Developer Performance Benchmark Starting...")
        print("=" * 60)

        # ë²¤ì¹˜ë§ˆí¬ ì‹œë‚˜ë¦¬ì˜¤
        scenarios = [
            ("Simple React App", self._benchmark_simple_react, 3),
            ("Todo Application", self._benchmark_todo_app, 3),
            ("Blog Website", self._benchmark_blog_website, 2),
            ("Dashboard App", self._benchmark_dashboard, 2),
            ("Complex E-commerce", self._benchmark_ecommerce, 1),
        ]

        benchmark_results = []
        overall_start_time = time.time()

        for scenario_name, benchmark_func, run_count in scenarios:
            print(f"\nğŸ“Š Running: {scenario_name} ({run_count} runs)")

            try:
                result = await self._run_scenario_benchmark(
                    scenario_name, benchmark_func, run_count
                )
                benchmark_results.append(result)

                # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
                print(f"âœ… {scenario_name}:")
                print(f"   í‰ê·  ìƒì„±ì‹œê°„: {result.average_generation_time:.2f}ì´ˆ")
                print(f"   ì„±ê³µë¥ : {result.success_rate:.1f}%")
                print(f"   ëª©í‘œë‹¬ì„±: {'âœ…' if result.performance_target_met else 'âŒ'}")

            except Exception as e:
                print(f"âŒ {scenario_name}: ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨ - {e}")
                logger.error(
                    f"Benchmark failed for {scenario_name}: {e}", exc_info=True
                )

        total_benchmark_time = time.time() - overall_start_time

        # ì¢…í•© ê²°ê³¼ ë¶„ì„
        overall_results = self._analyze_overall_performance(benchmark_results)
        overall_results["total_benchmark_time"] = total_benchmark_time
        overall_results["benchmark_results"] = [
            {
                "scenario": r.scenario,
                "avg_time": r.average_generation_time,
                "success_rate": r.success_rate,
                "target_met": r.performance_target_met,
                "recommendations": r.recommendations,
            }
            for r in benchmark_results
        ]

        # ê²°ê³¼ ì¶œë ¥
        self._print_benchmark_summary(overall_results)

        return overall_results

    async def _run_scenario_benchmark(
        self, scenario_name: str, benchmark_func, run_count: int
    ) -> BenchmarkResult:
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""

        metrics_list = []

        for run_num in range(run_count):
            print(f"   Run {run_num + 1}/{run_count}...", end=" ")

            try:
                metrics = await benchmark_func()
                metrics_list.append(metrics)

                status = "âœ…" if metrics.success else "âŒ"
                print(f"{status} {metrics.total_time:.2f}s")

            except Exception as e:
                error_metrics = PerformanceMetrics(
                    generation_time=0,
                    response_time=0,
                    download_time=0,
                    total_time=0,
                    memory_usage={"memory_mb": {"avg": 0}},
                    cpu_usage=0,
                    file_count=0,
                    zip_size_mb=0,
                    success=False,
                    error_message=str(e),
                )
                metrics_list.append(error_metrics)
                print(f"âŒ ERROR: {str(e)[:50]}...")

        return self._calculate_scenario_results(scenario_name, metrics_list)

    async def _benchmark_simple_react(self) -> PerformanceMetrics:
        """ê°„ë‹¨í•œ React ì•± ë²¤ì¹˜ë§ˆí¬"""
        return await self._benchmark_project_generation(
            {
                "user_input": "Create a simple React app with modern design",
                "project_type": "react",
                "features": [],
            }
        )

    async def _benchmark_todo_app(self) -> PerformanceMetrics:
        """Todo ì•± ë²¤ì¹˜ë§ˆí¬"""
        return await self._benchmark_project_generation(
            {
                "user_input": "Create a React todo application with add, edit, delete functionality",
                "project_type": "react",
                "features": ["todo"],
            }
        )

    async def _benchmark_blog_website(self) -> PerformanceMetrics:
        """ë¸”ë¡œê·¸ ì›¹ì‚¬ì´íŠ¸ ë²¤ì¹˜ë§ˆí¬"""
        return await self._benchmark_project_generation(
            {
                "user_input": "Create a blog website with React and routing for multiple pages",
                "project_type": "react",
                "features": ["routing", "blog"],
            }
        )

    async def _benchmark_dashboard(self) -> PerformanceMetrics:
        """ëŒ€ì‹œë³´ë“œ ë²¤ì¹˜ë§ˆí¬"""
        return await self._benchmark_project_generation(
            {
                "user_input": "Create an admin dashboard with charts and data visualization",
                "project_type": "react",
                "features": ["dashboard", "charts"],
            }
        )

    async def _benchmark_ecommerce(self) -> PerformanceMetrics:
        """ì´ì»¤ë¨¸ìŠ¤ ë²¤ì¹˜ë§ˆí¬"""
        return await self._benchmark_project_generation(
            {
                "user_input": "Create a complete e-commerce website with React, routing, state management, and shopping cart",
                "project_type": "react",
                "features": ["routing", "state-management", "ecommerce", "cart"],
            }
        )

    async def _benchmark_project_generation(
        self, payload: Dict[str, Any]
    ) -> PerformanceMetrics:
        """í”„ë¡œì íŠ¸ ìƒì„± ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""

        # ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.system_monitor.start_monitoring()

        total_start_time = time.time()

        try:
            # 1. í”„ë¡œì íŠ¸ ìƒì„± ìš”ì²­
            generation_start_time = time.time()

            response = await self.client.post(
                f"{self.base_url}/api/v1/generate", json=payload
            )

            generation_time = time.time() - generation_start_time
            response_time = generation_time

            if response.status_code != 200:
                raise Exception(f"Generation failed: {response.status_code}")

            data = response.json()

            if not data.get("success"):
                raise Exception(f"Generation unsuccessful: {data.get('message')}")

            project_id = data.get("project_id")
            download_url = data.get("download_url")

            # 2. ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸
            download_start_time = time.time()

            download_response = await self.client.get(f"{self.base_url}{download_url}")

            download_time = time.time() - download_start_time

            if download_response.status_code != 200:
                raise Exception(f"Download failed: {download_response.status_code}")

            # 3. ZIP íŒŒì¼ ê²€ì¦
            with tempfile.NamedTemporaryFile(suffix=".zip", delete=False) as temp_zip:
                temp_zip.write(download_response.content)
                temp_zip_path = temp_zip.name

            try:
                file_count, zip_size_mb = self._analyze_zip_file(temp_zip_path)
            finally:
                os.unlink(temp_zip_path)

            total_time = time.time() - total_start_time

            # ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            system_metrics = self.system_monitor.stop_monitoring()

            return PerformanceMetrics(
                generation_time=generation_time,
                response_time=response_time,
                download_time=download_time,
                total_time=total_time,
                memory_usage=system_metrics,
                cpu_usage=system_metrics.get("cpu_percent", {}).get("avg", 0),
                file_count=file_count,
                zip_size_mb=zip_size_mb,
                success=True,
            )

        except Exception as e:
            total_time = time.time() - total_start_time
            system_metrics = self.system_monitor.stop_monitoring()

            return PerformanceMetrics(
                generation_time=0,
                response_time=0,
                download_time=0,
                total_time=total_time,
                memory_usage=system_metrics,
                cpu_usage=system_metrics.get("cpu_percent", {}).get("avg", 0),
                file_count=0,
                zip_size_mb=0,
                success=False,
                error_message=str(e),
            )

    def _analyze_zip_file(self, zip_path: str) -> Tuple[int, float]:
        """ZIP íŒŒì¼ ë¶„ì„"""
        try:
            with zipfile.ZipFile(zip_path, "r") as zipf:
                file_count = len([f for f in zipf.filelist if not f.is_dir()])
                zip_size_mb = os.path.getsize(zip_path) / (1024 * 1024)
                return file_count, zip_size_mb
        except Exception:
            return 0, 0

    def _calculate_scenario_results(
        self, scenario_name: str, metrics_list: List[PerformanceMetrics]
    ) -> BenchmarkResult:
        """ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ ê³„ì‚°"""

        successful_metrics = [m for m in metrics_list if m.success]
        success_rate = (len(successful_metrics) / len(metrics_list)) * 100

        if not successful_metrics:
            return BenchmarkResult(
                scenario=scenario_name,
                metrics=metrics_list,
                average_generation_time=0,
                median_generation_time=0,
                success_rate=success_rate,
                performance_target_met=False,
                recommendations=["ëª¨ë“  ì‹¤í–‰ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."],
            )

        generation_times = [m.generation_time for m in successful_metrics]
        average_generation_time = statistics.mean(generation_times)
        median_generation_time = statistics.median(generation_times)

        # ì„±ëŠ¥ ëª©í‘œ: 30ì´ˆ ì´ë‚´
        performance_target = 30.0
        performance_target_met = average_generation_time <= performance_target

        # ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(
            scenario_name,
            successful_metrics,
            average_generation_time,
            performance_target,
        )

        return BenchmarkResult(
            scenario=scenario_name,
            metrics=metrics_list,
            average_generation_time=average_generation_time,
            median_generation_time=median_generation_time,
            success_rate=success_rate,
            performance_target_met=performance_target_met,
            recommendations=recommendations,
        )

    def _generate_recommendations(
        self,
        scenario: str,
        metrics: List[PerformanceMetrics],
        avg_time: float,
        target: float,
    ) -> List[str]:
        """ì„±ëŠ¥ ê°œì„  ì¶”ì²œì‚¬í•­ ìƒì„±"""

        recommendations = []

        # ì„±ëŠ¥ ëª©í‘œ ë¯¸ë‹¬ì„± ì‹œ
        if avg_time > target:
            recommendations.append(f"í‰ê·  ìƒì„±ì‹œê°„ì´ ëª©í‘œ({target}ì´ˆ)ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤ ({avg_time:.2f}ì´ˆ)")

            if avg_time > target * 1.5:
                recommendations.append("ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜ - Agent ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” í•„ìš”")

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
            avg_memory = statistics.mean(
                [m.memory_usage.get("memory_mb", {}).get("avg", 0) for m in metrics]
            )

            if avg_memory > 500:  # 500MB ì´ˆê³¼
                recommendations.append(
                    f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤ ({avg_memory:.1f}MB) - ë©”ëª¨ë¦¬ ìµœì í™” í•„ìš”"
                )

            # CPU ì‚¬ìš©ë¥  ë¶„ì„
            avg_cpu = statistics.mean([m.cpu_usage for m in metrics])
            if avg_cpu > 80:
                recommendations.append(f"CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤ ({avg_cpu:.1f}%) - ì²˜ë¦¬ ë¡œì§ ìµœì í™” í•„ìš”")
        else:
            recommendations.append(f"ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±! (í‰ê·  {avg_time:.2f}ì´ˆ)")

        # íŒŒì¼ í¬ê¸° ë¶„ì„
        avg_zip_size = statistics.mean([m.zip_size_mb for m in metrics])
        if avg_zip_size > 10:
            recommendations.append(
                f"ZIP íŒŒì¼ì´ í½ë‹ˆë‹¤ ({avg_zip_size:.1f}MB) - ë¶ˆí•„ìš”í•œ íŒŒì¼ ì œê±° ê²€í† "
            )

        return recommendations

    def _analyze_overall_performance(
        self, results: List[BenchmarkResult]
    ) -> Dict[str, Any]:
        """ì „ì²´ ì„±ëŠ¥ ë¶„ì„"""

        if not results:
            return {"overall_grade": "F", "performance_met": False}

        successful_results = [r for r in results if r.success_rate > 0]

        if not successful_results:
            return {
                "overall_grade": "F",
                "performance_met": False,
                "summary": "ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨",
            }

        # ì „ì²´ í‰ê·  ì„±ëŠ¥
        avg_times = [r.average_generation_time for r in successful_results]
        overall_avg_time = statistics.mean(avg_times)

        # ì„±ê³µë¥ 
        success_rates = [r.success_rate for r in results]
        overall_success_rate = statistics.mean(success_rates)

        # ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±ë¥ 
        target_met_count = sum(
            1 for r in successful_results if r.performance_target_met
        )
        target_met_rate = (target_met_count / len(successful_results)) * 100

        # ë“±ê¸‰ ê³„ì‚°
        grade = self._calculate_performance_grade(
            overall_avg_time, overall_success_rate, target_met_rate
        )

        return {
            "overall_grade": grade,
            "overall_avg_time": overall_avg_time,
            "overall_success_rate": overall_success_rate,
            "target_met_rate": target_met_rate,
            "performance_met": grade in ["A", "B"],
            "summary": f"í‰ê·  {overall_avg_time:.1f}ì´ˆ, ì„±ê³µë¥  {overall_success_rate:.1f}%, ëª©í‘œë‹¬ì„±ë¥  {target_met_rate:.1f}%",
        }

    def _calculate_performance_grade(
        self, avg_time: float, success_rate: float, target_met_rate: float
    ) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°"""

        # Aë“±ê¸‰: í‰ê·  20ì´ˆ ì´ë‚´, ì„±ê³µë¥  95% ì´ìƒ, ëª©í‘œë‹¬ì„±ë¥  80% ì´ìƒ
        if avg_time <= 20 and success_rate >= 95 and target_met_rate >= 80:
            return "A"

        # Bë“±ê¸‰: í‰ê·  30ì´ˆ ì´ë‚´, ì„±ê³µë¥  90% ì´ìƒ, ëª©í‘œë‹¬ì„±ë¥  60% ì´ìƒ
        elif avg_time <= 30 and success_rate >= 90 and target_met_rate >= 60:
            return "B"

        # Cë“±ê¸‰: í‰ê·  45ì´ˆ ì´ë‚´, ì„±ê³µë¥  80% ì´ìƒ
        elif avg_time <= 45 and success_rate >= 80:
            return "C"

        # Dë“±ê¸‰: í‰ê·  60ì´ˆ ì´ë‚´, ì„±ê³µë¥  70% ì´ìƒ
        elif avg_time <= 60 and success_rate >= 70:
            return "D"

        # Fë“±ê¸‰: ê·¸ ì™¸
        else:
            return "F"

    def _print_benchmark_summary(self, results: Dict[str, Any]):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""

        print("\n" + "=" * 60)
        print("ğŸ“Š PERFORMANCE BENCHMARK SUMMARY")
        print("=" * 60)

        print(f"Overall Grade: {results['overall_grade']}")
        print(
            f"Performance Target Met: {'âœ… YES' if results['performance_met'] else 'âŒ NO'}"
        )
        print(f"{results['summary']}")

        print("\nğŸ¯ Scenario Results:")
        for scenario in results["benchmark_results"]:
            status = "âœ…" if scenario["target_met"] else "âŒ"
            print(
                f"  {status} {scenario['scenario']}: {scenario['avg_time']:.2f}s ({scenario['success_rate']:.1f}%)"
            )

        print(f"\nâ±ï¸  Total Benchmark Time: {results['total_benchmark_time']:.2f}s")

        # ì¶”ì²œì‚¬í•­ ì¶œë ¥
        print("\nğŸ’¡ Performance Recommendations:")
        all_recommendations = set()
        for scenario in results["benchmark_results"]:
            all_recommendations.update(scenario["recommendations"])

        for i, rec in enumerate(sorted(all_recommendations), 1):
            print(f"  {i}. {rec}")

        print("\n" + "=" * 60)

    async def save_benchmark_report(
        self, results: Dict[str, Any], output_path: Optional[str] = None
    ) -> str:
        """ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ ì €ì¥"""

        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"performance_benchmark_report_{timestamp}.json"

        report_data = {
            "benchmark_suite": "T-Developer MVP Performance Benchmark",
            "execution_time": datetime.now().isoformat(),
            "system_info": {"python_version": sys.version, "platform": sys.platform},
            "results": results,
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, default=str, ensure_ascii=False)

        print(f"ğŸ“‹ Benchmark report saved to: {output_path}")
        return output_path

    async def cleanup(self):
        """ì •ë¦¬"""
        await self.client.aclose()


async def run_performance_benchmark():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë©”ì¸ í•¨ìˆ˜"""

    benchmark = PerformanceBenchmark()

    try:
        # ì„œë²„ ìƒíƒœ í™•ì¸
        try:
            response = await benchmark.client.get(f"{BASE_URL}/health")
            if response.status_code != 200:
                raise Exception("ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            print("âœ… Server health check passed")
        except Exception as e:
            print(f"âŒ Server not available: {e}")
            return 1

        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        results = await benchmark.run_comprehensive_benchmark()

        # ë³´ê³ ì„œ ì €ì¥
        await benchmark.save_benchmark_report(results)

        # ê²°ê³¼ì— ë”°ë¥¸ ì¢…ë£Œ ì½”ë“œ
        if results["performance_met"]:
            print("\nğŸ‰ Performance benchmark passed!")
            return 0
        else:
            print(
                f"\nğŸ’¥ Performance benchmark failed (Grade: {results['overall_grade']})!"
            )
            return 1

    except Exception as e:
        print(f"\nâŒ Benchmark execution failed: {e}")
        logger.error("Benchmark failed", exc_info=True)
        return 1
    finally:
        await benchmark.cleanup()


if __name__ == "__main__":
    exit_code = asyncio.run(run_performance_benchmark())
    sys.exit(exit_code)
