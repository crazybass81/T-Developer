"""Upgrade Orchestrator for comprehensive system analysis and upgrade.

This orchestrator coordinates all analysis agents to provide complete
system insights and generate upgrade recommendations.
"""

from __future__ import annotations

import asyncio
import logging
import json
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, field, asdict
from datetime import datetime
import os
import sys
from pathlib import Path

# No need to add path - we're in the right location now

from backend.packages.agents.base import AgentTask, AgentResult, TaskStatus
from backend.packages.agents.ai_providers import get_ai_provider, BedrockAIProvider
from backend.packages.agents.requirement_analyzer import RequirementAnalyzer
from backend.packages.agents.static_analyzer import StaticAnalyzer
from backend.packages.agents.code_analysis import CodeAnalysisAgent
from backend.packages.agents.gap_analyzer import GapAnalyzer
from backend.packages.agents.behavior_analyzer import BehaviorAnalyzer
from backend.packages.agents.impact_analyzer import ImpactAnalyzer
from backend.packages.agents.code_generator import CodeGenerator
from backend.packages.agents.quality_gate import QualityGate
# from backend.packages.agents.report_generator import ReportGenerator  # Not implemented yet
from backend.packages.agents.external_researcher import ExternalResearcher
from backend.packages.agents.planner_agent import PlannerAgent
from backend.packages.agents.task_creator_agent import TaskCreatorAgent
from backend.packages.agents.system_architect import SystemArchitect
from backend.packages.agents.orchestrator_designer import OrchestratorDesigner
from backend.packages.memory.hub import MemoryHub
from backend.packages.memory.contexts import ContextType
from backend.packages.memory.document_context import SharedDocumentContext
from backend.packages.safety import CircuitBreaker, CircuitBreakerConfig, ResourceLimiter, ResourceLimit

# Agno í†µí•© - ìë™ ì—ì´ì „íŠ¸ ìƒì„±
from backend.packages.agno import AgnoManager
from backend.packages.agno.spec import AgentSpec as AgnoSpec
from backend.packages.agno.generator import CodeGenerator as AgnoCodeGenerator

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


@dataclass
class UpgradeConfig:
    """Configuration for upgrade analysis."""
    
    project_path: str
    output_dir: str = "/tmp/t-developer/reports"  # ë¬¸ì„œ ì €ì¥ ê²½ë¡œ
    enable_dynamic_analysis: bool = False  # Enable code execution
    include_behavior_analysis: bool = True  # Parse logs if available
    generate_impact_matrix: bool = True  # Generate dependency matrix
    generate_recommendations: bool = True  # Generate upgrade recommendations
    safe_mode: bool = True  # Run in safe mode
    max_execution_time: int = 600  # 10 minutes max
    parallel_analysis: bool = True  # Run analyses in parallel
    
    # Evolution Loop ì„¤ì •
    enable_evolution_loop: bool = False  # Evolution Loop í™œì„±í™”
    max_evolution_iterations: int = 10  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
    auto_generate_agents: bool = False  # Agnoë¥¼ í†µí•œ ìë™ ì—ì´ì „íŠ¸ ìƒì„±
    auto_implement_code: bool = False  # CodeGeneratorë¥¼ í†µí•œ ìë™ ì½”ë“œ êµ¬í˜„
    evolution_convergence_threshold: float = 0.95  # ìˆ˜ë ´ ì„ê³„ê°’ (ê°­ í•´ì†Œìœ¨)
    
    # AI ë“œë¦¬ë¸ ë™ì  ì›Œí¬í”Œë¡œìš° ì„¤ì •
    ai_driven_workflow: bool = True  # AIê°€ ì—ì´ì „íŠ¸ ì‹¤í–‰ ìˆœì„œ ê²°ì •
    allow_parallel_execution: bool = True  # ë³‘ë ¬ ì‹¤í–‰ í—ˆìš©


@dataclass
class AnalysisPhase:
    """Represents a phase of analysis."""
    
    name: str
    agent: str
    status: str  # pending, running, completed, failed
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None


@dataclass
class EvolutionGoalSpec:
    """ì§„í™” ëª©í‘œ ëª…ì„¸ì„œ."""
    
    background: str  # ë°°ê²½/ëª©ì 
    stakeholders: List[str]  # ì´í•´ê´€ê³„ì
    change_scope: Dict[str, Any]  # ë³€ê²½ ë²”ìœ„
    success_criteria: Dict[str, Any]  # ì„±ê³µ ê¸°ì¤€
    constraints: Dict[str, Any]  # ì œì•½ì‚¬í•­
    compatibility_requirements: List[str]  # í˜¸í™˜ì„± ìš”êµ¬ì‚¬í•­
    open_questions: List[str]  # ì—´ë¦° ì§ˆë¬¸
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class CurrentStateReport:
    """í˜„ì¬ ìƒíƒœ ì§„ë‹¨ ë³´ê³ ì„œ."""
    
    static_analysis: Dict[str, Any]  # ì •ì  ë¶„ì„
    dynamic_analysis: Dict[str, Any]  # ë™ì  ë¶„ì„
    ai_summary: Dict[str, Any]  # AI ìš”ì•½
    contracts: Dict[str, Any]  # ê³„ì•½/ì¸í„°í˜ì´ìŠ¤
    test_gaps: List[Dict[str, Any]]  # í…ŒìŠ¤íŠ¸ ê°­
    ux_metrics: Dict[str, Any]  # UX/í–‰ë™ ë°ì´í„°
    

@dataclass  
class GapReport:
    """ê°­ ë¶„ì„ ë³´ê³ ì„œ."""
    
    gaps: List[Dict[str, Any]]  # í˜„ì¬â†”ëª©í‘œ ì°¨ì´
    impact_matrix: Dict[str, Any]  # ì˜í–¥ë„ ë§¤íŠ¸ë¦­ìŠ¤
    risk_scores: Dict[str, float]  # ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´
    migration_plan: Dict[str, Any]  # ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ
    batch_plan: List[Dict[str, Any]]  # ì‘ì€ ë°°ì¹˜ ê³„íš


@dataclass
class UpgradeResearchPack:
    """ì—…ê·¸ë ˆì´ë“œ ë¦¬ì„œì¹˜ íŒ© (URP).
    
    upgrade.md ë¬¸ì„œì˜ 4A) ì„¹ì…˜ì— ì •ì˜ëœ êµ¬ì¡°ë¥¼ ë”°ë¦„.
    """
    
    one_line_conclusion: str  # í•œ ì¤„ ê²°ë¡ 
    recommended_approach: Dict[str, Any]  # ì¶”ì²œ ì ‘ê·¼ 1ì•ˆ
    alternative_approaches: List[Dict[str, Any]]  # ëŒ€ì•ˆ 2ì•ˆ
    compatibility_checklist: List[Dict[str, Any]]  # í˜¸í™˜ì„±/ê³„ì•½ ì˜í–¥ ì²´í¬ë¦¬ìŠ¤íŠ¸
    migration_strategy: Dict[str, Any]  # ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ ìš”ì•½
    code_snippets: List[Dict[str, Any]]  # í•µì‹¬ ì½”ë“œ ìŠ¤ë‹ˆí« 3ê°œ
    warnings: List[str]  # ì£¼ì˜ì‚¬í•­/í•¨ì •
    success_criteria: Dict[str, Any]  # ì„±ê³µ/ì‹¤íŒ¨ ê¸°ì¤€
    cost_risk_summary: Dict[str, Any]  # ë¹„ìš©/ë¦¬ìŠ¤í¬ ìš”ì•½
    dedup_results: Dict[str, Any]  # De-dup ê²°ê³¼
    references: List[str]  # ì°¸ê³ ìë£Œ ëª©ë¡
    ttl_days: int = 14  # ìœ íš¨ê¸°ê°„
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class EvolutionResult:
    """Evolution Loop ì‹¤í–‰ ê²°ê³¼."""
    
    success: bool  # ëª¨ë“  ê°­ì´ í•´ì†Œë˜ì—ˆëŠ”ì§€
    iterations: int  # ì‹¤í–‰ëœ ë°˜ë³µ íšŸìˆ˜
    final_gaps: List[Dict[str, Any]]  # ìµœì¢… ë‚¨ì€ ê°­
    agents_created: List[str]  # ìƒì„±ëœ ì—ì´ì „íŠ¸ ëª©ë¡
    code_generated: int  # ìƒì„±ëœ ì½”ë“œ ë¼ì¸ ìˆ˜
    tests_passed: int  # í†µê³¼í•œ í…ŒìŠ¤íŠ¸ ìˆ˜
    tests_failed: int  # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìˆ˜
    evolution_history: List[Dict[str, Any]]  # ê° ë°˜ë³µì˜ ìƒì„¸ ì´ë ¥
    convergence_rate: float  # ìˆ˜ë ´ë¥ 
    total_time: float  # ì´ ì‹¤í–‰ ì‹œê°„
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class UpgradeReport:
    """Comprehensive upgrade analysis report."""
    
    timestamp: str
    project_path: str
    
    # 1) ì§„í™” ëª©í‘œ ëª…ì„¸ì„œ
    evolution_goal: Optional[EvolutionGoalSpec] = None
    
    # 2) í˜„ì¬ ìƒíƒœ ë³´ê³ ì„œ  
    current_state: Optional[CurrentStateReport] = None
    
    # 3) ì™¸ë¶€ ë¦¬ì„œì¹˜ (URP)
    research_pack: Optional[UpgradeResearchPack] = None
    
    # 4) ê°­ ë¶„ì„ ë° ê³„íš
    gap_report: Optional[GapReport] = None
    
    # Legacy analysis results (backward compatibility)
    requirement_analysis: Optional[Dict[str, Any]] = None
    static_analysis: Optional[Dict[str, Any]] = None
    code_analysis: Optional[Dict[str, Any]] = None
    gap_analysis: Optional[Dict[str, Any]] = None
    behavior_analysis: Optional[Dict[str, Any]] = None
    impact_analysis: Optional[Dict[str, Any]] = None
    quality_metrics: Optional[Dict[str, Any]] = None
    
    # Aggregated insights
    system_health_score: float = 0.0
    upgrade_risk_score: float = 0.0
    total_issues_found: int = 0
    critical_issues: List[Dict[str, Any]] = field(default_factory=list)
    
    # Recommendations
    immediate_actions: List[str] = field(default_factory=list)
    short_term_goals: List[str] = field(default_factory=list)
    long_term_goals: List[str] = field(default_factory=list)
    
    # Execution metadata
    total_execution_time: float = 0.0
    phases_completed: int = 0
    phases_failed: int = 0
    tasks_breakdown: List[Dict[str, Any]] = field(default_factory=list)  # 5~20ë¶„ íƒœìŠ¤í¬
    
    # Evolution Loop metadata
    evolution_iterations: int = 0  # Evolution Loop ë°˜ë³µ íšŸìˆ˜
    agents_created: List[str] = field(default_factory=list)  # Agnoë¡œ ìƒì„±ëœ ì—ì´ì „íŠ¸ ëª©ë¡


class UpgradeOrchestrator:
    """AI-driven orchestrator for intelligent upgrade analysis.
    
    This orchestrator uses AI to:
    1. Dynamically select relevant agents based on requirements
    2. Optimize execution order and parallelization
    3. Predict resource needs and allocate efficiently
    4. Detect potential failures and adjust strategy
    5. Learn from past executions to improve
    6. Generate optimal upgrade paths
    7. Intelligently coordinate analysis agents
    """
    
    def __init__(self, config: UpgradeConfig):
        """Initialize Upgrade Orchestrator.
        
        Args:
            config: Upgrade configuration
        """
        self.config = config
        self.memory_hub = None
        self.phases: List[AnalysisPhase] = []
        self.document_context = SharedDocumentContext()  # ê³µìœ  ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
        
        # Initialize agents (will be created when needed)
        self.planner_agent = None
        self.task_creator_agent = None
        self.requirement_analyzer = None
        self.static_analyzer = None
        self.code_analyzer = None
        self.gap_analyzer = None
        self.behavior_analyzer = None
        self.impact_analyzer = None
        self.code_generator = None
        self.quality_gate = None
        
        # ì•„í‚¤í…ì²˜ ë° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë””ìì¸ ì—ì´ì „íŠ¸
        self.system_architect = None
        self.orchestrator_designer = None
        
        # Agno í†µí•© - ìë™ ì—ì´ì „íŠ¸ ìƒì„±
        self.agno_manager = None
        self.agno_code_generator = None
        self.report_generator = None
        self.external_researcher = None
        
        # AI Provider for orchestration decisions - 100% REAL AI
        self.ai_provider = BedrockAIProvider(
            model="claude-3-sonnet",
            region="us-east-1"
        )
        
        # í˜ë¥´ì†Œë‚˜ ì„¤ì • - ì§„í™” ë§ˆì—ìŠ¤íŠ¸ë¡œ
        from backend.packages.agents.personas import get_persona
        self.persona = get_persona("UpgradeOrchestrator")
        if self.persona:
            logger.info(f"ğŸ­ í˜ë¥´ì†Œë‚˜ í™œì„±í™”: {self.persona.name} - '{self.persona.catchphrase}'")
        
        # Safety mechanisms
        self.circuit_breaker = CircuitBreaker(
            name="UpgradeOrchestrator",
            config=CircuitBreakerConfig(
                failure_threshold=3,
                recovery_timeout=30,
                half_open_max_calls=2
            )
        )
        
        self.resource_limiter = ResourceLimiter(
            limits=ResourceLimit(
                max_memory_mb=2000,  # 2GB
                max_cpu_percent=80,
                max_execution_time=config.max_execution_time,
                max_concurrent_tasks=10
            )
        )
    
    async def initialize(self) -> None:
        """Initialize all components."""
        logger.info("Initializing Upgrade Orchestrator...")
        
        # Initialize Memory Hub
        self.memory_hub = MemoryHub()
        await self.memory_hub.initialize()
        
        # Initialize all agents with memory hub and shared document context
        self.planner_agent = PlannerAgent(memory_hub=self.memory_hub, document_context=self.document_context)
        self.task_creator_agent = TaskCreatorAgent(memory_hub=self.memory_hub, document_context=self.document_context)
        self.requirement_analyzer = RequirementAnalyzer(memory_hub=self.memory_hub, document_context=self.document_context)
        self.external_researcher = ExternalResearcher(memory_hub=self.memory_hub, document_context=self.document_context)
        self.static_analyzer = StaticAnalyzer(memory_hub=self.memory_hub, document_context=self.document_context)
        self.code_analyzer = CodeAnalysisAgent(memory_hub=self.memory_hub, document_context=self.document_context)
        self.gap_analyzer = GapAnalyzer(memory_hub=self.memory_hub, document_context=self.document_context)
        self.behavior_analyzer = BehaviorAnalyzer(memory_hub=self.memory_hub, document_context=self.document_context)
        self.impact_analyzer = ImpactAnalyzer(
            memory_hub=self.memory_hub,
            static_analyzer=self.static_analyzer,
            document_context=self.document_context
        )
        self.system_architect = SystemArchitect(memory_hub=self.memory_hub, document_context=self.document_context)
        self.orchestrator_designer = OrchestratorDesigner(memory_hub=self.memory_hub, document_context=self.document_context)
        self.code_generator = CodeGenerator(memory_hub=self.memory_hub, document_context=self.document_context)
        self.quality_gate = QualityGate(memory_hub=self.memory_hub, document_context=self.document_context)
        # self.report_generator = ReportGenerator(memory_hub=self.memory_hub)  # Not implemented yet
        
        # Agno ì´ˆê¸°í™” (Evolution Loopìš©)
        if self.config.enable_evolution_loop:
            logger.info("Initializing Agno for Evolution Loop...")
            from backend.packages.agents.registry import AgentRegistry
            
            registry = AgentRegistry()
            self.agno_manager = AgnoManager(
                memory_hub=self.memory_hub,
                registry=registry
            )
            self.agno_code_generator = AgnoCodeGenerator(
                memory_hub=self.memory_hub
            )
            logger.info("Agno initialized successfully")
        
        logger.info("Orchestrator initialized successfully")
    
    async def analyze(self, requirements: str, include_research: bool = True) -> UpgradeReport:
        """AI-ë“œë¦¬ë¸ ì—…ê·¸ë ˆì´ë“œ ë¶„ì„ ì‹¤í–‰ (Evolution Loop í†µí•©).
        
        ì •í•´ì§„ ê¸°ë³¸ ìˆœì„œì— ë”°ë¼ ì—ì´ì „íŠ¸ë“¤ì„ ì‹¤í–‰í•˜ì§€ë§Œ,
        ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ AIê°€ ë™ì ìœ¼ë¡œ ìˆœì„œì™€ ì—ì´ì „íŠ¸ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŒ.
        Evolution Loopì´ í™œì„±í™”ë˜ë©´ ê°­ì´ í•´ì†Œë  ë•Œê¹Œì§€ ìë™ìœ¼ë¡œ ë°˜ë³µ.
        
        ê¸°ë³¸ ì‹¤í–‰ ìˆœì„œ:
        1. RequirementAnalyzer - ìš”êµ¬ì‚¬í•­ ë¶„ì„/ë¬¸ì„œí™”
        2. í˜„ì¬ìƒíƒœ ë¶„ì„ (ë³‘ë ¬) - behavior, code, impact, static, quality
        3. ExternalResearcher - ì™¸ë¶€ ìë£Œ ì¡°ì‚¬
        4. GapAnalyzer - ë³€ê²½ì‚¬í•­ ë¶„ì„/ìˆ˜ì¹˜í™”
        5. SystemArchitect - ì•„í‚¤í…ì²˜ ì„¤ê³„
        6. OrchestratorDesigner - ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°/ì—ì´ì „íŠ¸ ë””ìì¸
        7. PlannerAgent - Phase ë‹¨ìœ„ ê³„íš
        8. TaskCreatorAgent - ì„¸ë¶€ íƒœìŠ¤í¬ ê³„íš
        9. CodeGenerator - ì½”ë“œ ìƒì„± (Agno í†µí•©)
        10. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        11. ê°­ ì¬í™•ì¸ ë° Evolution Loop (ê°­ì´ 0ì´ ë  ë•Œê¹Œì§€ ë°˜ë³µ)
        
        Args:
            requirements: ìì—°ì–´ ìš”êµ¬ì‚¬í•­
            include_research: ì™¸ë¶€ ë¦¬ì„œì¹˜ í¬í•¨ ì—¬ë¶€
            
        Returns:
            ì¢…í•© ì—…ê·¸ë ˆì´ë“œ ë³´ê³ ì„œ
        """
        logger.info(f"Starting AI-driven upgrade analysis for: {self.config.project_path}")
        logger.info(f"Evolution Loop enabled: {self.config.enable_evolution_loop}")
        start_time = datetime.now()
        
        # Initialize report
        report = UpgradeReport(
            timestamp=start_time.isoformat(),
            project_path=self.config.project_path
        )
        
        # Check circuit breaker
        if not await self.circuit_breaker.call(self._check_system_ready):
            report.critical_issues.append({
                "type": "system",
                "message": "Circuit breaker is open - too many recent failures"
            })
            return report
        
        # Evolution Loop ì¹´ìš´í„°
        evolution_iteration = 0
        max_iterations = self.config.max_evolution_iterations if self.config.enable_evolution_loop else 1
        
        try:
            # ì´ˆê¸° ìš”êµ¬ì‚¬í•­ ë¶„ì„ì€ í•œ ë²ˆë§Œ ìˆ˜í–‰
            logger.info("Phase 1: Analyzing requirements...")
            requirement_result = await self._execute_requirement_analysis(requirements)
            report.requirement_analysis = requirement_result
            
            # Evolution Loop ì‹œì‘
            while evolution_iteration < max_iterations:
                evolution_iteration += 1
                logger.info(f"\n{'='*80}")
                logger.info(f"ğŸ”„ Evolution Loop Iteration {evolution_iteration}/{max_iterations}")
                logger.info(f"{'='*80}\n")
                
                # ìƒˆ ë£¨í”„ ì‹œì‘ - ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
                self.document_context.start_new_loop()
                
                # ìš”êµ¬ì‚¬í•­ì„ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                self.document_context.add_document("RequirementAnalyzer", requirement_result, "requirement")
                
                # AI ë“œë¦¬ë¸ ë™ì  ì›Œí¬í”Œë¡œìš° ì‚¬ìš© ì—¬ë¶€ ê²°ì •
                if self.config.ai_driven_workflow:
                    # AIê°€ ë‹¤ìŒ ì‹¤í–‰í•  ì—ì´ì „íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •
                    logger.info("ğŸ¤– AI-driven dynamic workflow enabled")
                    await self._execute_dynamic_workflow(report, evolution_iteration)
                else:
                    # ê¸°ì¡´ ì •ì  ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                    # Phase 2: í˜„ì¬ ìƒíƒœ ë¶„ì„ (ë³‘ë ¬ ì‹¤í–‰)
                    logger.info(f"Phase 2 (Iteration {evolution_iteration}): Analyzing current state...")
                    current_state_results = await self._execute_current_state_analysis()
                    report.static_analysis = current_state_results.get('static')
                    report.code_analysis = current_state_results.get('code')
                    report.behavior_analysis = current_state_results.get('behavior')
                    report.impact_analysis = current_state_results.get('impact')
                    report.quality_metrics = current_state_results.get('quality')
                    
                    # ê²°ê³¼ë¥¼ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                    for agent_name, result in current_state_results.items():
                        if result:
                            self.document_context.add_document(f"{agent_name}_analyzer", result, "analysis")
                
                # Phase 3: ì™¸ë¶€ ë¦¬ì„œì¹˜ (ì²« ë°˜ë³µì—ì„œë§Œ)
                if include_research and evolution_iteration == 1:
                    logger.info("Phase 3: Conducting external research...")
                    research_result = await self._execute_external_research(
                        requirement_result,
                        current_state_results
                    )
                    report.research_pack = research_result
                
                # Phase 4: ê°­ ë¶„ì„ (ë§¤ ë°˜ë³µë§ˆë‹¤ ìˆ˜í–‰)
                logger.info(f"Phase 4 (Iteration {evolution_iteration}): Analyzing gaps...")
                gap_result = await self._execute_gap_analysis(
                    requirement_result,
                    current_state_results,
                    report.research_pack
                )
                report.gap_analysis = gap_result
                
                # ê°­ ì²´í¬ - Evolution Loop ì¢…ë£Œ ì¡°ê±´
                gaps = gap_result.get('gaps', []) if gap_result else []
                gap_score = gap_result.get('gap_score', 0) if gap_result else 0
                
                # ê°­ì´ í•´ì†Œë˜ì—ˆê±°ë‚˜ ìˆ˜ë ´ ì„ê³„ê°’ì— ë„ë‹¬í•œ ê²½ìš°
                if not gaps or gap_score >= self.config.evolution_convergence_threshold:
                    logger.info(f"âœ… All gaps resolved or convergence reached (score: {gap_score:.2%})")
                    break
                
                logger.info(f"ğŸ“Š Remaining gaps: {len(gaps)}, Gap score: {gap_score:.2%}")
                
                # Phase 5: ì•„í‚¤í…ì²˜ ì„¤ê³„
                logger.info(f"Phase 5 (Iteration {evolution_iteration}): Designing architecture...")
                architecture_design = await self._execute_architecture_design(
                    requirement_result,
                    gap_result
                )
                
                # Phase 6: ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë””ìì¸
                logger.info(f"Phase 6 (Iteration {evolution_iteration}): Designing orchestrator...")
                orchestrator_design = await self._execute_orchestrator_design(
                    architecture_design,
                    requirement_result
                )
                
                # Agnoë¥¼ í†µí•œ ìë™ ì—ì´ì „íŠ¸ ìƒì„± (í™œì„±í™”ëœ ê²½ìš°)
                if self.config.auto_generate_agents and self.config.enable_evolution_loop:
                    logger.info("ğŸ¤– Auto-generating agents with Agno...")
                    generated_agents = await self._generate_agents_with_agno(
                        orchestrator_design,
                        gaps
                    )
                    if generated_agents:
                        logger.info(f"Generated {len(generated_agents)} new agents")
                        report.agents_created = report.agents_created or []
                        report.agents_created.extend(generated_agents)
                
                # Phase 7: ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
                logger.info(f"Phase 7 (Iteration {evolution_iteration}): Creating execution plan...")
                execution_plan = await self._execute_planning(
                    architecture_design,
                    orchestrator_design,
                    requirement_result
                )
                
                # Phase 8: ì„¸ë¶€ íƒœìŠ¤í¬ ìƒì„±
                logger.info(f"Phase 8 (Iteration {evolution_iteration}): Creating detailed tasks...")
                detailed_tasks = await self._execute_task_creation(
                    execution_plan,
                    orchestrator_design
                )
                report.tasks_breakdown = detailed_tasks
                
                # Phase 9: ì½”ë“œ ìƒì„± (ìë™ êµ¬í˜„ì´ í™œì„±í™”ëœ ê²½ìš°)
                if self.config.auto_implement_code:
                    logger.info(f"Phase 9 (Iteration {evolution_iteration}): Generating code...")
                    code_generation_result = await self._execute_code_generation(
                        detailed_tasks,
                        architecture_design
                    )
                    
                    # Phase 10: í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                    logger.info(f"Phase 10 (Iteration {evolution_iteration}): Running tests...")
                    test_result = await self._execute_tests(code_generation_result)
                    
                    # Phase 11: Evolution Loop ì¢…ë£Œ ì—¬ë¶€ ê²°ì •
                    if not self.config.enable_evolution_loop:
                        logger.info("Evolution Loop disabled, stopping after first iteration")
                        break
                
                # Evolution Loopì´ í™œì„±í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì²« ë°˜ë³µ í›„ ì¢…ë£Œ
                if not self.config.enable_evolution_loop:
                    break
            
            # Evolution Loop ì™„ë£Œ ë¡œê·¸
            if self.config.enable_evolution_loop:
                logger.info(f"\n{'='*80}")
                logger.info(f"ğŸ§¬ Evolution Loop completed after {evolution_iteration} iterations")
                logger.info(f"{'='*80}\n")
                report.evolution_iterations = evolution_iteration
            
            # Aggregate results
            self._aggregate_results(report)
            
            # Generate recommendations
            self._generate_recommendations(report)
            
            # Calculate scores
            self._calculate_scores(report)
            
            # Save reports as markdown files
            await self._save_all_reports_as_markdown(report)
            
        except Exception as e:
            logger.error(f"Analysis failed: {e}")
            report.critical_issues.append({
                "type": "execution",
                "message": str(e)
            })
        finally:
            # Calculate execution time
            end_time = datetime.now()
            report.total_execution_time = (end_time - start_time).total_seconds()
            
            # Count phase results
            report.phases_completed = len([p for p in self.phases if p.status == "completed"])
            report.phases_failed = len([p for p in self.phases if p.status == "failed"])
            
            # Store report in memory
            await self._store_report(report)
        
        logger.info(f"Analysis completed in {report.total_execution_time:.2f} seconds")
        return report
    
    async def _define_phases_with_ai(self, requirements: str) -> List[AnalysisPhase]:
        """AIë¥¼ ì‚¬ìš©í•œ ì§€ëŠ¥ì ì¸ ë¶„ì„ ë‹¨ê³„ ì„ íƒ ë° ìˆœì„œ ê²°ì •.
        
        ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ì—ì´ì „íŠ¸ì™€ ì‹¤í–‰ ìˆœì„œë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •í•©ë‹ˆë‹¤.
        ê¸°ë³¸ ìˆœì„œë¥¼ ë”°ë¥´ë˜, ìš”êµ¬ì‚¬í•­ì˜ íŠ¹ì„±ì— ë”°ë¼ ì¡°ì •í•©ë‹ˆë‹¤.
        
        Args:
            requirements: ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­
            
        Returns:
            ìµœì í™”ëœ ë¶„ì„ ë‹¨ê³„ ëª©ë¡
        """
        try:
            # AI Provider ì´ˆê¸°í™”
            ai_provider = get_ai_provider()
            
            # AIì—ê²Œ ìš”êµ¬ì‚¬í•­ ë¶„ì„ ë° ìµœì  ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ìš”ì²­
            prompt = f"""
            Analyze these requirements and determine the optimal agent execution order:
            
            Requirements: {requirements[:1000]}
            
            Available agents:
            1. RequirementAnalyzer - Parses and documents requirements
            2. StaticAnalyzer - Analyzes code structure and complexity
            3. CodeAnalysisAgent - AI-based code understanding
            4. BehaviorAnalyzer - Analyzes runtime behavior from logs
            5. ImpactAnalyzer - Analyzes change impact and dependencies
            6. QualityGate - Checks code quality metrics
            7. ExternalResearcher - Searches external resources
            8. GapAnalyzer - Identifies gaps between current and desired state
            9. SystemArchitect - Designs system architecture
            10. OrchestratorDesigner - Designs orchestrator flow
            11. PlannerAgent - Creates phase-level plans
            12. TaskCreatorAgent - Creates detailed tasks
            13. CodeGenerator - Generates code
            
            Based on the requirements, determine:
            1. Which agents are essential (must run)
            2. Which agents are optional (can be skipped)
            3. Optimal execution order
            4. Which agents can run in parallel
            
            Consider:
            - If requirements mention "debug" or "fix", prioritize analysis agents
            - If requirements mention "new feature", prioritize design agents
            - If requirements mention "refactor", prioritize quality and impact agents
            - If requirements mention "upgrade", run all agents
            
            Return a structured JSON response with:
            {{
                "essential_agents": ["agent1", "agent2", ...],
                "optional_agents": ["agent3", ...],
                "execution_order": [
                    {{"phase": 1, "agents": ["agent1"], "parallel": false}},
                    {{"phase": 2, "agents": ["agent2", "agent3"], "parallel": true}},
                    ...
                ],
                "reasoning": "Brief explanation of choices"
            }}
            """
            
            response = await ai_provider.complete(prompt)
            
            # Parse AI response
            import json
            try:
                ai_plan = json.loads(response)
                logger.info(f"AI-driven phase selection: {ai_plan.get('reasoning', 'No reasoning provided')}")
                
                # Convert AI plan to AnalysisPhase objects
                phases = []
                for phase_info in ai_plan.get('execution_order', []):
                    for agent_name in phase_info.get('agents', []):
                        # Map agent names to phase names
                        phase_name = self._get_phase_name_for_agent(agent_name)
                        if phase_name:
                            phases.append(AnalysisPhase(
                                name=phase_name,
                                agent=agent_name,
                                status="pending"
                            ))
                
                # If AI planning failed or returned empty, fallback to default
                if not phases:
                    logger.warning("AI phase planning returned empty, using default phases")
                    return self._define_phases(requirements)
                
                return phases
                
            except json.JSONDecodeError:
                logger.warning("Failed to parse AI response, using default phases")
                return self._define_phases(requirements)
                
        except Exception as e:
            logger.error(f"AI-driven phase selection failed: {e}, falling back to default")
            return self._define_phases(requirements)
    
    def _get_phase_name_for_agent(self, agent_name: str) -> Optional[str]:
        """ì—ì´ì „íŠ¸ ì´ë¦„ì„ phase ì´ë¦„ìœ¼ë¡œ ë§¤í•‘."""
        mapping = {
            "RequirementAnalyzer": "requirement_analysis",
            "StaticAnalyzer": "static_analysis",
            "CodeAnalysisAgent": "code_analysis",
            "BehaviorAnalyzer": "behavior_analysis",
            "ImpactAnalyzer": "impact_analysis",
            "QualityGate": "quality_gate",
            "ExternalResearcher": "external_research",
            "GapAnalyzer": "gap_analysis",
            "SystemArchitect": "architecture_design",
            "OrchestratorDesigner": "orchestrator_design",
            "PlannerAgent": "planning",
            "TaskCreatorAgent": "task_creation",
            "CodeGenerator": "code_generation"
        }
        return mapping.get(agent_name)
    
    def _define_phases(self, requirements: str) -> List[AnalysisPhase]:
        """Define analysis phases based on configuration.
        
        Args:
            requirements: User requirements
            
        Returns:
            List of analysis phases
        """
        phases = []
        
        # Always run requirement analysis first
        phases.append(AnalysisPhase(
            name="requirement_analysis",
            agent="RequirementAnalyzer",
            status="pending"
        ))
        
        # Static analysis
        phases.append(AnalysisPhase(
            name="static_analysis",
            agent="StaticAnalyzer",
            status="pending"
        ))
        
        # Code analysis (AI + optional dynamic)
        phases.append(AnalysisPhase(
            name="code_analysis",
            agent="CodeAnalysisAgent",
            status="pending"
        ))
        
        # Gap analysis
        phases.append(AnalysisPhase(
            name="gap_analysis",
            agent="GapAnalyzer",
            status="pending"
        ))
        
        # Behavior analysis (if logs available)
        if self.config.include_behavior_analysis:
            phases.append(AnalysisPhase(
                name="behavior_analysis",
                agent="BehaviorAnalyzer",
                status="pending"
            ))
        
        # Impact analysis
        if self.config.generate_impact_matrix:
            phases.append(AnalysisPhase(
                name="impact_analysis",
                agent="ImpactAnalyzer",
                status="pending"
            ))
        
        # Quality gate
        phases.append(AnalysisPhase(
            name="quality_check",
            agent="QualityGate",
            status="pending"
        ))
        
        self.phases = phases
        return phases
    
    async def _execute_sequential(
        self,
        phases: List[AnalysisPhase],
        report: UpgradeReport
    ) -> None:
        """Execute phases sequentially.
        
        Args:
            phases: List of phases
            report: Report to populate
        """
        for phase in phases:
            await self._execute_phase(phase, report)
            if phase.status == "failed":
                logger.warning(f"Phase {phase.name} failed, continuing...")
    
    async def _execute_parallel(
        self,
        phases: List[AnalysisPhase],
        report: UpgradeReport
    ) -> None:
        """Execute phases in parallel where possible, respecting dependencies.
        
        Execution order:
        1. RequirementAnalyzer (í•„ìˆ˜ ì„ í–‰)
        2. StaticAnalyzer, CodeAnalysisAgent, BehaviorAnalyzer (ë³‘ë ¬ ê°€ëŠ¥)
        3. GapAnalyzer, ImpactAnalyzer (ì˜ì¡´ì„± ìˆìŒ, ìˆœì°¨ ì‹¤í–‰)
        4. QualityGate (ë§ˆì§€ë§‰ ì‹¤í–‰)
        
        Args:
            phases: List of phases
            report: Report to populate
        """
        if not phases:
            return
        
        # Phase 1: Requirement analysis must run first
        requirement_phases = [p for p in phases if p.agent == "RequirementAnalyzer"]
        for phase in requirement_phases:
            await self._execute_phase(phase, report)
            if phase.status == "failed":
                logger.warning("Requirement analysis had issues, continuing with limited functionality")
        
        # Phase 2: Independent analyses can run in parallel
        parallel_agents = ["StaticAnalyzer", "CodeAnalysisAgent", "BehaviorAnalyzer"]
        parallel_phases = [p for p in phases if p.agent in parallel_agents]
        
        if parallel_phases:
            tasks = [self._execute_phase(phase, report) for phase in parallel_phases]
            await asyncio.gather(*tasks, return_exceptions=True)
        
        # Phase 3: Dependent analyses run sequentially
        # GapAnalyzer needs requirement and static analysis results
        gap_phases = [p for p in phases if p.agent == "GapAnalyzer"]
        for phase in gap_phases:
            await self._execute_phase(phase, report)
        
        # ImpactAnalyzer needs static analysis results
        impact_phases = [p for p in phases if p.agent == "ImpactAnalyzer"]
        for phase in impact_phases:
            await self._execute_phase(phase, report)
        
        # Phase 4: Quality gate runs last (ë¶„ì„ ë‹¨ê³„ì˜ ë§ˆì§€ë§‰)
        quality_phases = [p for p in phases if p.agent == "QualityGate"]
        for phase in quality_phases:
            await self._execute_phase(phase, report)
        
        # NOTE: CodeGeneratorëŠ” ì´ ë‹¨ê³„ì—ì„œ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
        # ê³„íš ìˆ˜ë¦½ í›„ ì‚¬ìš©ì ìŠ¹ì¸ì„ ë°›ì€ ë‹¤ìŒ ë³„ë„ ì‹¤í–‰
    
    async def _execute_phase(
        self,
        phase: AnalysisPhase,
        report: UpgradeReport
    ) -> None:
        """Execute a single analysis phase.
        
        Args:
            phase: Phase to execute
            report: Report to populate
        """
        logger.info(f"Executing phase: {phase.name}")
        phase.status = "running"
        phase.start_time = datetime.now()
        
        try:
            # Get the appropriate agent
            agent_name = phase.agent.lower()
            # Handle special cases
            if agent_name == "qualitygate":
                agent_name = "quality_gate"
            elif agent_name == "codeanalysisagent":
                agent_name = "code_analyzer"
            else:
                agent_name = agent_name.replace("analyzer", "_analyzer")
            agent = getattr(self, agent_name)
            
            # Prepare task based on phase
            task = self._prepare_task(phase.name, report)
            
            # Execute agent
            result = await agent.execute(task)
            
            # Store result
            phase.result = result.data if result.success else None
            phase.status = "completed" if result.success else "failed"
            phase.error = result.error if not result.success else None
            
            # Update report
            self._update_report(phase, report)
            
        except Exception as e:
            logger.error(f"Phase {phase.name} failed: {e}")
            phase.status = "failed"
            phase.error = str(e)
        finally:
            phase.end_time = datetime.now()
    
    def _prepare_task(self, phase_name: str, report: UpgradeReport) -> AgentTask:
        """Prepare task for agent execution.
        
        Args:
            phase_name: Name of the phase
            report: Current report
            
        Returns:
            Agent task
        """
        task = AgentTask(
            task_id=f"{phase_name}_{datetime.now().timestamp()}",
            intent=phase_name,
            inputs={}
        )
        
        if phase_name == "requirement_analysis":
            # Use requirements from report or config
            task.inputs["requirements"] = report.requirement_analysis or {}
            task.inputs["project_path"] = self.config.project_path
            
        elif phase_name == "static_analysis":
            task.inputs["path"] = self.config.project_path
            task.inputs["recursive"] = True
            
        elif phase_name == "code_analysis":
            task.inputs["file_path"] = self.config.project_path
            task.inputs["analysis_type"] = "general"
            task.inputs["enable_dynamic"] = self.config.enable_dynamic_analysis
            task.inputs["safe_mode"] = self.config.safe_mode
            
        elif phase_name == "gap_analysis":
            task.inputs["project_path"] = self.config.project_path
            task.inputs["min_coverage"] = 80
            
        elif phase_name == "behavior_analysis":
            # Find log files
            log_paths = self._find_log_files()
            task.inputs["log_paths"] = log_paths
            
        elif phase_name == "impact_analysis":
            task.inputs["project_path"] = self.config.project_path
            task.inputs["analysis_type"] = "report"
            
        elif phase_name == "quality_check":
            task.inputs["project_path"] = self.config.project_path
            
        return task
    
    def _find_log_files(self) -> List[str]:
        """Find log files in the project.
        
        Returns:
            List of log file paths
        """
        log_files = []
        log_dirs = ["logs", "log", ".logs", "var/log"]
        log_extensions = [".log", ".txt", ".out"]
        
        for log_dir in log_dirs:
            dir_path = os.path.join(self.config.project_path, log_dir)
            if os.path.exists(dir_path):
                for root, _, files in os.walk(dir_path):
                    for file in files:
                        if any(file.endswith(ext) for ext in log_extensions):
                            log_files.append(os.path.join(root, file))
        
        return log_files[:10]  # Limit to 10 most recent logs
    
    def _update_report(self, phase: AnalysisPhase, report: UpgradeReport) -> None:
        """Update report with phase results.
        
        Args:
            phase: Completed phase
            report: Report to update
        """
        if phase.result:
            if phase.name == "requirement_analysis":
                report.requirement_analysis = phase.result
            elif phase.name == "static_analysis":
                report.static_analysis = phase.result
            elif phase.name == "code_analysis":
                report.code_analysis = phase.result
            elif phase.name == "gap_analysis":
                report.gap_analysis = phase.result
            elif phase.name == "behavior_analysis":
                report.behavior_analysis = phase.result
            elif phase.name == "impact_analysis":
                report.impact_analysis = phase.result
            elif phase.name == "quality_check":
                report.quality_metrics = phase.result
    
    def _aggregate_results(self, report: UpgradeReport) -> None:
        """Aggregate results from all analyses.
        
        Args:
            report: Report to aggregate
        """
        # Count total issues
        issue_count = 0
        
        if report.static_analysis:
            issue_count += len(report.static_analysis.get("security_issues", []))
            issue_count += len(report.static_analysis.get("complexity_hotspots", []))
        
        if report.gap_analysis:
            issue_count += report.gap_analysis.get("gaps_found", 0)
        
        if report.behavior_analysis:
            issue_count += len(report.behavior_analysis.get("error_patterns", []))
        
        if report.quality_metrics:
            if report.quality_metrics.get("coverage", 100) < 80:
                issue_count += 1
            if report.quality_metrics.get("complexity", 0) > 10:
                issue_count += 1
        
        report.total_issues_found = issue_count
        
        # Extract critical issues
        critical = []
        
        if report.static_analysis:
            for issue in report.static_analysis.get("security_issues", []):
                if issue.get("severity") in ["HIGH", "CRITICAL"]:
                    critical.append(issue)
        
        if report.behavior_analysis:
            for error in report.behavior_analysis.get("critical_errors", []):
                critical.append(error)
        
        if report.impact_analysis:
            for risk in report.impact_analysis.get("risks", {}).get("high_risk_components", []):
                critical.append(risk)
        
        report.critical_issues = critical[:10]  # Top 10 critical issues
    
    def _generate_recommendations(self, report: UpgradeReport) -> None:
        """Generate upgrade recommendations.
        
        Args:
            report: Report with analysis results
        """
        immediate = []
        short_term = []
        long_term = []
        
        # Based on critical issues
        if report.critical_issues:
            immediate.append("Address critical security and stability issues immediately")
        
        # Based on test coverage
        if report.gap_analysis:
            coverage = report.gap_analysis.get("total_coverage", 0)
            if coverage < 50:
                immediate.append(f"Critical: Test coverage is only {coverage:.1f}%")
            elif coverage < 80:
                short_term.append(f"Improve test coverage from {coverage:.1f}% to 80%")
        
        # Based on code quality
        if report.quality_metrics:
            if report.quality_metrics.get("complexity", 0) > 15:
                short_term.append("Refactor complex functions to reduce cyclomatic complexity")
        
        # Based on behavior analysis
        if report.behavior_analysis:
            if report.behavior_analysis.get("error_rate", 0) > 0.05:
                immediate.append("High error rate detected - investigate and fix")
            
            hotspots = report.behavior_analysis.get("performance_hotspots", [])
            if hotspots:
                short_term.append(f"Optimize {len(hotspots)} performance bottlenecks")
        
        # Based on impact analysis
        if report.impact_analysis:
            if report.impact_analysis.get("system_health", {}).get("technical_debt_score", 0) > 50:
                long_term.append("Develop technical debt reduction strategy")
            
            circular_deps = report.impact_analysis.get("risks", {}).get("circular_dependencies", 0)
            if circular_deps > 0:
                short_term.append(f"Resolve {circular_deps} circular dependencies")
        
        # Based on static analysis
        if report.static_analysis:
            if report.static_analysis.get("architecture_layers", {}):
                long_term.append("Consider architectural improvements for better separation of concerns")
        
        report.immediate_actions = immediate
        report.short_term_goals = short_term
        report.long_term_goals = long_term
    
    def _calculate_scores(self, report: UpgradeReport) -> None:
        """Calculate overall scores.
        
        Args:
            report: Report with analysis results
        """
        # System health score (0-100)
        health_factors = []
        
        if report.gap_analysis:
            coverage = report.gap_analysis.get("total_coverage", 0)
            health_factors.append(coverage)
        
        if report.quality_metrics:
            quality = 100 - min(report.quality_metrics.get("complexity", 0) * 5, 50)
            health_factors.append(quality)
        
        if report.behavior_analysis:
            error_rate = report.behavior_analysis.get("error_rate", 0)
            behavior_health = max(0, 100 - (error_rate * 1000))
            health_factors.append(behavior_health)
        
        if health_factors:
            report.system_health_score = sum(health_factors) / len(health_factors)
        
        # Upgrade risk score (0-100)
        risk_factors = []
        
        # More critical issues = higher risk
        risk_factors.append(min(len(report.critical_issues) * 10, 50))
        
        # High technical debt = higher risk
        if report.impact_analysis:
            tech_debt = report.impact_analysis.get("system_health", {}).get("technical_debt_score", 0)
            risk_factors.append(tech_debt * 0.5)
        
        # Low test coverage = higher risk
        if report.gap_analysis:
            coverage = report.gap_analysis.get("total_coverage", 0)
            risk_factors.append(max(0, 50 - coverage * 0.5))
        
        if risk_factors:
            report.upgrade_risk_score = min(sum(risk_factors) / len(risk_factors), 100)
    
    async def execute_plan(self, plan_id: str, task_ids: List[str] = None) -> Dict[str, Any]:
        """ê³„íšì„ ì‹¤í–‰í•˜ëŠ” ë©”ì„œë“œ (Phase 2).
        
        ë¶„ì„ ì™„ë£Œ í›„ ìƒì„±ëœ ê³„íš(4_Tasks.json)ì„ ì‹¤ì œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
        ì‚¬ìš©ìê°€ ìŠ¹ì¸í•œ íƒœìŠ¤í¬ë§Œ ì„ íƒì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        
        Args:
            plan_id: ì‹¤í–‰í•  ê³„íš ID (ë³´í†µ timestamp)
            task_ids: ì‹¤í–‰í•  íŠ¹ì • íƒœìŠ¤í¬ ID ëª©ë¡ (Noneì´ë©´ ì „ì²´ ì‹¤í–‰)
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"Executing plan: {plan_id}")
        
        # 1. ê³„íš ë¡œë“œ
        plan = await self.memory_hub.get(ContextType.O_CTX, f"plan_{plan_id}")
        if not plan:
            raise ValueError(f"Plan {plan_id} not found")
        
        # 2. íƒœìŠ¤í¬ í•„í„°ë§
        tasks_to_execute = plan['tasks']
        if task_ids:
            tasks_to_execute = [t for t in tasks_to_execute if t['id'] in task_ids]
        
        execution_results = {
            'plan_id': plan_id,
            'started_at': datetime.now().isoformat(),
            'tasks': []
        }
        
        # 3. ê° íƒœìŠ¤í¬ ì‹¤í–‰
        for task in tasks_to_execute:
            if task['type'] == 'code_generation':
                # CodeGenerator ì‚¬ìš©
                result = await self._execute_code_generation(task)
            elif task['type'] == 'code_modification':
                # ê¸°ì¡´ ì½”ë“œ ìˆ˜ì •
                result = await self._execute_code_modification(task)
            elif task['type'] == 'test_creation':
                # í…ŒìŠ¤íŠ¸ ìƒì„±
                result = await self._execute_test_creation(task)
            else:
                result = {'status': 'skipped', 'reason': 'Unknown task type'}
            
            execution_results['tasks'].append({
                'task_id': task['id'],
                'task_type': task['type'],
                'result': result
            })
        
        execution_results['completed_at'] = datetime.now().isoformat()
        
        # 4. ê²°ê³¼ ì €ì¥
        await self.memory_hub.put(
            ContextType.O_CTX,
            f"execution_result_{plan_id}",
            execution_results,
            ttl_seconds=86400 * 30
        )
        
        return execution_results
    
    async def _execute_code_generation(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """ì½”ë“œ ìƒì„± íƒœìŠ¤í¬ ì‹¤í–‰."""
        if not self.code_generator:
            from backend.packages.agents.code_generator import CodeGenerator
            self.code_generator = CodeGenerator(memory_hub=self.memory_hub)
        
        generation_task = AgentTask(
            type="code_generation",
            data=task['specification']
        )
        
        result = await self.code_generator.execute(generation_task)
        return result.data if result else {'status': 'failed'}
    
    async def _execute_code_modification(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """ì½”ë“œ ìˆ˜ì • íƒœìŠ¤í¬ ì‹¤í–‰."""
        # TODO: êµ¬í˜„ í•„ìš”
        return {'status': 'not_implemented'}
    
    async def _execute_test_creation(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ìƒì„± íƒœìŠ¤í¬ ì‹¤í–‰."""
        # TODO: êµ¬í˜„ í•„ìš”
        return {'status': 'not_implemented'}
    
    async def shutdown(self) -> None:
        """Shutdown orchestrator and cleanup resources."""
        logger.info("Shutting down Upgrade Orchestrator")
        # Cleanup if needed
        pass
    
    async def _check_system_ready(self) -> bool:
        """Check if system is ready for analysis.
        
        Returns:
            True if ready
        """
        # Check if project path exists
        if not os.path.exists(self.config.project_path):
            logger.error(f"Project path does not exist: {self.config.project_path}")
            return False
        
        # Check if it's a valid project
        if not os.path.isdir(self.config.project_path):
            logger.error("Project path is not a directory")
            return False
        
        return True
    
    def _break_into_small_tasks(self, requirements: str) -> List[Dict[str, Any]]:
        """ì‘ì—…ì„ 5~20ë¶„ ë‹¨ìœ„ë¡œ ìª¼ê° ë‹¤.
        
        Args:
            requirements: ìš”êµ¬ì‚¬í•­
            
        Returns:
            ì‘ì€ íƒœìŠ¤í¬ ëª©ë¡
        """
        tasks = [
            {"name": "ìš”ì²­ íŒŒì•…", "duration": "5ë¶„", "status": "pending"},
            {"name": "ë¹ ì§„ ì •ë³´ ì§ˆë¬¸", "duration": "5ë¶„", "status": "pending"},
            {"name": "ëª©í‘œ ëª…ì„¸ì„œ ì‘ì„±", "duration": "10ë¶„", "status": "pending"},
            {"name": "ì •ì  ë¶„ì„ ì‹¤í–‰", "duration": "15ë¶„", "status": "pending"},
            {"name": "ë™ì  ë¶„ì„ ì‹¤í–‰", "duration": "15ë¶„", "status": "pending"},
            {"name": "AI ì•„í‚¤í…ì²˜ ìš”ì•½", "duration": "10ë¶„", "status": "pending"},
            {"name": "ê³„ì•½/ì¸í„°í˜ì´ìŠ¤ ì±„êµ´", "duration": "10ë¶„", "status": "pending"},
            {"name": "í…ŒìŠ¤íŠ¸ ê°­ ë¶„ì„", "duration": "10ë¶„", "status": "pending"},
            {"name": "ì˜í–¥ë„ ë§¤íŠ¸ë¦­ìŠ¤ ì‘ì„±", "duration": "15ë¶„", "status": "pending"},
            {"name": "ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ ê³„ì‚°", "duration": "5ë¶„", "status": "pending"},
            {"name": "ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ ìˆ˜ë¦½", "duration": "20ë¶„", "status": "pending"},
            {"name": "ë°°ì¹˜ ê³„íš ì‘ì„±", "duration": "15ë¶„", "status": "pending"},
            {"name": "ìµœì¢… ë³´ê³ ì„œ ìƒì„±", "duration": "10ë¶„", "status": "pending"}
        ]
        return tasks
    
    async def _create_evolution_goal(self, requirements: str) -> EvolutionGoalSpec:
        """ì§„í™” ëª©í‘œ ëª…ì„¸ì„œ ì‘ì„±.
        
        Args:
            requirements: ìš”êµ¬ì‚¬í•­
            
        Returns:
            ì§„í™” ëª©í‘œ ëª…ì„¸ì„œ
        """
        # ìš”êµ¬ì‚¬í•­ ë¶„ì„
        task = AgentTask(
            task_id="evolution_goal",
            intent="analyze_requirements",
            inputs={
                "requirements": requirements,
                "project_context": {
                    "path": self.config.project_path,
                    "type": "upgrade_analysis"
                }
            }
        )
        req_result = await self.requirement_analyzer.execute(task)
        
        # ëª©í‘œ ëª…ì„¸ì„œ ìƒì„±
        spec_data = req_result.data.get("specification", {}) if req_result.success else {}
        goal_spec = EvolutionGoalSpec(
            background=spec_data.get("background", "System upgrade and evolution"),
            stakeholders=spec_data.get("stakeholders", ["developers", "users", "operations"]),
            change_scope={
                "included": spec_data.get("in_scope", []),
                "excluded": spec_data.get("out_of_scope", []),
                "preserve": ["existing APIs", "data integrity", "user workflows"]
            },
            success_criteria={
                "metrics": spec_data.get("success_metrics", {}),
                "performance": {"latency": "<100ms", "throughput": ">1000 rps"},
                "cost": {"monthly": "<$10000", "per_request": "<$0.001"},
                "risk_reduction": {"security_issues": 0, "critical_bugs": 0}
            },
            constraints={
                "deadline": spec_data.get("deadline", "30 days"),
                "budget": spec_data.get("budget", "unlimited"),
                "regulations": spec_data.get("regulations", []),
                "compatibility": spec_data.get("compatibility", [])
            },
            compatibility_requirements=[
                "Backward compatible APIs",
                "No breaking changes for existing clients",
                "Gradual migration path",
                "Feature flags for new functionality"
            ],
            open_questions=spec_data.get("open_questions", [])
        )
        
        return goal_spec
    
    async def _analyze_current_state(self) -> CurrentStateReport:
        """í˜„ì¬ ìƒíƒœ ì¢…í•© ë¶„ì„ (ì˜ì¡´ì„± ê¸°ë°˜ ì‹¤í–‰ ê·¸ë£¹).
        
        ì‹¤í–‰ ê·¸ë£¹ë³„ ë¶„ì„:
        Group 1 (ë…ë¦½ì , ë³‘ë ¬ ì‹¤í–‰):
        - ì •ì  ë¶„ì„: ì½”ë“œ êµ¬ì¡°, ë³µì¡ë„
        - ì˜ì¡´ì„± ë¶„ì„: ë²„ì „, ì·¨ì•½ì , ë¼ì´ì„ ìŠ¤
        - ê³„ì•½ ì¶”ì¶œ: API, ìŠ¤í‚¤ë§ˆ, ì¸í„°í˜ì´ìŠ¤
        
        Group 2 (Group 1 ê²°ê³¼ í™œìš©, ë³‘ë ¬ ì‹¤í–‰):
        - ë³´ì•ˆ ë¶„ì„: ì •ì  ë¶„ì„ ê²°ê³¼ ê¸°ë°˜
        - ì•„í‚¤í…ì²˜ ë¶„ì„: ì •ì  ë¶„ì„ + ì˜ì¡´ì„± ë¶„ì„ ê¸°ë°˜
        - ì½”ë“œ í’ˆì§ˆ: ì •ì  ë¶„ì„ ê¸°ë°˜ í’ˆì§ˆ í‰ê°€
        - í…ŒìŠ¤íŠ¸ ë¶„ì„: ì½”ë“œ êµ¬ì¡° ê¸°ë°˜ ì»¤ë²„ë¦¬ì§€ ë¶„ì„
        
        Group 3 (Group 1,2 ê²°ê³¼ í™œìš©, ë³‘ë ¬ ì‹¤í–‰):
        - ë™ì  ë¶„ì„: ì•„í‚¤í…ì²˜ ì´í•´ ê¸°ë°˜ ëŸ°íƒ€ì„ ë¶„ì„
        - í–‰ë™ ë¶„ì„: ë¡œê·¸/ë©”íŠ¸ë¦­ + ì•„í‚¤í…ì²˜ ì´í•´
        - ì„±ëŠ¥ ë¶„ì„: ë™ì  í”„ë¡œíŒŒì¼ë§ + ë³‘ëª© ì§€ì 
        
        Returns:
            ì¢…í•© í˜„ì¬ ìƒíƒœ ë³´ê³ ì„œ
        """
        logger.info("Starting comprehensive current state analysis with dependency groups...")
        start_time = datetime.now()
        analysis_results = {}
        
        # Group 1: ë…ë¦½ì  ë¶„ì„ë“¤ (ë³‘ë ¬ ì‹¤í–‰)
        logger.info("Executing Group 1: Independent analyses...")
        group1_tasks = {
            "static": self._run_static_analysis(),
            "dependencies": self._run_dependency_analysis(),
            "contracts": self._mine_contracts()
        }
        
        group1_results = await asyncio.gather(
            *group1_tasks.values(),
            return_exceptions=True
        )
        
        for (name, _), result in zip(group1_tasks.items(), group1_results):
            if isinstance(result, Exception):
                logger.warning(f"Group 1 analysis {name} failed: {result}")
                analysis_results[name] = {}
            else:
                analysis_results[name] = result
                
        # Group 2: Group 1 ê²°ê³¼ì— ì˜ì¡´í•˜ëŠ” ë¶„ì„ë“¤ (ë³‘ë ¬ ì‹¤í–‰)
        logger.info("Executing Group 2: Dependent analyses...")
        group2_tasks = {
            "security": self._run_security_analysis(analysis_results.get("static", {})),
            "architecture": self._analyze_architecture(
                static_data=analysis_results.get("static", {}),
                dependency_data=analysis_results.get("dependencies", {})
            ),
            "quality": self._run_code_quality_analysis(analysis_results.get("static", {})),
            "test": self._run_test_analysis(analysis_results.get("static", {}))
        }
        
        group2_results = await asyncio.gather(
            *group2_tasks.values(),
            return_exceptions=True
        )
        
        for (name, _), result in zip(group2_tasks.items(), group2_results):
            if isinstance(result, Exception):
                logger.warning(f"Group 2 analysis {name} failed: {result}")
                analysis_results[name] = {}
            else:
                analysis_results[name] = result
                
        # Group 3: ì´ì „ ê²°ê³¼ë“¤ì„ í™œìš©í•˜ëŠ” ëŸ°íƒ€ì„ ë¶„ì„ë“¤ (ë³‘ë ¬ ì‹¤í–‰)
        logger.info("Executing Group 3: Runtime analyses...")
        group3_tasks = {
            "dynamic": self._run_dynamic_analysis(
                architecture=analysis_results.get("architecture", {})
            ),
            "behavior": self._run_behavior_analysis(
                architecture=analysis_results.get("architecture", {})
            ),
            "performance": self._run_performance_analysis(
                static_data=analysis_results.get("static", {}),
                architecture=analysis_results.get("architecture", {})
            )
        }
        
        group3_results = await asyncio.gather(
            *group3_tasks.values(),
            return_exceptions=True
        )
        
        for (name, _), result in zip(group3_tasks.items(), group3_results):
            if isinstance(result, Exception):
                logger.warning(f"Group 3 analysis {name} failed: {result}")
                analysis_results[name] = {}
            else:
                analysis_results[name] = result
        
        execution_time = (datetime.now() - start_time).total_seconds()
        
        # ë™ì  ë¶„ì„ ê²°ê³¼ í†µí•© (ëŸ°íƒ€ì„ + í–‰ë™ + ì„±ëŠ¥)
        enhanced_dynamic = {
            **analysis_results.get("dynamic", {}),
            "behavior_analysis": analysis_results.get("behavior", {}),
            "performance_profile": analysis_results.get("performance", {}),
            "execution_metrics": {
                "analysis_time": execution_time,
                "execution_groups": 3,
                "total_analyses": 10,
                "successful_analyses": sum(1 for k, v in analysis_results.items() if v)
            }
        }
        
        # ì •ì  ë¶„ì„ ê²°ê³¼ ê°•í™” (í’ˆì§ˆ + ë³´ì•ˆ + ì˜ì¡´ì„±)
        enhanced_static = {
            **analysis_results.get("static", {}),
            "code_quality": analysis_results.get("quality", {}),
            "security_scan": analysis_results.get("security", {}),
            "dependency_analysis": analysis_results.get("dependencies", {}),
            "architecture": analysis_results.get("architecture", {})
        }
        
        # AI ìš”ì•½ ìƒì„±
        ai_summary = {
            "overview": "Comprehensive analysis completed",
            "key_findings": [
                f"Found {enhanced_static.get('total_files', 0)} files",
                f"Test coverage: {analysis_results.get('test', {}).get('coverage', 0)}%",
                f"Complexity hotspots: {enhanced_static.get('complexity_hotspots', 0)}"
            ],
            "recommendations": [
                "Improve test coverage",
                "Reduce complexity",
                "Optimize performance"
            ]
        }
        
        # í…ŒìŠ¤íŠ¸ ê°­ ì‹ë³„
        test_gaps = self._identify_test_gaps(
            analysis_results.get("test", {}),
            enhanced_static,
            enhanced_dynamic
        )
        
        # UX/í–‰ë™ ë©”íŠ¸ë¦­ ì¶”ì¶œ
        ux_metrics = self._extract_ux_metrics(
            analysis_results.get("behavior", {}),
            analysis_results.get("performance", {})
        )
        
        logger.info(f"Current state analysis complete in {execution_time:.2f}s: "
                   f"Files: {enhanced_static.get('total_files', 0)}, "
                   f"Coverage: {analysis_results.get('test', {}).get('coverage', 0)}%, "
                   f"Issues: {enhanced_static.get('total_issues', 0)}")
        
        return CurrentStateReport(
            static_analysis=enhanced_static,
            dynamic_analysis=enhanced_dynamic,
            ai_summary=ai_summary,
            contracts=analysis_results.get("contracts", {}),
            test_gaps=test_gaps,
            ux_metrics=ux_metrics
        )
    
    async def _run_static_analysis(self) -> Dict[str, Any]:
        """ì •ì  ë¶„ì„ ì‹¤í–‰."""
        task = AgentTask(
            task_id="static_analysis",
            intent="analyze_static",
            inputs={
                "path": self.config.project_path,
                "recursive": True
            }
        )
        result = await self.static_analyzer.execute(task)
        return result.data if result.success else {}
    
    async def _run_dynamic_analysis(self, architecture: Dict[str, Any] = None) -> Dict[str, Any]:
        """ë™ì  ë¶„ì„ ì‹¤í–‰."""
        if not self.config.enable_dynamic_analysis:
            return {"skipped": True, "reason": "Dynamic analysis disabled"}
        
        # ì•„í‚¤í…ì²˜ ì •ë³´ë¥¼ í™œìš©í•œ ë™ì  ë¶„ì„
        return {
            "runtime_metrics": {
                "memory_usage": 512,  # MB
                "cpu_usage": 45,  # %
                "network_io": 100,  # MB/s
                "disk_io": 50  # MB/s
            },
            "performance": {
                "avg_response_time": 250,
                "p95_latency": 800,
                "throughput": 1000
            },
            "architecture_runtime": architecture if architecture else {},
            "profiling_data": {}
        }
    
    async def _run_ai_summary(self) -> Dict[str, Any]:
        """AI ì•„í‚¤í…ì²˜ ìš”ì•½."""
        task = AgentTask(
            task_id="ai_summary",
            intent="analyze_architecture",
            inputs={
                "file_path": self.config.project_path,
                "analysis_type": "architecture"
            }
        )
        result = await self.code_analyzer.execute(task)
        return result.data if result.success else {}
    
    async def _mine_contracts(self) -> Dict[str, Any]:
        """ê³„ì•½/ì¸í„°í˜ì´ìŠ¤ ìë™ ì±„êµ´."""
        # API, ìŠ¤í‚¤ë§ˆ, ì´ë²¤íŠ¸, DB í…Œì´ë¸” ì¶”ì¶œ
        contracts = {
            "apis": [],
            "schemas": [],
            "events": [],
            "db_tables": []
        }
        
        # Static analyzerë¥¼ í†µí•´ ì¸í„°í˜ì´ìŠ¤ ì¶”ì¶œ
        task = AgentTask(
            task_id="mine_contracts",
            intent="extract_interfaces",
            inputs={
                "path": self.config.project_path,
                "extract_interfaces": True
            }
        )
        result = await self.static_analyzer.execute(task)
        
        if result.success:
            contracts.update(result.data.get("interfaces", {}))
        
        return contracts
    
    async def _find_test_gaps(self) -> List[Dict[str, Any]]:
        """í…ŒìŠ¤íŠ¸ ê°­ ì°¾ê¸°."""
        task = AgentTask(
            task_id="find_gaps",
            intent="analyze_gaps",
            inputs={
                "project_path": self.config.project_path,
                "min_coverage": 80
            }
        )
        result = await self.gap_analyzer.execute(task)
        return result.data.get("gaps", []) if result.success else []
    
    async def _collect_ux_metrics(self) -> Dict[str, Any]:
        """UX/í–‰ë™ ë°ì´í„° ìˆ˜ì§‘."""
        # ë¡œê·¸ì—ì„œ UX ë©”íŠ¸ë¦­ ì¶”ì¶œ
        metrics = {
            "conversion_rate": 0.0,
            "retention_rate": 0.0,
            "key_actions": [],
            "user_flows": []
        }
        
        if self.config.include_behavior_analysis:
            task = AgentTask(
                task_id="ux_metrics",
                intent="extract_metrics",
                inputs={
                    "log_paths": self._find_log_files(),
                    "extract_ux_metrics": True
                }
            )
            result = await self.behavior_analyzer.execute(task)
            if result.success:
                metrics.update(result.data.get("ux_metrics", {}))
        
        return metrics
    
    async def _conduct_external_research(
        self,
        evolution_goal: Optional[EvolutionGoalSpec],
        current_state: Optional[CurrentStateReport]
    ) -> UpgradeResearchPack:
        """ì™¸ë¶€ ë¦¬ì„œì¹˜ ìˆ˜í–‰ (URP ìƒì„±).
        
        upgrade.mdì˜ 4A) ì„¹ì…˜ êµ¬í˜„.
        
        Args:
            evolution_goal: ì§„í™” ëª©í‘œ ëª…ì„¸ì„œ
            current_state: í˜„ì¬ ìƒíƒœ ë³´ê³ ì„œ
            
        Returns:
            ì—…ê·¸ë ˆì´ë“œ ë¦¬ì„œì¹˜ íŒ©
        """
        logger.info("Conducting external research for URP...")
        
        # ExternalResearcher í†µí•©
        try:
            from backend.packages.agents.external_researcher import ExternalResearcher
            researcher = ExternalResearcher(memory_hub=self.memory_hub)
            use_real_researcher = True
        except Exception as e:
            logger.warning(f"Could not load ExternalResearcher: {e}. Using simulation mode.")
            use_real_researcher = False
        
        # ë¦¬ì„œì¹˜ ë°ì´í„° ì¤€ë¹„ - í˜„ì¬ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜
        research_data = {
            'project_context': self.config.project_path,
            'project_type': 'software',
            'primary_language': 'python',
            'identified_issues': [],
            'low_metrics': [],
            'security_vulnerabilities': [],
            'performance_issues': [],
            'outdated_dependencies': [],
            'architecture_issues': [],
            'improvement_goals': []
        }
        
        # í˜„ì¬ ìƒíƒœ ë¶„ì„ ê²°ê³¼ì—ì„œ ë¬¸ì œì  ì¶”ì¶œ
        if current_state and current_state.static_analysis:
            static = current_state.static_analysis
            
            # ì–¸ì–´ ì •ë³´
            languages = static.get('languages', {})
            if languages:
                research_data['primary_language'] = max(languages, key=languages.get)
            
            # ë³µì¡ë„ ë¬¸ì œ
            if static.get('complexity_hotspots', 0) > 10:
                research_data['identified_issues'].append({
                    'type': 'high_complexity',
                    'description': f"{static['complexity_hotspots']} complexity hotspots found"
                })
            
            # í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ë¬¸ì œ
            coverage_str = static.get('test_coverage_estimate', '0%')
            coverage = float(coverage_str.rstrip('%'))
            if coverage < 50:
                research_data['low_metrics'].append({
                    'name': 'test_coverage',
                    'value': coverage
                })
            
            # ë³´ì•ˆ ì´ìŠˆ
            if static.get('security_issues', 0) > 0:
                research_data['security_vulnerabilities'].append({
                    'type': 'static_analysis',
                    'component': 'codebase',
                    'severity': 'high' if static['security_issues'] > 5 else 'medium'
                })
        
        # ë™ì  ë¶„ì„ ê²°ê³¼ì—ì„œ ì„±ëŠ¥ ë¬¸ì œ ì¶”ì¶œ
        if current_state and current_state.dynamic_analysis:
            dynamic = current_state.dynamic_analysis
            if dynamic.get('performance'):
                perf = dynamic['performance']
                if perf.get('avg_response_time', 0) > 1000:  # 1ì´ˆ ì´ìƒ
                    research_data['performance_issues'].append({
                        'area': 'response_time',
                        'description': 'High average response time',
                        'current_metric': f"{perf['avg_response_time']}ms"
                    })
        
        # ì§„í™” ëª©í‘œì—ì„œ ê°œì„  ëª©í‘œ ì¶”ì¶œ
        if evolution_goal:
            # ì„±ê³µ ê¸°ì¤€ì„ ê°œì„  ëª©í‘œë¡œ ë³€í™˜
            if evolution_goal.success_criteria:
                for criteria, value in evolution_goal.success_criteria.items():
                    research_data['improvement_goals'].append(f"{criteria}: {value}")
            
            # ì—´ë¦° ì§ˆë¬¸ë“¤ì„ ì´ìŠˆë¡œ ì¶”ê°€
            for question in evolution_goal.open_questions[:3]:
                research_data['identified_issues'].append({
                    'type': 'open_question',
                    'description': question
                })
        
        # ë¦¬ì„œì¹˜ ìˆ˜í–‰
        if use_real_researcher:
            # ì‹¤ì œ ExternalResearcher ì‚¬ìš©
            research_task = AgentTask(
                type="external_research",
                intent="Conduct external research for upgrade planning",
                data=research_data
            )
            
            result = await researcher.execute(research_task)
            research_findings = result.data if result.status == "completed" else self._get_simulation_research()
        else:
            # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
            research_findings = self._get_simulation_research()
        
        # URP ìƒì„±
        urp = UpgradeResearchPack(
            one_line_conclusion=research_findings.get(
                'one_line_conclusion',
                'Research conducted for upgrade planning'
            ),
            recommended_approach={
                'name': 'Incremental Migration',
                'description': 'Gradual transition with feature flags',
                'pros': ['Lower risk', 'Rollback capability'],
                'cons': ['Longer timeline', 'Complexity overhead']
            },
            alternative_approaches=[
                {
                    'name': 'Big Bang Migration',
                    'description': 'Complete transition at once',
                    'pros': ['Faster completion', 'Simpler management'],
                    'cons': ['Higher risk', 'Difficult rollback']
                }
            ],
            compatibility_checklist=[
                {'item': 'API backward compatibility', 'status': 'pending'},
                {'item': 'Data schema compatibility', 'status': 'pending'},
                {'item': 'Client compatibility', 'status': 'pending'}
            ],
            migration_strategy={
                'approach': 'Expand-Contract',
                'phases': ['Add new', 'Migrate data', 'Remove old'],
                'estimated_duration': '2-4 weeks'
            },
            code_snippets=research_findings.get('code_snippets', []),
            warnings=research_findings.get('warnings', [
                'Ensure proper backup before migration',
                'Monitor performance during transition'
            ]),
            success_criteria={
                'metrics': ['Zero downtime', 'No data loss', 'Performance maintained'],
                'measurement': 'Automated monitoring and alerts'
            },
            cost_risk_summary={
                'cost': 'Medium',
                'risk': 'Low-Medium',
                'mitigation': 'Feature flags and gradual rollout'
            },
            dedup_results=research_findings.get('dedup_results', {
                'similar_projects': [],
                'reusable_components': []
            }),
            references=research_findings.get('sources', [
                'Internal documentation',
                'Industry best practices',
                'Security advisories'
            ])
        )
        
        # ë©”ëª¨ë¦¬ì— ì €ì¥
        if self.memory_hub:
            await self.memory_hub.put(
                ContextType.S_CTX,
                f"urp_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                asdict(urp),
                ttl_seconds=86400 * urp.ttl_days
            )
        
        # URP ë¬¸ì„œ ì €ì¥
        await self._save_urp_document(urp)
        
        return urp
    
    def _get_simulation_research(self) -> Dict[str, Any]:
        """ì‹œë®¬ë ˆì´ì…˜ ë¦¬ì„œì¹˜ ë°ì´í„° ë°˜í™˜."""
        return {
            'key_findings': [
                {'finding': 'Test coverage tools like pytest-cov can help achieve 80% coverage'},
                {'finding': 'Complexity can be reduced using design patterns'},
                {'finding': 'Performance optimization through caching and async operations'}
            ],
            'best_practices': [
                'Use TDD for new features',
                'Implement CI/CD pipeline',
                'Regular code reviews'
            ],
            'security_updates': [],
            'technology_trends': [
                {'name': 'AI-driven testing', 'adoption_rate': 0.3},
                {'name': 'Microservices architecture', 'adoption_rate': 0.6}
            ],
            'recommendations': [
                'Implement pytest-cov for coverage measurement',
                'Use radon for complexity analysis',
                'Apply SOLID principles'
            ],
            'action_items': [
                {'type': 'improvement', 'priority': 'high', 'description': 'Add unit tests for uncovered code'},
                {'type': 'improvement', 'priority': 'medium', 'description': 'Refactor complex functions'}
            ]
        }
    
    async def _save_urp_document(self, urp: UpgradeResearchPack) -> None:
        """URPë¥¼ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¡œ ì €ì¥.
        
        Args:
            urp: ì—…ê·¸ë ˆì´ë“œ ë¦¬ì„œì¹˜ íŒ©
        """
        # ë¬¸ì„œ ìƒì„±
        doc_content = f"""# Upgrade Research Pack (URP)
Generated: {urp.created_at}
TTL: {urp.ttl_days} days

## 1. í•œ ì¤„ ê²°ë¡ 
{urp.one_line_conclusion}

## 2. ì¶”ì²œ ì ‘ê·¼ë²•
### {urp.recommended_approach.get('name', 'Primary Approach')}
- **ì„¤ëª…**: {urp.recommended_approach.get('description', '')}
- **ì¥ì **: {', '.join(urp.recommended_approach.get('pros', []))}
- **ë‹¨ì **: {', '.join(urp.recommended_approach.get('cons', []))}

## 3. ëŒ€ì•ˆ ì ‘ê·¼ë²•
"""
        for i, alt in enumerate(urp.alternative_approaches, 1):
            doc_content += f"""
### ëŒ€ì•ˆ {i}: {alt.get('name', f'Alternative {i}')}
- **ì„¤ëª…**: {alt.get('description', '')}
- **ì¥ì **: {', '.join(alt.get('pros', []))}
- **ë‹¨ì **: {', '.join(alt.get('cons', []))}
"""

        doc_content += f"""
## 4. í˜¸í™˜ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸
"""
        for item in urp.compatibility_checklist:
            status = "âœ…" if item.get('status') == 'completed' else "â³"
            doc_content += f"- {status} {item.get('item', '')}\n"

        doc_content += f"""
## 5. ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ
- **ì ‘ê·¼ë²•**: {urp.migration_strategy.get('approach', '')}
- **ë‹¨ê³„**: {' â†’ '.join(urp.migration_strategy.get('phases', []))}
- **ì˜ˆìƒ ê¸°ê°„**: {urp.migration_strategy.get('estimated_duration', '')}

## 6. í•µì‹¬ ì½”ë“œ ìŠ¤ë‹ˆí«
"""
        for i, snippet in enumerate(urp.code_snippets[:3], 1):
            if isinstance(snippet, dict):
                doc_content += f"""
### ìŠ¤ë‹ˆí« {i}: {snippet.get('title', f'Snippet {i}')}
```{snippet.get('language', 'python')}
{snippet.get('code', '')}
```
**ì ìš© ìœ„ì¹˜**: {snippet.get('location', '')}
**ì£¼ì˜ì‚¬í•­**: {snippet.get('notes', '')}
"""

        doc_content += f"""
## 7. ì£¼ì˜ì‚¬í•­/í•¨ì •
"""
        for warning in urp.warnings:
            doc_content += f"- âš ï¸ {warning}\n"

        doc_content += f"""
## 8. ì„±ê³µ/ì‹¤íŒ¨ ê¸°ì¤€
- **ì§€í‘œ**: {', '.join(urp.success_criteria.get('metrics', []))}
- **ì¸¡ì • ë°©ë²•**: {urp.success_criteria.get('measurement', '')}

## 9. ë¹„ìš©/ë¦¬ìŠ¤í¬ ìš”ì•½
- **ë¹„ìš©**: {urp.cost_risk_summary.get('cost', '')}
- **ë¦¬ìŠ¤í¬**: {urp.cost_risk_summary.get('risk', '')}
- **ì™„í™” ì „ëµ**: {urp.cost_risk_summary.get('mitigation', '')}

## 10. De-dup ê²°ê³¼
- **ìœ ì‚¬ í”„ë¡œì íŠ¸**: {len(urp.dedup_results.get('similar_projects', []))}ê°œ
- **ì¬ì‚¬ìš© ê°€ëŠ¥ ì»´í¬ë„ŒíŠ¸**: {len(urp.dedup_results.get('reusable_components', []))}ê°œ

## 11. ì°¸ê³ ìë£Œ
"""
        for ref in urp.references:
            doc_content += f"- {ref}\n"

        # íŒŒì¼ ì €ì¥
        from pathlib import Path
        import os
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ë””ë ‰í† ë¦¬ ìƒì„±
        project_name = Path(self.config.project_path).name
        timestamp = datetime.now().isoformat()
        output_dir = Path(self.config.output_dir) / project_name / timestamp
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # URP ë¬¸ì„œ ì €ì¥
        urp_path = output_dir / "0_URP.md"
        with open(urp_path, 'w', encoding='utf-8') as f:
            f.write(doc_content)
        
        logger.info(f"URP document saved to {urp_path}")
        
        # ìƒì„¸ ë¦¬í¬íŠ¸ë„ ì €ì¥
        detailed_path = output_dir / "0_URP_detailed.json"
        import json
        with open(detailed_path, 'w', encoding='utf-8') as f:
            json.dump(asdict(urp), f, indent=2, default=str)
        logger.info(f"Detailed URP saved to {detailed_path}")
    
    async def _analyze_gaps_and_plan(
        self,
        goal: EvolutionGoalSpec,
        current: CurrentStateReport,
        research_pack: Optional[UpgradeResearchPack] = None
    ) -> GapReport:
        """ê°­ ë¶„ì„ ë° ê³„íš ìˆ˜ë¦½ (ë¦¬ì„œì¹˜ ê²°ê³¼ ë°˜ì˜).
        
        Args:
            goal: ëª©í‘œ ëª…ì„¸
            current: í˜„ì¬ ìƒíƒœ
            research_pack: ì™¸ë¶€ ë¦¬ì„œì¹˜ ê²°ê³¼ (ì„ íƒì )
            
        Returns:
            ê°­ ë³´ê³ ì„œ
        """
        # ê°­ ë¶„ì„ (ë¦¬ì„œì¹˜ ê²°ê³¼ í™œìš©)
        gaps = []
        
        # ë¦¬ì„œì¹˜ ê¸°ë°˜ ì•¡ì…˜ ì•„ì´í…œ ì¶”ê°€
        if research_pack:
            # ë³´ì•ˆ ì—…ë°ì´íŠ¸ê°€ ìˆìœ¼ë©´ ìµœìš°ì„ 
            for warning in research_pack.warnings:
                if 'security' in warning.lower() or 'vulnerability' in warning.lower():
                    gaps.append({
                        "type": "security",
                        "description": warning,
                        "priority": "critical",
                        "source": "external_research"
                    })
            
            # ì¶”ì²œ ì ‘ê·¼ë²• ê¸°ë°˜ ê°­ ì¶”ê°€
            if research_pack.recommended_approach:
                gaps.append({
                    "type": "strategy",
                    "description": f"Implement {research_pack.recommended_approach.get('name', 'recommended approach')}",
                    "priority": "high",
                    "details": research_pack.recommended_approach,
                    "source": "external_research"
                })
        
        # ê¸°ëŠ¥ ê°­
        if goal.change_scope.get("included"):
            for feature in goal.change_scope["included"]:
                gaps.append({
                    "type": "feature",
                    "description": f"Implement {feature}",
                    "priority": "high"
                })
        
        # ì„±ëŠ¥ ê°­
        if current.dynamic_analysis.get("performance"):
            perf = current.dynamic_analysis["performance"]
            if perf.get("p95_latency", 0) > 100:
                gaps.append({
                    "type": "performance",
                    "description": "Reduce P95 latency below 100ms",
                    "current": perf.get("p95_latency"),
                    "target": 100,
                    "priority": "medium"
                })
        
        # ë³´ì•ˆ ê°­
        if current.static_analysis.get("security_issues"):
            gaps.append({
                "type": "security",
                "description": "Fix security vulnerabilities",
                "count": len(current.static_analysis["security_issues"]),
                "priority": "critical"
            })
        
        # PlannerAgentë¥¼ ì‚¬ìš©í•œ ì‹¤í–‰ ê³„íš ìƒì„±
        logger.info("Creating execution plan with PlannerAgent...")
        
        # goalì˜ change_scopeì™€ success_criteriaë¥¼ requirementë¡œ ë³€í™˜
        requirement_text = f"""
        Background: {goal.background}
        Change Scope: {goal.change_scope}
        Success Criteria: {goal.success_criteria}
        Constraints: {goal.constraints}
        """
        
        planner_task = AgentTask(
            task_id="create_plan",
            intent="create_plan",
            inputs={
                "requirement": requirement_text,
                "context": {
                    "current_state": current.static_analysis,
                    "gaps": gaps,
                    "research": research_pack.__dict__ if research_pack else {}
                }
            }
        )
        
        plan_result = await self.planner_agent.execute(planner_task)
        execution_plan = plan_result.data.get("plan", {}) if plan_result.success else {}
        logger.info(f"PlannerAgent result: {plan_result.success}, phases: {len(execution_plan.get('phases', []))}")
        
        # TaskCreatorAgentë¥¼ ì‚¬ìš©í•œ íƒœìŠ¤í¬ ìƒì„±
        logger.info("Creating tasks with TaskCreatorAgent...")
        task_creator_task = AgentTask(
            task_id="create_tasks",
            intent="create_tasks",
            inputs={
                "plan": execution_plan,
                "requirement": requirement_text,
                "context": {"project_path": self.config.project_path}
            }
        )
        
        tasks_result = await self.task_creator_agent.execute(task_creator_task)
        executable_tasks = tasks_result.data.get("tasks", []) if tasks_result.success else []
        logger.info(f"TaskCreatorAgent result: {tasks_result.success}, tasks: {len(executable_tasks)}")
        
        # ì˜í–¥ë„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        task = AgentTask(
            task_id="impact_analysis",
            intent="analyze_impact",
            inputs={
                "project_path": self.config.project_path,
                "proposed_changes": gaps
            }
        )
        impact_result = await self.impact_analyzer.execute(task)
        
        impact_matrix = impact_result.data if impact_result.success else {}
        
        # ë¦¬ìŠ¤í¬ ìŠ¤ì½”ì–´ ê³„ì‚°
        risk_scores = {
            "complexity": self._calculate_complexity_risk(current),
            "impact": self._calculate_impact_risk(impact_matrix),
            "rollback": self._calculate_rollback_risk(gaps)
        }
        
        # ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ
        # ë¦¬ì„œì¹˜ ê¸°ë°˜ ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ ì„ íƒ
        if research_pack and research_pack.migration_strategy:
            migration_strategy = research_pack.migration_strategy.get('approach', 'expand_contract')
            migration_phases = research_pack.migration_strategy.get('phases', [])
            estimated_duration = research_pack.migration_strategy.get('estimated_duration', '4-6 weeks')
        else:
            migration_strategy = "expand_contract"
            migration_phases = ["Add new structure", "Shadow read/write", "Gradual migration", "Remove old structure"]
            estimated_duration = "4-6 weeks"
        
        migration_plan = {
            "strategy": migration_strategy,
            "phases": [
                {"phase": i+1, "action": phase, "duration": f"Week {i+1}"}
                for i, phase in enumerate(migration_phases)
            ],
            "estimated_duration": estimated_duration,
            "feature_flags": [f"feature_{i}" for i in range(len(gaps))],
            "canary_plan": {
                "stages": [1, 5, 25, 50, 100],
                "metrics": ["error_rate", "latency", "cpu_usage"]
            }
        }
        
        # ì‘ì€ ë°°ì¹˜ ê³„íš
        batch_plan = []
        for i, gap in enumerate(gaps):
            batch_plan.append({
                "batch": i + 1,
                "gap": gap,
                "estimated_time": "1-2 days",
                "dependencies": [],
                "rollback_plan": "Feature flag disable"
            })
        
        return GapReport(
            gaps=gaps,
            impact_matrix=impact_matrix,
            risk_scores=risk_scores,
            migration_plan=migration_plan,
            batch_plan=batch_plan
        )
    
    def _calculate_complexity_risk(self, current: CurrentStateReport) -> float:
        """ë³µì¡ë„ ë¦¬ìŠ¤í¬ ê³„ì‚°."""
        if current.ai_summary.get("complexity"):
            return min(current.ai_summary["complexity"] / 100, 1.0)
        return 0.5
    
    def _calculate_impact_risk(self, impact_matrix: Dict[str, Any]) -> float:
        """ì˜í–¥ë„ ë¦¬ìŠ¤í¬ ê³„ì‚°."""
        if impact_matrix.get("affected_components"):
            return min(len(impact_matrix["affected_components"]) / 20, 1.0)
        return 0.5
    
    def _calculate_rollback_risk(self, gaps: List[Dict[str, Any]]) -> float:
        """ë¡¤ë°± ë‚œì´ë„ ê³„ì‚°."""
        critical_gaps = [g for g in gaps if g.get("priority") == "critical"]
        return min(len(critical_gaps) / 5, 1.0)
    
    async def _store_report(self, report: UpgradeReport) -> None:
        """Store report in memory hub.
        
        Args:
            report: Report to store
        """
        if self.memory_hub:
            try:
                # Convert report to dict safely
                report_dict = {}
                for key, value in report.__dict__.items():
                    if value is None:
                        report_dict[key] = None
                    elif hasattr(value, '__dict__'):
                        # Convert nested dataclasses
                        try:
                            report_dict[key] = asdict(value)
                        except:
                            report_dict[key] = str(value)
                    elif isinstance(value, (list, dict, str, int, float, bool)):
                        report_dict[key] = value
                    else:
                        report_dict[key] = str(value)
                
                # Store main report
                await self.memory_hub.put(
                    ContextType.O_CTX,
                    f"upgrade_report_{report.timestamp}",
                    report_dict,
                    ttl_seconds=86400 * 30  # Keep for 30 days
                )
                
                # Store individual documents
                if report.evolution_goal:
                    await self.memory_hub.put(
                        ContextType.S_CTX,
                        "evolution_goal_latest",
                        asdict(report.evolution_goal),
                        ttl_seconds=86400 * 30
                    )
                
                if report.gap_report:
                    await self.memory_hub.put(
                        ContextType.S_CTX,
                        "gap_report_latest",
                        asdict(report.gap_report),
                        ttl_seconds=86400 * 30
                    )
            except Exception as e:
                logger.warning(f"Failed to store report in memory hub: {e}")
                # Don't fail the whole operation
            
            # Generate reports using ReportGenerator agent
            # await self._generate_reports_via_agent(report)  # ReportGenerator not implemented yet
    
    async def _generate_reports_via_agent(self, report: UpgradeReport) -> None:
        """ReportGenerator ì—ì´ì „íŠ¸ë¥¼ í†µí•´ ë¦¬í¬íŠ¸ ìƒì„±.
        
        Args:
            report: ì—…ê·¸ë ˆì´ë“œ ë¦¬í¬íŠ¸
        """
        # ReportGenerator ì—ì´ì „íŠ¸ ë™ì  ë¡œë“œ
        # from backend.packages.agents.report_generator import ReportGenerator  # Not implemented yet
        
        report_generator = ReportGenerator(memory_hub=self.memory_hub)
        
        # ë¬¸ì„œ ì €ì¥ ê²½ë¡œ í™•ì¸ ë° ìƒì„±
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # í”„ë¡œì íŠ¸ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        project_name = Path(self.config.project_path).name
        project_dir = output_dir / project_name / report.timestamp.replace(':', '-')
        project_dir.mkdir(parents=True, exist_ok=True)
        
        # ì§„í™” ëª©í‘œ ëª…ì„¸ì„œ ìƒì„± (ë„ì°©ì§€)
        if report.evolution_goal:
            await report_generator.execute(AgentTask(
                task_id="goal_report",
                intent="generate_report",
                inputs={
                    "report_data": {"evolution_goal": asdict(report.evolution_goal)},
                    "report_type": "evolution_goal",
                    "format": "markdown",
                    "output_path": str(project_dir / "1_Goal.md")
                }
            ))
            logger.info(f"Evolution Goal saved to {project_dir / '1_Goal.md'}")
        
        # í˜„ì¬ ìƒíƒœ ë³´ê³ ì„œ ìƒì„± (ì¶œë°œì§€)
        if report.current_state:
            # asdict ì•ˆì „í•˜ê²Œ í˜¸ì¶œ
            try:
                current_state_dict = asdict(report.current_state) if hasattr(report.current_state, '__dict__') else report.current_state.__dict__
            except:
                current_state_dict = {
                    "static_analysis": report.current_state.static_analysis if report.current_state else {},
                    "dynamic_analysis": report.current_state.dynamic_analysis if report.current_state else {},
                    "ai_summary": report.current_state.ai_summary if report.current_state else {},
                    "contracts": report.current_state.contracts if report.current_state else {},
                    "test_gaps": report.current_state.test_gaps if report.current_state else [],
                    "ux_metrics": report.current_state.ux_metrics if report.current_state else {}
                }
            
            await report_generator.execute(AgentTask(
                task_id="state_report",
                intent="generate_report",
                inputs={
                    "report_data": {"current_state": current_state_dict},
                    "report_type": "current_state",
                    "format": "markdown",
                    "output_path": str(project_dir / "2_CurrentState.md")
                }
            ))
            logger.info(f"Current State saved to {project_dir / '2_CurrentState.md'}")
        
        # ê°­ ë¶„ì„ ë° ì‹¤í–‰ ê³„íšì„œ ìƒì„±
        if report.gap_report:
            await report_generator.execute(AgentTask(
                task_id="plan_report",
                intent="generate_report",
                inputs={
                    "report_data": {"gap_report": asdict(report.gap_report)},
                    "report_type": "execution_plan",
                    "format": "markdown",
                    "output_path": str(project_dir / "3_Plan.md")
                }
            ))
            logger.info(f"Execution Plan saved to {project_dir / '3_Plan.md'}")
        
        # ì‘ì€ ì‘ì—… ë‹¨ìœ„ íƒœìŠ¤í¬ ëª©ë¡
        tasks_path = project_dir / "4_Tasks.json"
        with open(tasks_path, 'w', encoding='utf-8') as f:
            json.dump(report.tasks_breakdown, f, indent=2, ensure_ascii=False)
        logger.info(f"Task breakdown saved to {tasks_path}")
        
        # ì „ì²´ í†µí•© ë¦¬í¬íŠ¸ ìƒì„± (HTML)
        await report_generator.execute(AgentTask(
            task_id="full_report",
            intent="generate_report",
            inputs={
                "report_data": report.__dict__,
                "report_type": "upgrade",
                "format": "html",
                "output_path": str(project_dir / "FullReport.html")
            }
        ))
        logger.info(f"Full report saved to {project_dir / 'FullReport.html'}")
        
        # ê²°ê³¼ ìš”ì•½ íŒŒì¼ ìƒì„± (UIì—ì„œ ì‰½ê²Œ ì ‘ê·¼)
        summary = {
            "project": project_name,
            "timestamp": report.timestamp,
            "output_directory": str(project_dir),
            "files_generated": [
                "1_Goal.md",
                "2_CurrentState.md",
                "3_Plan.md",
                "4_Tasks.json",
                "FullReport.html"
            ],
            "health_score": report.system_health_score,
            "risk_score": report.upgrade_risk_score,
            "total_issues": report.total_issues_found,
            "critical_issues": len(report.critical_issues),
            "tasks_count": len(report.tasks_breakdown)
        }
        
        summary_path = project_dir / "summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2)
        logger.info(f"Summary saved to {summary_path}")
        
        # ë©”ëª¨ë¦¬ì—ë„ ê²½ë¡œ ì €ì¥ (UIì—ì„œ ì¡°íšŒ ê°€ëŠ¥)
        await self.memory_hub.put(
            ContextType.O_CTX,
            f"latest_report_path",
            str(project_dir),
            ttl_seconds=86400 * 7
        )
    
    async def _run_behavior_analysis(self, architecture: Dict[str, Any] = None) -> Dict[str, Any]:
        """í–‰ë™ ë¶„ì„ ì‹¤í–‰ (ë¡œê·¸ íŒ¨í„´, ì—ëŸ¬ íŒ¨í„´, ì‚¬ìš©ì í–‰ë™)."""
        if not self.config.include_behavior_analysis:
            return {"skipped": True, "reason": "Behavior analysis disabled"}
        
        task = AgentTask(
            task_id="behavior_analysis",
            intent="analyze_behavior",
            inputs={
                "log_paths": self._find_log_files(),
                "architecture": architecture or {}
            }
        )
        result = await self.behavior_analyzer.execute(task)
        return result.data if result.success else {}
    
    async def _run_performance_analysis(self, static_data: Dict[str, Any] = None, architecture: Dict[str, Any] = None) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰."""
        return {
            "avg_response_time": 250,  # ms
            "p95_latency": 800,
            "throughput": 1000,  # req/s
            "bottlenecks": [],
            "static_hints": static_data.get("complexity_hotspots", []) if static_data else [],
            "architecture_impact": architecture.get("layers", {}) if architecture else {}
        }
    
    async def _run_test_analysis(self, static_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ë¶„ì„ ì‹¤í–‰."""
        # ì •ì  ë¶„ì„ ë°ì´í„°ë¥¼ í™œìš©í•œ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ë¶„ì„
        code_files = static_data.get("total_files", 0) if static_data else 0
        test_files = static_data.get("test_files", 0) if static_data else 0
        
        coverage = 0
        if code_files > 0:
            coverage = min((test_files / code_files) * 100, 100)
        
        return {
            "coverage": coverage,
            "test_files": test_files,
            "code_files": code_files,
            "missing_tests": [],
            "test_quality": "moderate"
        }
    
    async def _run_code_quality_analysis(self, static_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """ì½”ë“œ í’ˆì§ˆ ë¶„ì„ ì‹¤í–‰."""
        # ì •ì  ë¶„ì„ ê¸°ë°˜ í’ˆì§ˆ í‰ê°€
        complexity = static_data.get("average_complexity", 10) if static_data else 10
        
        return {
            "linting_issues": 0,
            "type_coverage": 85,
            "code_smells": [],
            "complexity_score": complexity,
            "maintainability_index": max(0, 100 - complexity * 5)
        }
    
    async def _run_dependency_analysis(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ë¶„ì„ ì‹¤í–‰."""
        return {
            "total_dependencies": 0,
            "outdated": [],
            "vulnerable": [],
            "licenses": {},
            "dependency_graph": {}
        }
    
    async def _run_security_analysis(self, static_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """ë³´ì•ˆ ë¶„ì„ ì‹¤í–‰."""
        # ì •ì  ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³´ì•ˆ ì´ìŠˆ ì‹ë³„
        security_issues = []
        
        if static_data:
            # ì •ì  ë¶„ì„ì—ì„œ ë°œê²¬ëœ ë³´ì•ˆ ì´ìŠˆ
            if static_data.get("security_issues"):
                security_issues.extend(static_data["security_issues"])
        
        return {
            "vulnerabilities": security_issues,
            "severity_distribution": {
                "critical": 0,
                "high": len([i for i in security_issues if i.get("severity") == "HIGH"]),
                "medium": len([i for i in security_issues if i.get("severity") == "MEDIUM"]),
                "low": len([i for i in security_issues if i.get("severity") == "LOW"])
            },
            "owasp_top_10": [],
            "cve_matches": []
        }
    
    async def _analyze_architecture(self, static_data: Dict[str, Any] = None, dependency_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """ì•„í‚¤í…ì²˜ ë¶„ì„ ì‹¤í–‰."""
        layers = {}
        
        if static_data:
            # ì •ì  ë¶„ì„ì—ì„œ ë ˆì´ì–´ ì •ë³´ ì¶”ì¶œ
            layers = static_data.get("architecture_layers", {})
        
        if dependency_data:
            # ì˜ì¡´ì„± ê·¸ë˜í”„ë¡œ ì•„í‚¤í…ì²˜ ë³´ê°•
            layers["dependencies"] = dependency_data.get("dependency_graph", {})
        
        return {
            "pattern": "layered",  # or microservices, monolithic, etc.
            "layers": layers,
            "coupling": "moderate",
            "cohesion": "high",
            "circular_dependencies": 0
        }
    
    def _identify_test_gaps(self, test_data: Dict[str, Any], static_data: Dict[str, Any], dynamic_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """í…ŒìŠ¤íŠ¸ ê°­ ì‹ë³„."""
        gaps = []
        
        # ì»¤ë²„ë¦¬ì§€ ê°­
        coverage = test_data.get('coverage', 0)
        if coverage < 80:
            gaps.append({
                "type": "coverage",
                "current": coverage,
                "target": 80,
                "gap": 80 - coverage,
                "priority": "high" if coverage < 50 else "medium"
            })
        
        # ë³µì¡í•œ ì½”ë“œì˜ í…ŒìŠ¤íŠ¸ ë¶€ì¬
        if static_data.get('complexity_hotspots'):
            gaps.append({
                "type": "complex_code_untested",
                "count": static_data['complexity_hotspots'],
                "priority": "high"
            })
        
        # ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ë¶€ì¬
        if dynamic_data.get('behavior_analysis', {}).get('error_patterns'):
            gaps.append({
                "type": "error_handling",
                "untested_scenarios": len(dynamic_data['behavior_analysis']['error_patterns']),
                "priority": "medium"
            })
        
        return gaps
    
    def _extract_ux_metrics(self, behavior_data: Dict[str, Any], performance_data: Dict[str, Any]) -> Dict[str, Any]:
        """UX/í–‰ë™ ë©”íŠ¸ë¦­ ì¶”ì¶œ."""
        return {
            "response_time": performance_data.get('avg_response_time', 0),
            "error_rate": behavior_data.get('error_rate', 0),
            "user_flows": behavior_data.get('user_flows', []),
            "performance_score": max(0, 100 - (performance_data.get('p95_latency', 0) / 10))
        }
    
    # ===============================================
    # Evolution Loop êµ¬í˜„ - ê°­ì´ 0ì´ ë  ë•Œê¹Œì§€ ë°˜ë³µ
    # ===============================================
    
    async def execute_evolution_loop(
        self,
        requirements: str,
        max_iterations: Optional[int] = None
    ) -> EvolutionResult:
        """Evolution Loop ì‹¤í–‰ - ê°­ì´ í•´ì†Œë  ë•Œê¹Œì§€ ìë™ ì§„í™”.
        
        ì´ ë©”ì†Œë“œëŠ” ë‹¤ìŒ ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤:
        1. ìš”êµ¬ì‚¬í•­ ë¶„ì„
        2. í˜„ì¬ ìƒíƒœ ë¶„ì„ (5ê°œ ì—ì´ì „íŠ¸)
        3. ì™¸ë¶€ ë¦¬ì„œì¹˜
        4. ê°­ ë¶„ì„
        5. ê°­ì´ ìˆìœ¼ë©´:
           - Agnoë¡œ ìƒˆ ì—ì´ì „íŠ¸ ìƒì„±
           - CodeGeneratorë¡œ ì½”ë“œ êµ¬í˜„
           - í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        6. ê°­ì´ 0ì´ë©´ ì¢…ë£Œ
        
        Args:
            requirements: ìš”êµ¬ì‚¬í•­
            max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: config ê°’)
            
        Returns:
            EvolutionResult: ì§„í™” ê²°ê³¼
        """
        logger.info("=" * 80)
        logger.info("ğŸ§¬ Starting Evolution Loop")
        logger.info("=" * 80)
        
        start_time = datetime.now()
        max_iter = max_iterations or self.config.max_evolution_iterations
        
        result = EvolutionResult(
            success=False,
            iterations=0,
            final_gaps=[],
            agents_created=[],
            code_generated=0,
            tests_passed=0,
            tests_failed=0,
            evolution_history=[],
            convergence_rate=0.0,
            total_time=0.0
        )
        
        previous_gap_count = float('inf')
        
        for iteration in range(1, max_iter + 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ”„ Evolution Iteration {iteration}/{max_iter}")
            logger.info(f"{'='*60}")
            
            iteration_start = datetime.now()
            iteration_data = {
                "iteration": iteration,
                "start_time": iteration_start.isoformat(),
                "gaps_before": [],
                "gaps_after": [],
                "actions_taken": [],
                "results": {}
            }
            
            try:
                # ìƒˆ ë£¨í”„ ì‹œì‘ - SharedDocumentContext ì´ˆê¸°í™”
                if self.document_context:
                    self.document_context.start_new_loop()
                
                # AI ë“œë¦¬ë¸ ì›Œí¬í”Œë¡œìš° ì‚¬ìš© ì—¬ë¶€ í™•ì¸
                if self.config.ai_driven_workflow and self.document_context:
                    # Step 1: AI ë“œë¦¬ë¸ ë™ì  ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                    logger.info("ğŸ¤– Using AI-driven dynamic workflow...")
                    workflow_results = await self._execute_dynamic_workflow(requirements, iteration)
                    
                    # ì›Œí¬í”Œë¡œìš° ê²°ê³¼ë¥¼ ë¦¬í¬íŠ¸ë¡œ ë³€í™˜
                    report = UpgradeReport(
                        timestamp=datetime.now().isoformat(),
                        success=True,
                        phase_results=workflow_results
                    )
                    
                    # ê°­ ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
                    gap_data = workflow_results.get("GapAnalyzer_step1", {})
                    if gap_data and "gaps" in gap_data:
                        report.gap_report = GapAnalysisReport(
                            gaps=gap_data["gaps"],
                            gap_score=gap_data.get("gap_score", 0),
                            priority_gaps=gap_data.get("priority_gaps", []),
                            execution_plan=gap_data.get("execution_plan", {})
                        )
                else:
                    # Step 1: í‘œì¤€ ë¶„ì„ ì‹¤í–‰
                    logger.info("ğŸ“Š Running standard complete analysis...")
                    report = await self.analyze(requirements, include_research=True)
                
                # Step 2: ê°­ ì¶”ì¶œ
                current_gaps = []
                if report.gap_report and report.gap_report.gaps:
                    current_gaps = report.gap_report.gaps
                    logger.info(f"ğŸ“ Found {len(current_gaps)} gaps")
                else:
                    logger.info("âœ… No gaps found - Evolution complete!")
                    result.success = True
                    result.iterations = iteration
                    break
                
                iteration_data["gaps_before"] = current_gaps
                
                # Step 3: ìˆ˜ë ´ ì²´í¬
                gap_reduction = (previous_gap_count - len(current_gaps)) / max(previous_gap_count, 1)
                result.convergence_rate = gap_reduction
                
                if len(current_gaps) == 0:
                    logger.info("ğŸ‰ All gaps resolved!")
                    result.success = True
                    result.iterations = iteration
                    break
                
                if gap_reduction < 0.1 and iteration > 3:
                    logger.warning("âš ï¸ Evolution stalled - minimal progress")
                    iteration_data["actions_taken"].append("Evolution stalled")
                    break
                
                # Step 4: ê°­ ì²˜ë¦¬
                for gap in current_gaps[:5]:  # í•œ ë²ˆì— ìµœëŒ€ 5ê°œ ê°­ ì²˜ë¦¬
                    action = await self._process_evolution_gap(gap, iteration_data)
                    if action:
                        iteration_data["actions_taken"].append(action)
                        if "agent_created" in action:
                            result.agents_created.append(action["agent_created"])
                        if "code_lines" in action:
                            result.code_generated += action["code_lines"]
                
                # Step 5: í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                if self.config.auto_implement_code:
                    test_results = await self._run_evolution_tests()
                    result.tests_passed += test_results.get("passed", 0)
                    result.tests_failed += test_results.get("failed", 0)
                    iteration_data["results"]["tests"] = test_results
                
                # Step 6: ì¬ë¶„ì„í•˜ì—¬ ê°­ í™•ì¸
                logger.info("ğŸ” Re-analyzing to check gap resolution...")
                updated_report = await self.analyze(requirements, include_research=False)
                
                if updated_report.gap_report:
                    iteration_data["gaps_after"] = updated_report.gap_report.gaps
                    current_gaps = updated_report.gap_report.gaps
                else:
                    iteration_data["gaps_after"] = []
                    current_gaps = []
                
                previous_gap_count = len(current_gaps)
                
            except Exception as e:
                logger.error(f"âŒ Evolution iteration {iteration} failed: {e}")
                iteration_data["error"] = str(e)
                iteration_data["actions_taken"].append(f"Error: {e}")
            
            finally:
                iteration_end = datetime.now()
                iteration_data["end_time"] = iteration_end.isoformat()
                iteration_data["duration"] = (iteration_end - iteration_start).total_seconds()
                result.evolution_history.append(iteration_data)
                
                logger.info(f"ğŸ“ˆ Iteration {iteration} complete:")
                logger.info(f"   Gaps: {len(iteration_data['gaps_before'])} â†’ {len(iteration_data['gaps_after'])}")
                logger.info(f"   Duration: {iteration_data['duration']:.2f}s")
        
        # ìµœì¢… ê²°ê³¼ ì •ë¦¬
        end_time = datetime.now()
        result.total_time = (end_time - start_time).total_seconds()
        result.final_gaps = current_gaps if 'current_gaps' in locals() else []
        
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ§¬ Evolution Loop Complete")
        logger.info("=" * 80)
        logger.info(f"Success: {result.success}")
        logger.info(f"Iterations: {result.iterations}")
        logger.info(f"Final gaps: {len(result.final_gaps)}")
        logger.info(f"Agents created: {len(result.agents_created)}")
        logger.info(f"Code generated: {result.code_generated} lines")
        logger.info(f"Tests: {result.tests_passed} passed, {result.tests_failed} failed")
        logger.info(f"Total time: {result.total_time:.2f}s")
        
        return result
    
    async def _process_evolution_gap(
        self,
        gap: Dict[str, Any],
        iteration_data: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """Evolution Gap ì²˜ë¦¬ - Agnoì™€ CodeGenerator í™œìš©.
        
        Args:
            gap: ì²˜ë¦¬í•  ê°­
            iteration_data: ë°˜ë³µ ë°ì´í„°
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë˜ëŠ” None
        """
        action = {"gap": gap.get("description", "Unknown gap")}
        
        try:
            gap_type = gap.get("type", "unknown")
            
            # ìƒˆ ì—ì´ì „íŠ¸ê°€ í•„ìš”í•œ ê²½ìš°
            if gap_type == "missing_agent" and self.config.auto_generate_agents:
                logger.info(f"ğŸ¤– Creating new agent for: {gap.get('description')}")
                
                if self.agno_manager:
                    # Agnoë¡œ ì—ì´ì „íŠ¸ ìƒì„±
                    agent_result = await self.agno_manager.create_agent(
                        requirements=gap.get("requirements", {}),
                        auto_implement=True,
                        force_create=False
                    )
                    
                    if agent_result.get("status") != "duplicate_found":
                        action["agent_created"] = agent_result.get("spec", {}).get("name", "Unknown")
                        action["code_lines"] = len(agent_result.get("implementation", {}).get("agent.py", "").split("\n"))
                        logger.info(f"âœ… Agent created: {action['agent_created']}")
                    else:
                        action["reused_agent"] = agent_result.get("suggestion", "Use existing")
                        logger.info(f"â™»ï¸ Reusing existing agent: {action['reused_agent']}")
            
            # ì½”ë“œ êµ¬í˜„ì´ í•„ìš”í•œ ê²½ìš°
            elif gap_type in ["missing_implementation", "incomplete_feature"] and self.config.auto_implement_code:
                logger.info(f"ğŸ’» Implementing code for: {gap.get('description')}")
                
                if self.code_generator:
                    # CodeGeneratorë¡œ ì½”ë“œ êµ¬í˜„
                    task = AgentTask(
                        intent="generate_code",
                        inputs={
                            "gap": gap,
                            "requirements": gap.get("requirements", {}),
                            "context": gap.get("context", {})
                        }
                    )
                    
                    code_result = await self.code_generator.execute(task)
                    if code_result.success:
                        action["code_implemented"] = True
                        action["code_lines"] = code_result.data.get("lines_generated", 0)
                        logger.info(f"âœ… Code implemented: {action['code_lines']} lines")
            
            # í…ŒìŠ¤íŠ¸ê°€ í•„ìš”í•œ ê²½ìš°
            elif gap_type == "missing_tests":
                logger.info(f"ğŸ§ª Generating tests for: {gap.get('description')}")
                
                if self.code_generator:
                    task = AgentTask(
                        intent="generate_tests",
                        inputs={"target": gap.get("target", {})}
                    )
                    
                    test_result = await self.code_generator.execute(task)
                    if test_result.success:
                        action["tests_generated"] = test_result.data.get("test_count", 0)
                        logger.info(f"âœ… Tests generated: {action['tests_generated']}")
            
            return action
            
        except Exception as e:
            logger.error(f"Failed to process gap: {e}")
            action["error"] = str(e)
            return action
    
    async def _run_evolution_tests(self) -> Dict[str, int]:
        """Evolution í…ŒìŠ¤íŠ¸ ì‹¤í–‰.
        
        Returns:
            í…ŒìŠ¤íŠ¸ ê²°ê³¼ (passed, failed ìˆ˜)
        """
        try:
            if self.quality_gate:
                task = AgentTask(
                    intent="run_tests",
                    inputs={"project_path": self.config.project_path}
                )
                
                result = await self.quality_gate.execute(task)
                if result.success:
                    return {
                        "passed": result.data.get("tests_passed", 0),
                        "failed": result.data.get("tests_failed", 0)
                    }
            
            return {"passed": 0, "failed": 0}
            
        except Exception as e:
            logger.error(f"Test execution failed: {e}")
            return {"passed": 0, "failed": 0}
    
    async def _execute_requirement_analysis(self, requirements: str) -> Dict[str, Any]:
        """ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì‹¤í–‰"""
        logger.info("Executing RequirementAnalyzer...")
        task = AgentTask(
            task_id="requirement_analysis",
            intent="analyze_requirements",
            inputs={"requirements": requirements}
        )
        result = await self.requirement_analyzer.execute(task)
        return result.data if result.success else {}
    
    async def _execute_current_state_analysis(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ë¶„ì„ (ë³‘ë ¬ ì‹¤í–‰)"""
        logger.info("Executing current state analysis in parallel...")
        
        tasks = {
            "static": self._run_static_analysis(),
            "code": self._run_code_analysis(),
            "behavior": self._run_behavior_analysis() if self.config.include_behavior_analysis else asyncio.create_task(asyncio.sleep(0)),
            "impact": self._run_impact_analysis() if self.config.generate_impact_matrix else asyncio.create_task(asyncio.sleep(0)),
            "quality": self._run_quality_analysis()
        }
        
        results = await asyncio.gather(
            *tasks.values(),
            return_exceptions=True
        )
        
        analysis_results = {}
        for (name, _), result in zip(tasks.items(), results):
            if isinstance(result, Exception):
                logger.warning(f"Analysis {name} failed: {result}")
                analysis_results[name] = {}
            elif result is not None:
                analysis_results[name] = result
        
        return analysis_results
    
    async def _run_code_analysis(self) -> Dict[str, Any]:
        """ì½”ë“œ ë¶„ì„ ì‹¤í–‰"""
        task = AgentTask(
            task_id="code_analysis",
            intent="analyze_code",
            inputs={"file_path": self.config.project_path}
        )
        result = await self.code_analyzer.execute(task)
        return result.data if result.success else {}
    
    async def _run_behavior_analysis(self) -> Dict[str, Any]:
        """í–‰ë™ ë¶„ì„ ì‹¤í–‰"""
        task = AgentTask(
            task_id="behavior_analysis",
            intent="analyze_behavior",
            inputs={"log_paths": self._find_log_files()}
        )
        result = await self.behavior_analyzer.execute(task)
        return result.data if result.success else {}
    
    async def _run_impact_analysis(self) -> Dict[str, Any]:
        """ì˜í–¥ë„ ë¶„ì„ ì‹¤í–‰"""
        task = AgentTask(
            task_id="impact_analysis",
            intent="analyze_impact",
            inputs={"project_path": self.config.project_path}
        )
        result = await self.impact_analyzer.execute(task)
        return result.data if result.success else {}
    
    async def _run_quality_analysis(self) -> Dict[str, Any]:
        """í’ˆì§ˆ ë¶„ì„ ì‹¤í–‰"""
        task = AgentTask(
            task_id="quality_analysis",
            intent="check_quality",
            inputs={"project_path": self.config.project_path}
        )
        result = await self.quality_gate.execute(task)
        return result.data if result.success else {}
    
    async def _execute_external_research(
        self,
        requirement_result: Dict[str, Any],
        current_state_results: Dict[str, Any]
    ) -> Optional[UpgradeResearchPack]:
        """ì™¸ë¶€ ë¦¬ì„œì¹˜ ì‹¤í–‰"""
        logger.info("Executing ExternalResearcher...")
        
        task = AgentTask(
            task_id="external_research",
            intent="research",
            inputs={
                "requirements": requirement_result,
                "current_state": current_state_results,
                "research_depth": "comprehensive"
            }
        )
        
        result = await self.external_researcher.execute(task)
        if result.success:
            # Convert to UpgradeResearchPack
            data = result.data
            return UpgradeResearchPack(
                one_line_conclusion=data.get("conclusion", ""),
                recommended_approach=data.get("recommended", {}),
                alternative_approaches=data.get("alternatives", []),
                compatibility_checklist=data.get("compatibility", []),
                migration_strategy=data.get("migration", {}),
                code_snippets=data.get("snippets", []),
                warnings=data.get("warnings", []),
                success_criteria=data.get("success_criteria", {}),
                cost_risk_summary=data.get("cost_risk", {}),
                dedup_results=data.get("dedup", {}),
                references=data.get("references", [])
            )
        return None
    
    async def _execute_dynamic_workflow(
        self,
        requirements: str,
        iteration: int = 1
    ) -> Dict[str, Any]:
        """AI ë“œë¦¬ë¸ ë™ì  ì›Œí¬í”Œë¡œìš° ì‹¤í–‰.
        
        SharedDocumentContextì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ AIê°€ ë‹¤ìŒ ì‹¤í–‰í•  ì—ì´ì „íŠ¸ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
        ì´ë¥¼ í†µí•´ ì§„ì •í•œ AI ë“œë¦¬ë¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        
        Args:
            requirements: ìš”êµ¬ì‚¬í•­
            iteration: í˜„ì¬ ë°˜ë³µ ë²ˆí˜¸
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼
        """
        logger.info(f"ğŸ¤– Executing AI-driven dynamic workflow (iteration {iteration})")
        
        if not self.document_context:
            logger.warning("No document context available, falling back to standard workflow")
            return await self._execute_standard_workflow(requirements)
        
        # AI Provider ì¤€ë¹„
        from backend.packages.agents.ai_providers import BedrockAIProvider
        ai_provider = BedrockAIProvider(
            model="claude-3-sonnet",
            region="us-east-1"
        )
        
        # í˜„ì¬ê¹Œì§€ì˜ ëª¨ë“  ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        context_for_ai = self.document_context.get_context_for_ai(
            include_history=True,
            max_history_loops=2
        )
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ëª©ë¡
        available_agents = [
            "RequirementAnalyzer",
            "StaticAnalyzer", 
            "CodeAnalysisAgent",
            "BehaviorAnalyzer",
            "ImpactAnalyzer",
            "QualityGate",
            "ExternalResearcher",
            "GapAnalyzer",
            "SystemArchitect",
            "OrchestratorDesigner",
            "PlannerAgent",
            "TaskCreatorAgent",
            "CodeGenerator"
        ]
        
        workflow_results = {}
        max_steps = 10  # í•œ iteration ë‚´ ìµœëŒ€ ì‹¤í–‰ ë‹¨ê³„
        
        for step in range(1, max_steps + 1):
            # AIì—ê²Œ ë‹¤ìŒ ì—ì´ì „íŠ¸ ì„ íƒ ìš”ì²­
            prompt = f"""
            You are an AI orchestrator managing an evolution loop for project upgrade.
            Current iteration: {iteration}, Step: {step}
            
            Requirements: {requirements}
            
            Available agents: {available_agents}
            
            Current loop documents:
            {context_for_ai}
            
            Based on the current state and documents, determine:
            1. Which agent(s) should execute next (can be multiple for parallel execution)
            2. Whether we should continue or stop this iteration
            3. Reasoning for your decision
            
            Rules:
            - RequirementAnalyzer must run first if no requirement analysis exists
            - Some agents can run in parallel (e.g., StaticAnalyzer, CodeAnalysisAgent, BehaviorAnalyzer)
            - GapAnalyzer needs requirement and state analysis results
            - Stop when gaps are resolved or no meaningful progress can be made
            
            Return JSON:
            {{
                "continue": true/false,
                "next_agents": ["agent1", "agent2"],
                "parallel": true/false,
                "reasoning": "explanation",
                "expected_outcome": "what we expect from these agents"
            }}
            """
            
            try:
                response = await ai_provider.complete(prompt)
                decision = json.loads(response)
                
                logger.info(f"AI Decision - Step {step}: {decision.get('reasoning', 'No reasoning')}")
                
                if not decision.get("continue", False):
                    logger.info("AI decided to stop the iteration")
                    break
                
                next_agents = decision.get("next_agents", [])
                is_parallel = decision.get("parallel", False)
                
                if not next_agents:
                    logger.warning("AI returned no agents to execute, stopping")
                    break
                
                # ì—ì´ì „íŠ¸ ì‹¤í–‰
                if is_parallel and len(next_agents) > 1:
                    logger.info(f"Executing agents in parallel: {next_agents}")
                    tasks = []
                    for agent_name in next_agents:
                        task = self._execute_single_agent(agent_name, requirements)
                        tasks.append(task)
                    
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    for agent_name, result in zip(next_agents, results):
                        if isinstance(result, Exception):
                            logger.error(f"Agent {agent_name} failed: {result}")
                            workflow_results[f"{agent_name}_step{step}"] = {"error": str(result)}
                        else:
                            workflow_results[f"{agent_name}_step{step}"] = result
                            # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                            if result and self.document_context:
                                self.document_context.add_document(
                                    agent_name,
                                    result,
                                    document_type="analysis"
                                )
                else:
                    # ìˆœì°¨ ì‹¤í–‰
                    for agent_name in next_agents:
                        logger.info(f"Executing agent: {agent_name}")
                        result = await self._execute_single_agent(agent_name, requirements)
                        workflow_results[f"{agent_name}_step{step}"] = result
                        
                        # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                        if result and self.document_context:
                            self.document_context.add_document(
                                agent_name,
                                result,
                                document_type="analysis"
                            )
                
            except Exception as e:
                logger.error(f"Dynamic workflow step {step} failed: {e}")
                break
        
        logger.info(f"Dynamic workflow completed with {len(workflow_results)} agent executions")
        return workflow_results
    
    async def _execute_single_agent(self, agent_name: str, requirements: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ì—ì´ì „íŠ¸ ì‹¤í–‰.
        
        Args:
            agent_name: ì‹¤í–‰í•  ì—ì´ì „íŠ¸ ì´ë¦„
            requirements: ìš”êµ¬ì‚¬í•­
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼
        """
        try:
            # ì—ì´ì „íŠ¸ë³„ ì‹¤í–‰ ë¡œì§
            if agent_name == "RequirementAnalyzer":
                return await self._execute_requirement_analysis(requirements)
            elif agent_name == "StaticAnalyzer":
                return await self._run_static_analysis()
            elif agent_name == "CodeAnalysisAgent":
                return await self._run_code_analysis()
            elif agent_name == "BehaviorAnalyzer":
                return await self._run_behavior_analysis()
            elif agent_name == "ImpactAnalyzer":
                return await self._run_impact_analysis()
            elif agent_name == "QualityGate":
                return await self._run_quality_analysis()
            elif agent_name == "ExternalResearcher":
                req_result = self.document_context.get_document("RequirementAnalyzer") if self.document_context else {}
                state_results = self.document_context.get_all_documents() if self.document_context else {}
                research = await self._execute_external_research(
                    req_result.get("content", {}) if req_result else {},
                    state_results
                )
                return asdict(research) if research else {}
            elif agent_name == "GapAnalyzer":
                docs = self.document_context.get_all_documents() if self.document_context else {}
                return await self._execute_gap_analysis(
                    docs.get("RequirementAnalyzer", {}).get("content", {}),
                    docs,
                    None  # Research pack will be in documents
                )
            elif agent_name == "SystemArchitect":
                return await self._execute_architecture_design()
            elif agent_name == "OrchestratorDesigner":
                return await self._execute_orchestrator_design()
            elif agent_name == "PlannerAgent":
                return await self._execute_planning()
            elif agent_name == "TaskCreatorAgent":
                return await self._execute_task_creation()
            elif agent_name == "CodeGenerator":
                return await self._execute_code_generation_batch()
            else:
                logger.warning(f"Unknown agent: {agent_name}")
                return {"error": f"Unknown agent: {agent_name}"}
                
        except Exception as e:
            logger.error(f"Failed to execute agent {agent_name}: {e}")
            return {"error": str(e)}
    
    async def _execute_standard_workflow(self, requirements: str) -> Dict[str, Any]:
        """í‘œì¤€ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (fallback).
        
        Args:
            requirements: ìš”êµ¬ì‚¬í•­
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼
        """
        logger.info("Executing standard workflow...")
        
        results = {}
        
        # 1. ìš”êµ¬ì‚¬í•­ ë¶„ì„
        results["requirement"] = await self._execute_requirement_analysis(requirements)
        
        # 2. í˜„ì¬ ìƒíƒœ ë¶„ì„ (ë³‘ë ¬)
        state_results = await self._execute_current_state_analysis()
        results["current_state"] = state_results
        
        # 3. ì™¸ë¶€ ë¦¬ì„œì¹˜
        research = await self._execute_external_research(
            results["requirement"],
            state_results
        )
        if research:
            results["research"] = asdict(research)
        
        # 4. ê°­ ë¶„ì„
        results["gap"] = await self._execute_gap_analysis(
            results["requirement"],
            state_results,
            research
        )
        
        return results
    
    async def _execute_architecture_design(self) -> Dict[str, Any]:
        """ì•„í‚¤í…ì²˜ ì„¤ê³„ ì‹¤í–‰"""
        if not self.system_architect:
            return {"error": "SystemArchitect not initialized"}
        
        task = AgentTask(
            task_id="architecture_design",
            intent="design_architecture",
            inputs={
                "documents": self.document_context.get_all_documents() if self.document_context else {}
            }
        )
        result = await self.system_architect.execute(task)
        return result.data if result.success else {}
    
    async def _execute_orchestrator_design(self) -> Dict[str, Any]:
        """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„¤ê³„ ì‹¤í–‰"""
        if not self.orchestrator_designer:
            return {"error": "OrchestratorDesigner not initialized"}
        
        task = AgentTask(
            task_id="orchestrator_design",
            intent="design_orchestrator",
            inputs={
                "documents": self.document_context.get_all_documents() if self.document_context else {}
            }
        )
        result = await self.orchestrator_designer.execute(task)
        return result.data if result.success else {}
    
    async def _execute_planning(self) -> Dict[str, Any]:
        """ê³„íš ìˆ˜ë¦½ ì‹¤í–‰"""
        if not self.planner_agent:
            return {"error": "PlannerAgent not initialized"}
        
        task = AgentTask(
            task_id="planning",
            intent="create_plan",
            inputs={
                "documents": self.document_context.get_all_documents() if self.document_context else {}
            }
        )
        result = await self.planner_agent.execute(task)
        return result.data if result.success else {}
    
    async def _execute_task_creation(self) -> Dict[str, Any]:
        """íƒœìŠ¤í¬ ìƒì„± ì‹¤í–‰"""
        if not self.task_creator_agent:
            return {"error": "TaskCreatorAgent not initialized"}
        
        task = AgentTask(
            task_id="task_creation",
            intent="create_tasks",
            inputs={
                "documents": self.document_context.get_all_documents() if self.document_context else {}
            }
        )
        result = await self.task_creator_agent.execute(task)
        return result.data if result.success else {}
    
    async def _execute_code_generation_batch(self) -> Dict[str, Any]:
        """ì½”ë“œ ìƒì„± ë°°ì¹˜ ì‹¤í–‰"""
        if not self.code_generator:
            return {"error": "CodeGenerator not initialized"}
        
        # íƒœìŠ¤í¬ ëª©ë¡ì—ì„œ ì½”ë“œ ìƒì„±ì´ í•„ìš”í•œ ê²ƒë“¤ ì¶”ì¶œ
        tasks_doc = self.document_context.get_document("TaskCreatorAgent") if self.document_context else None
        if not tasks_doc:
            return {"error": "No tasks available for code generation"}
        
        tasks = tasks_doc.get("content", {}).get("tasks", [])
        code_tasks = [t for t in tasks if t.get("type") in ["code_generation", "code_modification"]]
        
        results = []
        for code_task in code_tasks[:5]:  # ìµœëŒ€ 5ê°œì”© ì²˜ë¦¬
            task = AgentTask(
                task_id=f"code_gen_{code_task.get('id', 'unknown')}",
                intent="generate_code",
                inputs=code_task
            )
            result = await self.code_generator.execute(task)
            results.append(result.data if result.success else {"error": "Failed"})
        
        return {
            "generated_count": len(results),
            "results": results
        }

    async def _execute_gap_analysis(
        self,
        requirement_result: Dict[str, Any],
        current_state_results: Dict[str, Any],
        research_pack: Optional[UpgradeResearchPack]
    ) -> Dict[str, Any]:
        """ê°­ ë¶„ì„ ì‹¤í–‰"""
        logger.info("Executing GapAnalyzer...")
        
        task = AgentTask(
            task_id="gap_analysis",
            intent="analyze_gaps",
            inputs={
                "requirements": requirement_result,
                "current_state": current_state_results,
                "research": research_pack.__dict__ if research_pack else None
            }
        )
        
        result = await self.gap_analyzer.execute(task)
        return result.data if result.success else {}
    
    async def _execute_architecture_design(
        self,
        requirement_result: Dict[str, Any],
        gap_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì•„í‚¤í…ì²˜ ì„¤ê³„ ì‹¤í–‰"""
        logger.info("Executing SystemArchitect...")
        
        design = await self.system_architect.design_architecture(
            requirements=requirement_result,
            gap_report=gap_result,
            current_architecture=None,
            constraints={"max_complexity": 10}
        )
        
        return design.dict()
    
    async def _execute_orchestrator_design(
        self,
        architecture_design: Dict[str, Any],
        requirement_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë””ìì¸ ì‹¤í–‰"""
        logger.info("Executing OrchestratorDesigner...")
        
        design_doc = await self.orchestrator_designer.design_orchestrator(
            architecture_design=architecture_design,
            requirements=requirement_result,
            constraints={"timeout": 3600}
        )
        
        return design_doc.dict()
    
    async def _execute_planning(
        self,
        architecture_design: Dict[str, Any],
        orchestrator_design: Dict[str, Any],
        requirement_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì‹¤í–‰ ê³„íš ìˆ˜ë¦½"""
        logger.info("Executing PlannerAgent...")
        
        task = AgentTask(
            task_id="create_plan",
            intent="create_plan",
            inputs={
                "requirement": requirement_result,
                "architecture": architecture_design,
                "orchestrator": orchestrator_design
            }
        )
        
        result = await self.planner_agent.execute(task)
        return result.data if result.success else {}
    
    async def _execute_task_creation(
        self,
        execution_plan: Dict[str, Any],
        orchestrator_design: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ì„¸ë¶€ íƒœìŠ¤í¬ ìƒì„±"""
        logger.info("Executing TaskCreatorAgent...")
        
        task = AgentTask(
            task_id="create_tasks",
            intent="create_tasks",
            inputs={
                "plan": execution_plan,
                "orchestrator": orchestrator_design,
                "task_duration": "5-20 minutes"
            }
        )
        
        result = await self.task_creator_agent.execute(task)
        return result.data.get("tasks", []) if result.success else []
    
    async def _execute_code_generation(
        self,
        detailed_tasks: List[Dict[str, Any]],
        architecture_design: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì½”ë“œ ìƒì„± ì‹¤í–‰"""
        logger.info("Executing CodeGenerator...")
        
        task = AgentTask(
            task_id="generate_code",
            intent="generate_code",
            inputs={
                "tasks": detailed_tasks,
                "architecture": architecture_design
            }
        )
        
        result = await self.code_generator.execute(task)
        return result.data if result.success else {}
    
    async def _generate_agents_with_agno(
        self,
        orchestrator_design: Dict[str, Any],
        gaps: List[Dict[str, Any]]
    ) -> List[str]:
        """Agnoë¥¼ í†µí•œ ìë™ ì—ì´ì „íŠ¸ ìƒì„±.
        
        ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë””ìì¸ê³¼ ê°­ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ
        í•„ìš”í•œ ì—ì´ì „íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            orchestrator_design: ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë””ìì¸ ë¬¸ì„œ
            gaps: í•´ê²°í•´ì•¼ í•  ê°­ ëª©ë¡
            
        Returns:
            ìƒì„±ëœ ì—ì´ì „íŠ¸ ì´ë¦„ ëª©ë¡
        """
        generated_agents = []
        
        if not hasattr(self, 'agno_manager'):
            logger.warning("Agno not initialized, skipping agent generation")
            return generated_agents
        
        try:
            # ê°­ ë¶„ì„ì„ í†µí•´ í•„ìš”í•œ ì—ì´ì „íŠ¸ ì‹ë³„
            for gap in gaps[:5]:  # ìµœëŒ€ 5ê°œ ê°­ì— ëŒ€í•´ì„œë§Œ ì—ì´ì „íŠ¸ ìƒì„±
                gap_type = gap.get('type', 'unknown')
                gap_description = gap.get('description', '')
                
                # AIë¥¼ í†µí•´ ì—ì´ì „íŠ¸ ìŠ¤í™ ìƒì„±
                agent_spec_prompt = f"""
                Based on this gap in our system:
                Type: {gap_type}
                Description: {gap_description}
                
                Design an agent specification to address this gap.
                Include name, purpose, capabilities, and implementation approach.
                """
                
                # Agno ìŠ¤í™ ìƒì„±
                from backend.packages.agno.spec import AgentSpec as AgnoSpec
                
                spec = AgnoSpec(
                    name=f"{gap_type}_resolver_agent",
                    description=f"Agent to resolve {gap_type} gaps",
                    purpose=f"Address gaps related to {gap_type}",
                    capabilities=[
                        f"analyze_{gap_type}",
                        f"resolve_{gap_type}",
                        f"validate_{gap_type}_resolution"
                    ],
                    inputs={"gap": "Gap information to resolve"},
                    outputs={"resolution": "Resolution details and status"},
                    dependencies=["backend.packages.agents.base"],
                    ai_enabled=True
                )
                
                # Agnoë¥¼ í†µí•œ ì—ì´ì „íŠ¸ ì½”ë“œ ìƒì„±
                try:
                    agent_name = await self.agno_manager.create_agent(spec)
                    if agent_name:
                        generated_agents.append(agent_name)
                        logger.info(f"Generated agent: {agent_name}")
                except Exception as e:
                    logger.error(f"Failed to generate agent for {gap_type}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Agent generation failed: {e}")
        
        return generated_agents
    
    async def _execute_tests(self, code_generation_result: Dict[str, Any]) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("Executing tests...")
        
        task = AgentTask(
            task_id="run_tests",
            intent="run_tests",
            inputs={
                "generated_code": code_generation_result,
                "project_path": self.config.project_path
            }
        )
        
        result = await self.quality_gate.execute(task)
        return result.data if result.success else {}
    
    async def _recheck_gaps(
        self,
        requirement_result: Dict[str, Any],
        test_result: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ê°­ ì¬í™•ì¸"""
        logger.info("Re-checking gaps...")
        
        # í˜„ì¬ ìƒíƒœ ë‹¤ì‹œ ë¶„ì„
        current_state = await self._execute_current_state_analysis()
        
        # ê°­ ì¬ë¶„ì„
        task = AgentTask(
            task_id="recheck_gaps",
            intent="analyze_gaps",
            inputs={
                "requirements": requirement_result,
                "current_state": current_state,
                "test_results": test_result
            }
        )
        
        result = await self.gap_analyzer.execute(task)
        return result.data.get("gaps", []) if result.success else []
    
    async def _save_all_reports_as_markdown(self, report: UpgradeReport) -> None:
        """ëª¨ë“  ë³´ê³ ì„œë¥¼ MD íŒŒì¼ë¡œ ì €ì¥"""
        from pathlib import Path
        import json
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        project_name = Path(self.config.project_path).name
        timestamp = report.timestamp.replace(':', '-').replace('.', '-')
        output_dir = Path(self.config.output_dir) / project_name / timestamp
        output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Saving reports to {output_dir}")
        
        # 1. ìš”êµ¬ì‚¬í•­ ë¶„ì„ ë³´ê³ ì„œ
        if report.requirement_analysis:
            req_path = output_dir / "01_requirement_analysis.md"
            self._save_as_markdown(
                req_path,
                "ìš”êµ¬ì‚¬í•­ ë¶„ì„ ë³´ê³ ì„œ",
                report.requirement_analysis
            )
        
        # 2. í˜„ì¬ ìƒíƒœ ë¶„ì„ ë³´ê³ ì„œë“¤
        if report.static_analysis:
            static_path = output_dir / "02_static_analysis.md"
            self._save_as_markdown(
                static_path,
                "ì •ì  ë¶„ì„ ë³´ê³ ì„œ",
                report.static_analysis
            )
        
        if report.code_analysis:
            code_path = output_dir / "03_code_analysis.md"
            self._save_as_markdown(
                code_path,
                "ì½”ë“œ ë¶„ì„ ë³´ê³ ì„œ",
                report.code_analysis
            )
        
        if report.behavior_analysis:
            behavior_path = output_dir / "04_behavior_analysis.md"
            self._save_as_markdown(
                behavior_path,
                "í–‰ë™ ë¶„ì„ ë³´ê³ ì„œ",
                report.behavior_analysis
            )
        
        if report.impact_analysis:
            impact_path = output_dir / "05_impact_analysis.md"
            self._save_as_markdown(
                impact_path,
                "ì˜í–¥ë„ ë¶„ì„ ë³´ê³ ì„œ",
                report.impact_analysis
            )
        
        if report.quality_metrics:
            quality_path = output_dir / "06_quality_metrics.md"
            self._save_as_markdown(
                quality_path,
                "í’ˆì§ˆ ë©”íŠ¸ë¦­ ë³´ê³ ì„œ",
                report.quality_metrics
            )
        
        # 3. ì™¸ë¶€ ë¦¬ì„œì¹˜ ë³´ê³ ì„œ
        if report.research_pack:
            research_path = output_dir / "07_external_research.md"
            self._save_research_pack_as_markdown(
                research_path,
                report.research_pack
            )
        
        # 4. ê°­ ë¶„ì„ ë³´ê³ ì„œ
        if report.gap_analysis:
            gap_path = output_dir / "08_gap_analysis.md"
            self._save_as_markdown(
                gap_path,
                "ê°­ ë¶„ì„ ë³´ê³ ì„œ",
                report.gap_analysis
            )
        
        # 5. íƒœìŠ¤í¬ ëª©ë¡
        if report.tasks_breakdown:
            tasks_path = output_dir / "09_tasks.md"
            self._save_tasks_as_markdown(
                tasks_path,
                report.tasks_breakdown
            )
        
        # 6. ì¢…í•© ë³´ê³ ì„œ (JSON)
        full_report_path = output_dir / "00_full_report.json"
        with open(full_report_path, 'w', encoding='utf-8') as f:
            # Convert report to dict
            report_dict = {}
            for key, value in report.__dict__.items():
                if value is None:
                    report_dict[key] = None
                elif hasattr(value, '__dict__'):
                    try:
                        report_dict[key] = asdict(value)
                    except:
                        report_dict[key] = str(value)
                elif isinstance(value, (list, dict, str, int, float, bool)):
                    report_dict[key] = value
                else:
                    report_dict[key] = str(value)
            
            json.dump(report_dict, f, indent=2, default=str, ensure_ascii=False)
        
        logger.info(f"All reports saved to {output_dir}")
    
    def _save_as_markdown(self, path: Path, title: str, data: Dict[str, Any]) -> None:
        """ë°ì´í„°ë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥"""
        from pathlib import Path
        import json
        
        content = f"# {title}\n\n"
        content += f"ìƒì„± ì‹œê°„: {datetime.now().isoformat()}\n\n"
        
        # Convert dict to markdown format
        for key, value in data.items():
            if isinstance(value, dict):
                content += f"## {key}\n\n"
                content += "```json\n"
                content += json.dumps(value, indent=2, default=str, ensure_ascii=False)
                content += "\n```\n\n"
            elif isinstance(value, list):
                content += f"## {key}\n\n"
                for i, item in enumerate(value, 1):
                    content += f"{i}. {item}\n"
                content += "\n"
            else:
                content += f"## {key}\n\n{value}\n\n"
        
        with open(path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def _save_research_pack_as_markdown(self, path: Path, research_pack: UpgradeResearchPack) -> None:
        """ë¦¬ì„œì¹˜ íŒ©ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì €ì¥"""
        content = f"# ì™¸ë¶€ ë¦¬ì„œì¹˜ ë³´ê³ ì„œ (URP)\n\n"
        content += f"ìƒì„± ì‹œê°„: {research_pack.created_at}\n"
        content += f"ìœ íš¨ ê¸°ê°„: {research_pack.ttl_days}ì¼\n\n"
        
        content += f"## í•œ ì¤„ ê²°ë¡ \n\n{research_pack.one_line_conclusion}\n\n"
        
        content += f"## ì¶”ì²œ ì ‘ê·¼ë²•\n\n"
        content += f"```json\n{json.dumps(research_pack.recommended_approach, indent=2, ensure_ascii=False)}\n```\n\n"
        
        if research_pack.alternative_approaches:
            content += f"## ëŒ€ì•ˆ ì ‘ê·¼ë²•ë“¤\n\n"
            for i, alt in enumerate(research_pack.alternative_approaches, 1):
                content += f"### ëŒ€ì•ˆ {i}\n"
                content += f"```json\n{json.dumps(alt, indent=2, ensure_ascii=False)}\n```\n\n"
        
        if research_pack.code_snippets:
            content += f"## ì½”ë“œ ì˜ˆì œ\n\n"
            for i, snippet in enumerate(research_pack.code_snippets, 1):
                content += f"### ì˜ˆì œ {i}\n"
                content += f"```python\n{snippet.get('code', '')}\n```\n\n"
        
        if research_pack.warnings:
            content += f"## âš ï¸ ì£¼ì˜ì‚¬í•­\n\n"
            for warning in research_pack.warnings:
                content += f"- {warning}\n"
            content += "\n"
        
        if research_pack.references:
            content += f"## ì°¸ê³ ìë£Œ\n\n"
            for ref in research_pack.references:
                content += f"- {ref}\n"
            content += "\n"
        
        with open(path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def _save_tasks_as_markdown(self, path: Path, tasks: List[Dict[str, Any]]) -> None:
        """íƒœìŠ¤í¬ ëª©ë¡ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì €ì¥"""
        content = f"# ì„¸ë¶€ íƒœìŠ¤í¬ ëª©ë¡\n\n"
        content += f"ì´ íƒœìŠ¤í¬ ìˆ˜: {len(tasks)}ê°œ\n\n"
        
        for i, task in enumerate(tasks, 1):
            content += f"## Task {i}: {task.get('name', 'Unnamed Task')}\n\n"
            content += f"- **ì„¤ëª…**: {task.get('description', '')}\n"
            content += f"- **ì˜ˆìƒ ì‹œê°„**: {task.get('duration', '5-20ë¶„')}\n"
            content += f"- **ìš°ì„ ìˆœìœ„**: {task.get('priority', 'normal')}\n"
            
            if task.get('dependencies'):
                content += f"- **ì˜ì¡´ì„±**: {', '.join(task['dependencies'])}\n"
            
            if task.get('outputs'):
                content += f"- **ì‚°ì¶œë¬¼**: {', '.join(task['outputs'])}\n"
            
            content += "\n"
        
        with open(path, 'w', encoding='utf-8') as f:
            f.write(content)


async def main():
    """Example usage."""
    config = UpgradeConfig(
        project_path="/home/ec2-user/T-Developer",
        enable_dynamic_analysis=False,
        include_behavior_analysis=True,
        generate_impact_matrix=True,
        generate_recommendations=True,
        safe_mode=True
    )
    
    orchestrator = UpgradeOrchestrator(config)
    await orchestrator.initialize()
    
    requirements = """
    Analyze the current system and provide upgrade recommendations.
    Focus on improving test coverage, reducing technical debt, and
    identifying performance bottlenecks.
    """
    
    report = await orchestrator.analyze(requirements)
    
    # Print summary
    print(f"System Health Score: {report.system_health_score:.1f}/100")
    print(f"Upgrade Risk Score: {report.upgrade_risk_score:.1f}/100")
    print(f"Total Issues Found: {report.total_issues_found}")
    print(f"Critical Issues: {len(report.critical_issues)}")
    
    print("\nImmediate Actions:")
    for action in report.immediate_actions:
        print(f"  - {action}")
    
    print("\nShort-term Goals:")
    for goal in report.short_term_goals:
        print(f"  - {goal}")


if __name__ == "__main__":
    asyncio.run(main())