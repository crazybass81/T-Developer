#!/usr/bin/env python3
"""í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ë¶„ì„ ì—ì´ì „íŠ¸

ì´ ì—ì´ì „íŠ¸ëŠ” Evolution Loopì—ì„œ ì½”ë“œ ìƒì„±/ìˆ˜ì • í›„ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³ 
ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³ 
ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ì— ëŒ€í•œ AI ê¸°ë°˜ ì›ì¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ ìë™ ê°ì§€ (pytest, unittest, jest, mocha ë“±)
- í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ì»¤ë²„ë¦¬ì§€ ì¸¡ì •
- ì‹¤íŒ¨ ì›ì¸ AI ë¶„ì„
- ê°œì„  ê¶Œì¥ì‚¬í•­ ì œì‹œ
- SharedDocumentContext í†µí•©
"""

from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
import asyncio
import subprocess
import json
import logging
from pathlib import Path
from datetime import datetime
import re

from .base import BaseAgent, AgentTask, AgentResult
from .personas import get_persona
from ..ai_providers import BedrockAIProvider

logger = logging.getLogger(__name__)


@dataclass
class TestReport:
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê²°ê³¼ ë¦¬í¬íŠ¸
    
    í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì˜ ëª¨ë“  ì¸¡ë©´ì„ ë‹´ì€ ì¢…í•© ë¦¬í¬íŠ¸ì…ë‹ˆë‹¤.
    Evolution Loopì—ì„œ í’ˆì§ˆ ê²€ì¦ì˜ í•µì‹¬ ë°ì´í„°ê°€ ë©ë‹ˆë‹¤.
    """
    total_tests: int = 0
    passed: int = 0
    failed: int = 0
    skipped: int = 0
    coverage: float = 0.0
    failed_tests: List[Dict[str, Any]] = field(default_factory=list)
    test_duration: float = 0.0
    test_output: str = ""
    recommendations: List[str] = field(default_factory=list)
    framework: str = "unknown"
    files_covered: int = 0
    lines_covered: int = 0
    lines_total: int = 0
    branch_coverage: float = 0.0


class TestAgent(BaseAgent):
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ë¶„ì„ ì—ì´ì „íŠ¸
    
    í’ˆì§ˆ ê²€ì¦ê´€ í˜ë¥´ì†Œë‚˜:
    - ì—„ê²©í•˜ê³  ì² ì €í•œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    - ëª¨ë“  ì—£ì§€ ì¼€ì´ìŠ¤ ê³ ë ¤
    - ì‹¤íŒ¨ì— ëŒ€í•œ ëª…í™•í•œ ì›ì¸ ë¶„ì„
    - ê°œì„ ì„ ìœ„í•œ êµ¬ì²´ì  ê¶Œê³ 
    
    ì´ ì—ì´ì „íŠ¸ëŠ” "í…ŒìŠ¤íŠ¸ë˜ì§€ ì•Šì€ ì½”ë“œëŠ” ê³ ì¥ë‚œ ì½”ë“œë‹¤"ë¼ëŠ”
    ì² í•™ìœ¼ë¡œ ì‹œìŠ¤í…œì˜ í’ˆì§ˆì„ ë³´ì¥í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, memory_hub=None, config=None, document_context=None):
        """TestAgent ì´ˆê¸°í™”
        
        Args:
            memory_hub: ë©”ëª¨ë¦¬ í—ˆë¸Œ ì¸ìŠ¤í„´ìŠ¤
            config: ì—ì´ì „íŠ¸ ì„¤ì •
            document_context: SharedDocumentContext ì¸ìŠ¤í„´ìŠ¤
        """
        # AI Provider ì„¤ì •
        ai_provider = BedrockAIProvider(
            model="claude-3-sonnet",
            region="us-east-1"
        )
        
        super().__init__(
            name="TestAgent",
            version="1.0.0",
            memory_hub=memory_hub,
            document_context=document_context,
            ai_provider=ai_provider
        )
        
        self.config = config or {}
        self.persona = get_persona("TestAgent")
        self.capabilities = ["test", "analyze", "validate", "coverage"]
        
        # ì§€ì›í•˜ëŠ” í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
        self.supported_frameworks = {
            "python": ["pytest", "unittest", "nose2", "doctest"],
            "javascript": ["jest", "mocha", "jasmine", "vitest"],
            "typescript": ["jest", "mocha", "vitest"],
            "go": ["go test"],
            "java": ["junit", "testng"],
            "rust": ["cargo test"]
        }
        
    async def execute(self, task: AgentTask) -> AgentResult:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ë¶„ì„
        
        Args:
            task: í…ŒìŠ¤íŠ¸ ì‹¤í–‰ íƒœìŠ¤í¬
                - project_path: í”„ë¡œì íŠ¸ ê²½ë¡œ
                - test_command: í…ŒìŠ¤íŠ¸ ëª…ë ¹ (ì„ íƒì‚¬í•­)
                - coverage_threshold: ì»¤ë²„ë¦¬ì§€ ì„ê³„ê°’ (ê¸°ë³¸ 80%)
                - test_pattern: í…ŒìŠ¤íŠ¸ íŒŒì¼ íŒ¨í„´
            
        Returns:
            í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë° ë¶„ì„
        """
        try:
            logger.info(f"[{self.persona.name}] í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘...")
            
            project_path = task.inputs.get("project_path", ".")
            test_command = task.inputs.get("test_command")
            coverage_threshold = task.inputs.get("coverage_threshold", 80.0)
            
            # 1. í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ ê°ì§€
            if not test_command:
                logger.info("í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ ìë™ ê°ì§€ ì¤‘...")
                test_command = await self._detect_test_framework(project_path)
                logger.info(f"ê°ì§€ëœ í…ŒìŠ¤íŠ¸ ëª…ë ¹: {test_command}")
            
            # 2. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            logger.info(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰: {test_command}")
            test_report = await self._run_tests(project_path, test_command)
            
            # 3. ì»¤ë²„ë¦¬ì§€ ë¶„ì„
            if test_report.coverage < coverage_threshold:
                test_report.recommendations.append(
                    f"âš ï¸ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€({test_report.coverage:.1f}%)ê°€ "
                    f"ëª©í‘œì¹˜({coverage_threshold}%)ë³´ë‹¤ ë‚®ìŠµë‹ˆë‹¤."
                )
            
            # 4. ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ AI ë¶„ì„
            if test_report.failed > 0:
                logger.info(f"ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ {test_report.failed}ê°œ ë¶„ì„ ì¤‘...")
                failure_analysis = await self._analyze_failures(test_report)
                test_report.recommendations.extend(failure_analysis)
            
            # 5. SharedDocumentContextì— ê²°ê³¼ ì¶”ê°€
            if self.document_context:
                self.document_context.add_document(
                    "TestAgent",
                    {
                        "test_report": test_report.__dict__,
                        "quality_status": "passed" if test_report.failed == 0 else "failed",
                        "coverage": test_report.coverage,
                        "framework": test_report.framework,
                        "timestamp": datetime.now().isoformat()
                    },
                    document_type="test_result"
                )
            
            # 6. ë©”ëª¨ë¦¬ í—ˆë¸Œì— ì €ì¥
            if self.memory_hub:
                from ..memory.context_types import ContextType
                await self.memory_hub.put(
                    ContextType.S_CTX,
                    f"test_report_{task.task_id}",
                    test_report.__dict__,
                    ttl_seconds=86400
                )
            
            # ì„±ê³µ ì—¬ë¶€ ê²°ì •
            success = test_report.failed == 0 and test_report.coverage >= coverage_threshold
            
            return AgentResult(
                success=success,
                data={
                    "tests_passed": test_report.passed,
                    "tests_failed": test_report.failed,
                    "tests_skipped": test_report.skipped,
                    "total_tests": test_report.total_tests,
                    "coverage": test_report.coverage,
                    "duration": test_report.test_duration,
                    "framework": test_report.framework,
                    "recommendations": test_report.recommendations,
                    "full_report": test_report.__dict__
                },
                message=f"{self.persona.catchphrase} - "
                       f"í…ŒìŠ¤íŠ¸: {test_report.passed}/{test_report.total_tests} í†µê³¼, "
                       f"ì»¤ë²„ë¦¬ì§€: {test_report.coverage:.1f}%",
                metadata={
                    "agent": self.name,
                    "persona": self.persona.name,
                    "timestamp": datetime.now().isoformat()
                }
            )
            
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return AgentResult(
                success=False,
                error=str(e),
                data={},
                message=f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            )
    
    async def _detect_test_framework(self, project_path: str) -> str:
        """í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ ìë™ ê°ì§€
        
        í”„ë¡œì íŠ¸ì˜ ì„¤ì • íŒŒì¼ê³¼ ì˜ì¡´ì„±ì„ ë¶„ì„í•˜ì—¬
        ì ì ˆí•œ í…ŒìŠ¤íŠ¸ ëª…ë ¹ì„ ìë™ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.
        """
        path = Path(project_path)
        
        # Python í”„ë¡œì íŠ¸
        if (path / "pytest.ini").exists() or (path / "setup.cfg").exists():
            return "pytest --cov=. --cov-report=term-missing -v"
        elif (path / "pyproject.toml").exists():
            with open(path / "pyproject.toml") as f:
                content = f.read()
                if "pytest" in content:
                    return "pytest --cov=. --cov-report=term-missing -v"
        elif (path / "manage.py").exists():  # Django
            return "python manage.py test --verbosity=2"
        elif (path / "setup.py").exists():
            return "python -m pytest"
        
        # JavaScript/TypeScript
        elif (path / "package.json").exists():
            with open(path / "package.json") as f:
                pkg = json.load(f)
                scripts = pkg.get("scripts", {})
                
                # package.jsonì— ì •ì˜ëœ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
                if "test" in scripts:
                    return "npm test"
                elif "test:coverage" in scripts:
                    return "npm run test:coverage"
                    
                # ì˜ì¡´ì„± ê¸°ë°˜ ê°ì§€
                deps = {**pkg.get("dependencies", {}), **pkg.get("devDependencies", {})}
                if "jest" in deps:
                    return "npx jest --coverage"
                elif "mocha" in deps:
                    return "npx mocha --recursive"
                elif "vitest" in deps:
                    return "npx vitest run --coverage"
        
        # Go í”„ë¡œì íŠ¸
        elif (path / "go.mod").exists():
            return "go test -v -cover ./..."
        
        # Rust í”„ë¡œì íŠ¸
        elif (path / "Cargo.toml").exists():
            return "cargo test --verbose"
        
        # Java í”„ë¡œì íŠ¸
        elif (path / "pom.xml").exists():  # Maven
            return "mvn test"
        elif (path / "build.gradle").exists():  # Gradle
            return "gradle test"
        
        # ê¸°ë³¸ê°’ (Python pytest)
        logger.warning("í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pytestë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
        return "pytest"
    
    async def _run_tests(self, project_path: str, command: str) -> TestReport:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        
        ì§€ì •ëœ ëª…ë ¹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
        """
        import time
        start_time = time.time()
        
        report = TestReport()
        report.framework = self._identify_framework(command)
        
        try:
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            logger.info(f"ì‹¤í–‰ ëª…ë ¹: {command}")
            result = subprocess.run(
                command,
                shell=True,
                cwd=project_path,
                capture_output=True,
                text=True,
                timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            report.test_duration = time.time() - start_time
            report.test_output = result.stdout + result.stderr
            
            # ê²°ê³¼ íŒŒì‹±
            report = self._parse_test_output(report, report.test_output)
            
            return report
            
        except subprocess.TimeoutExpired:
            report.test_output = "í…ŒìŠ¤íŠ¸ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ (5ë¶„ ì´ˆê³¼)"
            report.test_duration = 300
            report.recommendations.append("í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return report
            
        except Exception as e:
            report.test_output = f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}"
            report.recommendations.append(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í™˜ê²½ì„ í™•ì¸í•˜ì„¸ìš”: {e}")
            return report
    
    def _identify_framework(self, command: str) -> str:
        """ëª…ë ¹ì–´ì—ì„œ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ ì‹ë³„"""
        command_lower = command.lower()
        
        if "pytest" in command_lower:
            return "pytest"
        elif "unittest" in command_lower:
            return "unittest"
        elif "jest" in command_lower:
            return "jest"
        elif "mocha" in command_lower:
            return "mocha"
        elif "vitest" in command_lower:
            return "vitest"
        elif "go test" in command_lower:
            return "go"
        elif "cargo test" in command_lower:
            return "rust"
        elif "mvn test" in command_lower or "gradle test" in command_lower:
            return "java"
        else:
            return "unknown"
    
    def _parse_test_output(self, report: TestReport, output: str) -> TestReport:
        """í…ŒìŠ¤íŠ¸ ì¶œë ¥ íŒŒì‹±
        
        ê° í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ì˜ ì¶œë ¥ í˜•ì‹ì— ë§ê²Œ ê²°ê³¼ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
        """
        # pytest ìŠ¤íƒ€ì¼ íŒŒì‹±
        if report.framework == "pytest" or "passed" in output or "failed" in output:
            # ì˜ˆ: "5 passed, 2 failed, 1 skipped in 3.24s"
            pattern = r'(\d+)\s+passed'
            match = re.search(pattern, output)
            if match:
                report.passed = int(match.group(1))
            
            pattern = r'(\d+)\s+failed'
            match = re.search(pattern, output)
            if match:
                report.failed = int(match.group(1))
            
            pattern = r'(\d+)\s+skipped'
            match = re.search(pattern, output)
            if match:
                report.skipped = int(match.group(1))
            
            # ì»¤ë²„ë¦¬ì§€ íŒŒì‹± (pytest-cov)
            # ì˜ˆ: "TOTAL                     1234    567    54%"
            pattern = r'TOTAL\s+(\d+)\s+(\d+)\s+(\d+)%'
            match = re.search(pattern, output)
            if match:
                report.lines_total = int(match.group(1))
                report.lines_covered = report.lines_total - int(match.group(2))
                report.coverage = float(match.group(3))
            
            # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ì¶”ì¶œ
            if report.failed > 0:
                report.failed_tests = self._extract_failed_tests(output, "pytest")
        
        # Jest ìŠ¤íƒ€ì¼ íŒŒì‹±
        elif report.framework == "jest":
            # ì˜ˆ: "Tests:       2 failed, 5 passed, 7 total"
            pattern = r'Tests:\s+(\d+)\s+failed,\s+(\d+)\s+passed,\s+(\d+)\s+total'
            match = re.search(pattern, output)
            if match:
                report.failed = int(match.group(1))
                report.passed = int(match.group(2))
                report.total_tests = int(match.group(3))
            
            # ì»¤ë²„ë¦¬ì§€ íŒŒì‹±
            pattern = r'All files\s+\|\s+([\d.]+)\s+\|'
            match = re.search(pattern, output)
            if match:
                report.coverage = float(match.group(1))
        
        # Go test ìŠ¤íƒ€ì¼ íŒŒì‹±
        elif report.framework == "go":
            # PASS/FAIL ì¹´ìš´íŠ¸
            report.passed = output.count("PASS")
            report.failed = output.count("FAIL")
            
            # ì»¤ë²„ë¦¬ì§€ íŒŒì‹±
            pattern = r'coverage:\s+([\d.]+)%'
            match = re.search(pattern, output)
            if match:
                report.coverage = float(match.group(1))
        
        # ì´ í…ŒìŠ¤íŠ¸ ìˆ˜ ê³„ì‚°
        report.total_tests = report.passed + report.failed + report.skipped
        
        return report
    
    def _extract_failed_tests(self, output: str, framework: str) -> List[Dict[str, Any]]:
        """ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ"""
        failed_tests = []
        
        if framework == "pytest":
            # FAILED ë¼ì¸ ì°¾ê¸°
            lines = output.split("\n")
            for i, line in enumerate(lines):
                if "FAILED" in line:
                    test_info = {
                        "name": line.split("FAILED")[0].strip(),
                        "error": ""
                    }
                    
                    # ì—ëŸ¬ ë©”ì‹œì§€ ì°¾ê¸° (ë‹¤ìŒ ëª‡ ì¤„)
                    for j in range(i+1, min(i+10, len(lines))):
                        if lines[j].strip():
                            test_info["error"] += lines[j] + "\n"
                        else:
                            break
                    
                    failed_tests.append(test_info)
        
        return failed_tests
    
    async def _analyze_failures(self, report: TestReport) -> List[str]:
        """ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ AI ë¶„ì„
        
        AIë¥¼ í™œìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì›ì¸ì„ ë¶„ì„í•˜ê³ 
        êµ¬ì²´ì ì¸ í•´ê²° ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.
        """
        if not self.ai_provider or not report.failed_tests:
            return ["í…ŒìŠ¤íŠ¸ ì¶œë ¥ì„ í™•ì¸í•˜ì—¬ ì‹¤íŒ¨ ì›ì¸ì„ íŒŒì•…í•˜ì„¸ìš”."]
        
        # í˜ë¥´ì†Œë‚˜ ì ìš©
        persona_prompt = self.persona.to_prompt() if self.persona else ""
        
        # í˜„ì¬ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        context = ""
        if self.document_context:
            all_docs = self.document_context.get_all_documents()
            # ê´€ë ¨ ë¬¸ì„œë§Œ ì¶”ì¶œ (ì½”ë“œ ë³€ê²½, íƒœìŠ¤í¬ ë“±)
            relevant_docs = {
                k: v for k, v in all_docs.items()
                if k in ["CodeGenerator", "TaskCreatorAgent", "PlannerAgent"]
            }
            if relevant_docs:
                context = f"\nìµœê·¼ ë³€ê²½ì‚¬í•­:\n{json.dumps(relevant_docs, indent=2)[:2000]}"
        
        prompt = f"""{persona_prompt}

ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ë¥¼ ë¶„ì„í•˜ê³  í•´ê²° ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”:

í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬: {report.framework}
ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìˆ˜: {report.failed}
ì „ì²´ í…ŒìŠ¤íŠ¸ ìˆ˜: {report.total_tests}

ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë“¤:
{json.dumps(report.failed_tests[:5], indent=2)}  # ìµœëŒ€ 5ê°œë§Œ

í…ŒìŠ¤íŠ¸ ì¶œë ¥ (ë§ˆì§€ë§‰ ë¶€ë¶„):
{report.test_output[-2000:]}  # ë§ˆì§€ë§‰ 2000ì
{context}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•˜ì„¸ìš”:
1. ì£¼ìš” ì‹¤íŒ¨ ì›ì¸ (í•œ ì¤„ ìš”ì•½)
2. êµ¬ì²´ì  í•´ê²° ë°©ì•ˆ (3ê°œ ì´ë‚´)
3. ìš°ì„ ìˆœìœ„ (ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ)
4. ì˜ˆìƒ ìˆ˜ì • ì‹œê°„

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "main_cause": "...",
    "solutions": ["solution1", "solution2", "solution3"],
    "priority": "high|medium|low",
    "estimated_time": "5-10 minutes"
}}
"""
        
        try:
            response = await self.ai_provider.complete(prompt)
            analysis = json.loads(response)
            
            recommendations = [
                f"ğŸ” ì£¼ìš” ì›ì¸: {analysis.get('main_cause', 'ë¶„ì„ ì‹¤íŒ¨')}",
                f"âš¡ ìš°ì„ ìˆœìœ„: {analysis.get('priority', 'medium')}",
                f"â±ï¸ ì˜ˆìƒ ì‹œê°„: {analysis.get('estimated_time', 'ì•Œ ìˆ˜ ì—†ìŒ')}"
            ]
            
            for i, solution in enumerate(analysis.get('solutions', []), 1):
                recommendations.append(f"  {i}. {solution}")
            
            return recommendations
            
        except Exception as e:
            logger.error(f"AI ë¶„ì„ ì‹¤íŒ¨: {e}")
            return [
                "AI ë¶„ì„ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì¶œë ¥ì„ ì§ì ‘ í™•ì¸í•˜ì„¸ìš”.",
                f"ì—ëŸ¬: {str(e)}"
            ]
    
    async def generate_test_improvement_plan(self) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ê°œì„  ê³„íš ìƒì„±
        
        í˜„ì¬ í…ŒìŠ¤íŠ¸ ìƒíƒœë¥¼ ë¶„ì„í•˜ê³  ê°œì„  ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.
        """
        if not self.document_context:
            return {"error": "No document context available"}
        
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì§‘
        all_docs = self.document_context.get_all_documents()
        test_docs = {k: v for k, v in all_docs.items() if "test" in k.lower()}
        
        if not test_docs:
            return {"message": "No test documents found"}
        
        # AIë¥¼ í™œìš©í•œ ê°œì„  ê³„íš ìˆ˜ë¦½
        if self.ai_provider:
            prompt = f"""{self.persona.to_prompt() if self.persona else ''}

í˜„ì¬ í…ŒìŠ¤íŠ¸ ìƒíƒœë¥¼ ë¶„ì„í•˜ê³  ê°œì„  ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”:

í…ŒìŠ¤íŠ¸ ë¬¸ì„œë“¤:
{json.dumps(test_docs, indent=2)[:3000]}

ë‹¤ìŒì„ í¬í•¨í•œ ê°œì„  ê³„íšì„ ì‘ì„±í•˜ì„¸ìš”:
1. í˜„ì¬ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ìƒíƒœ
2. ë¶€ì¡±í•œ í…ŒìŠ¤íŠ¸ ì˜ì—­
3. í…ŒìŠ¤íŠ¸ í’ˆì§ˆ ê°œì„  ë°©ì•ˆ
4. êµ¬ì²´ì  ì‹¤í–‰ ê³„íš

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
"""
            
            try:
                response = await self.ai_provider.complete(prompt)
                return json.loads(response)
            except Exception as e:
                logger.error(f"ê°œì„  ê³„íš ìƒì„± ì‹¤íŒ¨: {e}")
                return {"error": str(e)}
        
        return {"message": "AI provider not available"}