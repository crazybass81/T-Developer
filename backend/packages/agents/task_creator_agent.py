"""íƒœìŠ¤í¬ ìƒì„±ê¸° (TaskCreatorAgent) - AI ê¸°ë°˜ ì„¸ë¶€ ì‹¤í–‰ íƒœìŠ¤í¬ ìƒì„±

ì´ ì—ì´ì „íŠ¸ëŠ” PlannerAgentê°€ ìƒì„±í•œ ì¶”ìƒì ì¸ ì‹¤í–‰ ê³„íšì„ ë°›ì•„ì„œ êµ¬ì²´ì ì´ê³ 
ì‹¤í–‰ ê°€ëŠ¥í•œ ì„¸ë¶€ íƒœìŠ¤í¬ë“¤ë¡œ ë¶„í•´í•˜ì—¬ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ì‹¤í–‰ ê³„íšì„ ì›ìì  ì‹¤í–‰ ë‹¨ìœ„ë¡œ ë¶„í•´
2. ê° íƒœìŠ¤í¬ì˜ ì…ë ¥/ì¶œë ¥ ìŠ¤í™ ì •ì˜ ë° ë°ì´í„° í˜•ì‹ ëª…ì‹œ
3. íƒœìŠ¤í¬ë³„ ê²€ì¦ ê¸°ì¤€ ì„¤ì • ë° ì„±ê³µ/ì‹¤íŒ¨ ì¡°ê±´ ì •ì˜
4. Python ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìë™ ìƒì„±
5. ì‹¤íŒ¨ ì‹œ ë¡¤ë°± ê³„íš ìˆ˜ë¦½
6. íƒœìŠ¤í¬ê°„ ì˜ì¡´ì„± ìµœì í™” ë° ë³‘ë ¬ ì‹¤í–‰ ê¸°íšŒ ì‹ë³„
7. ìš°ì„ ìˆœìœ„ ì¡°ì • ë° í¬ë¦¬í‹°ì»¬ íŒ¨ìŠ¤ ì‹ë³„
8. ì¬ì‹œë„ ì „ëµ ì„¤ì • ë° ì—ëŸ¬ ì²˜ë¦¬ ë°©ì•ˆ ìˆ˜ë¦½

ì…ë ¥ ë§¤ê°œë³€ìˆ˜:
- plan: PlannerAgentì˜ ì‹¤í–‰ ê³„íš (phases, goals, tasks í¬í•¨)
- requirement: ì›ë³¸ ìš”êµ¬ì‚¬í•­ ëª…ì„¸
- context: í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
- optimization_goal: ìµœì í™” ëª©í‘œ (speed/cost/quality/balanced)

ì¶œë ¥ í˜•ì‹:
- tasks: ExecutableTask ê°ì²´ ë°°ì—´
  * id: ê³ ìœ  íƒœìŠ¤í¬ ì‹ë³„ì
  * name: íƒœìŠ¤í¬ ëª…
  * description: ìƒì„¸ ì„¤ëª…
  * type: íƒœìŠ¤í¬ ìœ í˜• (analysis/development/testing/deployment ë“±)
  * agent: ì‹¤í–‰í•  ì—ì´ì „íŠ¸ëª…
  * inputs: ì…ë ¥ ìŠ¤í™ (íƒ€ì…, í•„ìˆ˜ì—¬ë¶€, ì„¤ëª…)
  * expected_outputs: ì˜ˆìƒ ì¶œë ¥ êµ¬ì¡°
  * validation_criteria: ê²€ì¦ ê¸°ì¤€ ë¦¬ìŠ¤íŠ¸
  * execution_script: Python ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
  * rollback_plan: ë¡¤ë°± ì ˆì°¨
  * dependencies: ì˜ì¡´ íƒœìŠ¤í¬ ID ë¦¬ìŠ¤íŠ¸
  * estimated_time: ì˜ˆìƒ ì†Œìš” ì‹œê°„ (ë¶„)
  * priority: ìš°ì„ ìˆœìœ„ (1-10)
  * retry_strategy: ì¬ì‹œë„ ì „ëµ
- total_tasks: ì´ íƒœìŠ¤í¬ ìˆ˜
- estimated_total_time: ì´ ì˜ˆìƒ ì‹œê°„
- execution_order: ì‹¤í–‰ ìˆœì„œ
- critical_path: í¬ë¦¬í‹°ì»¬ íŒ¨ìŠ¤

ë¬¸ì„œ ì°¸ì¡° ê´€ê³„:
- ì½ì–´ì˜¤ëŠ” ë³´ê³ ì„œ:
  * PlannerAgent ì‹¤í–‰ ê³„íš
  * ExternalResearcher ë¦¬ì„œì¹˜ ë³´ê³ ì„œ
  * GapAnalyzer ê°­ ë¶„ì„ ë³´ê³ ì„œ
- ì¶œë ¥ì„ ì‚¬ìš©í•˜ëŠ” ì—ì´ì „íŠ¸:
  * UpgradeOrchestrator: íƒœìŠ¤í¬ ì‹¤í–‰ ì¡°ìœ¨
  * QualityGate: íƒœìŠ¤í¬ í’ˆì§ˆ ê²€ì¦
  * MonitoringAgent: ì‹¤í–‰ ëª¨ë‹ˆí„°ë§

ì‚¬ìš© ì˜ˆì‹œ:
```python
task_creator = TaskCreatorAgent(memory_hub=hub)
result = await task_creator.execute(AgentTask(
    inputs={
        'plan': planner_output,
        'requirement': "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ë¡œ ì „í™˜",
        'context': {'framework': 'FastAPI', 'db': 'PostgreSQL'},
        'optimization_goal': 'balanced'
    }
))

print(f"ìƒì„±ëœ íƒœìŠ¤í¬: {result.data['total_tasks']}ê°œ")
print(f"ì˜ˆìƒ ì†Œìš”ì‹œê°„: {result.data['estimated_total_time']}ë¶„")
```

ì‘ì„±ì: T-Developer v2 Team
ë²„ì „: 2.1.0
ìµœì¢… ìˆ˜ì •: 2025-08-23
"""

import json
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum

from .base import BaseAgent, AgentTask, AgentResult, TaskStatus
from .ai_providers import BedrockAIProvider
from ..memory.contexts import ContextType
from ..safety import CircuitBreaker, ResourceLimiter

logger = logging.getLogger(__name__)


class TaskType(Enum):
    """íƒœìŠ¤í¬ ìœ í˜•."""
    ANALYSIS = "analysis"
    DEVELOPMENT = "development"
    TESTING = "testing"
    DEPLOYMENT = "deployment"
    DOCUMENTATION = "documentation"
    REVIEW = "review"
    MONITORING = "monitoring"


@dataclass
class ExecutableTask:
    """ì‹¤í–‰ ê°€ëŠ¥í•œ íƒœìŠ¤í¬."""
    id: str
    name: str
    description: str
    type: TaskType
    agent: str  # ì‹¤í–‰í•  ì—ì´ì „íŠ¸
    inputs: Dict[str, Any] = field(default_factory=dict)
    expected_outputs: Dict[str, Any] = field(default_factory=dict)
    validation_criteria: List[str] = field(default_factory=list)
    execution_script: str = ""
    rollback_plan: str = ""
    dependencies: List[str] = field(default_factory=list)
    estimated_time: int = 30  # minutes
    priority: int = 5  # 1-10
    retry_strategy: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)


class TaskCreatorAgent(BaseAgent):
    """ì„¸ë¶€ íƒœìŠ¤í¬ ìƒì„± ì—ì´ì „íŠ¸.
    
    ì´ ì—ì´ì „íŠ¸ëŠ”:
    1. ì¶”ìƒì ì¸ ê³„íšì„ êµ¬ì²´ì  íƒœìŠ¤í¬ë¡œ ë³€í™˜
    2. ê° íƒœìŠ¤í¬ì˜ ì‹¤í–‰ ë°©ë²• ì •ì˜
    3. ì…ë ¥/ì¶œë ¥ ìŠ¤í™ ìƒì„±
    4. ê²€ì¦ ë° ë¡¤ë°± ì „ëµ ìˆ˜ë¦½
    5. ì‹¤í–‰ ìµœì í™”
    """
    
    def __init__(self, memory_hub=None, document_context=None):
        """TaskCreatorAgent ì´ˆê¸°í™”.
        
        Args:
            memory_hub: ë©”ëª¨ë¦¬ í—ˆë¸Œ ì¸ìŠ¤í„´ìŠ¤
            document_context: SharedDocumentContext ì¸ìŠ¤í„´ìŠ¤
        """
        super().__init__(
            name="TaskCreatorAgent",
            version="1.0.0",
            memory_hub=memory_hub,
            document_context=document_context
        )
        
        # AI Provider ì´ˆê¸°í™” - ì‹¤ì œ AWS Bedrock ì‚¬ìš©
        self.ai_provider = BedrockAIProvider(
            model="claude-3-sonnet",
            region="us-east-1"
        )
        
        # Safety mechanisms
        self.circuit_breaker = CircuitBreaker(name="TaskCreatorAgent")
        self.resource_limiter = ResourceLimiter()
        
        # í˜ë¥´ì†Œë‚˜ ì ìš© - TaskCreatorAgent
        from .personas import get_persona
        self.persona = get_persona("TaskCreatorAgent")
        if self.persona:
            logger.info(f"ğŸ­ {self.persona.name}: {self.persona.catchphrase}")

    
    async def _get_external_and_gap_reports(self) -> Dict[str, Any]:
        """Fetch external researcher and gap analyzer reports from memory.
        
        Returns:
            Dictionary containing external research and gap analysis reports
        """
        if not self.memory_hub:
            return {}
        
        from ..memory.contexts import ContextType
        
        reports = {}
        
        try:
            # Get external research reports
            external_research = await self.memory_hub.search(
                context_type=ContextType.S_CTX,
                tags=["external_research", "ExternalResearcher"],
                limit=3
            )
            if external_research:
                reports["external_research"] = external_research
            
            # Get gap analysis reports
            gap_analysis = await self.memory_hub.search(
                context_type=ContextType.A_CTX,
                tags=["test_gaps", "GapAnalyzer"],
                limit=3
            )
            if gap_analysis:
                reports["gap_analysis"] = gap_analysis
                
        except Exception as e:
            logger.debug(f"Failed to get external/gap reports: {e}")
        
        return reports
    
    async def execute(self, task: AgentTask) -> AgentResult:
        """ì„¸ë¶€ íƒœìŠ¤í¬ ìƒì„±.
        
        Args:
            task: AgentTask with inputs:
                - plan: ì‹¤í–‰ ê³„íš (PlannerAgentì˜ ì¶œë ¥)
                - requirement: ì›ë³¸ ìš”êµ¬ì‚¬í•­
                - context: í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸
                - optimization_goal: ìµœì í™” ëª©í‘œ
                
        Returns:
            AgentResult with executable tasks
        """
        logger.info("Creating executable tasks from plan")
        
        try:
            inputs = task.inputs if hasattr(task, 'inputs') else task
            
            # Get reports from external researcher and gap analyzer
            research_reports = await self._get_external_and_gap_reports()
            
            plan = inputs.get("plan", {})
            requirement = inputs.get("requirement", "")
            context = inputs.get("context", {})
            optimization_goal = inputs.get("optimization_goal", "balanced")
            
            # Enrich context with reports
            if research_reports:
                context["research_insights"] = research_reports
            
            if not plan:
                return AgentResult(
                    success=False,
                    status=TaskStatus.FAILED,
                    error="No execution plan provided",
                    data={}
                )
            
            # 1. ê³„íš ë¶„ì„
            plan_analysis = await self._analyze_plan(plan, requirement)
            
            # 2. íƒœìŠ¤í¬ ìƒì„±
            tasks = await self._create_tasks(plan, plan_analysis, context)
            
            # 3. ì…ì¶œë ¥ ìŠ¤í™ ì •ì˜
            tasks = await self._define_io_specs(tasks, requirement)
            
            # 4. ê²€ì¦ ê¸°ì¤€ ì„¤ì •
            tasks = await self._set_validation_criteria(tasks)
            
            # 5. ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
            tasks = await self._generate_execution_scripts(tasks, context)
            
            # 6. ë¡¤ë°± ê³„íš ìˆ˜ë¦½
            tasks = await self._create_rollback_plans(tasks)
            
            # 7. ì˜ì¡´ì„± ìµœì í™”
            tasks = await self._optimize_dependencies(tasks, optimization_goal)
            
            # 8. ìš°ì„ ìˆœìœ„ ì¡°ì •
            tasks = await self._adjust_priorities(tasks, requirement)
            
            # 9. ë¦¬íŠ¸ë¼ì´ ì „ëµ ì„¤ì •
            tasks = await self._set_retry_strategies(tasks)
            
            # 10. ìµœì¢… ê²€ì¦
            validation_result = await self._validate_tasks(tasks)
            
            # ë©”ëª¨ë¦¬ì— ì €ì¥
            if self.memory_hub:
                await self.memory_hub.put(
                    key=f"executable_tasks_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    value=[t.__dict__ for t in tasks],
                    context_type=ContextType.O_CTX
                )
            
            return AgentResult(
                success=True,
                status=TaskStatus.COMPLETED,
                data={
                    "tasks": [t.__dict__ for t in tasks],
                    "total_tasks": len(tasks),
                    "estimated_total_time": sum(t.estimated_time for t in tasks),
                    "validation": validation_result,
                    "execution_order": self._get_execution_order(tasks),
                    "critical_path": self._identify_critical_path(tasks)
                }
            )
            
        except Exception as e:
            logger.error(f"Task creation failed: {e}")
            return AgentResult(
                success=False,
                status=TaskStatus.FAILED,
                error=str(e),
                data={}
            )
    
    async def _analyze_plan(self, plan: Dict[str, Any], requirement: str) -> Dict[str, Any]:
        """ê³„íš ë¶„ì„.
        
        Args:
            plan: ì‹¤í–‰ ê³„íš
            requirement: ìš”êµ¬ì‚¬í•­
            
        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        prompt = f"""
        Analyze this execution plan and identify key execution requirements:
        
        Plan Summary:
        - Goals: {len(plan.get('goals', []))}
        - Phases: {len(plan.get('phases', []))}
        - Tasks: {len(plan.get('tasks', []))}
        
        Requirement:
        {requirement[:500]}
        
        Identify:
        1. Critical tasks that must succeed
        2. Parallel execution opportunities
        3. Resource bottlenecks
        4. Risk points
        5. Optimization opportunities
        
        Response format (JSON):
        {{
            "critical_tasks": ["..."],
            "parallel_groups": [["task1", "task2"], ...],
            "bottlenecks": ["..."],
            "risk_points": ["..."],
            "optimizations": ["..."]
        }}
        """
        
        response = await self.ai_provider.complete(prompt)
        
        try:
            return json.loads(response)
        except:
            return {
                "critical_tasks": [],
                "parallel_groups": [],
                "bottlenecks": [],
                "risk_points": [],
                "optimizations": []
            }
    
    async def _create_tasks(self, plan: Dict[str, Any], analysis: Dict[str, Any], context: Dict[str, Any]) -> List[ExecutableTask]:
        """ì‹¤í–‰ ê°€ëŠ¥í•œ íƒœìŠ¤í¬ ìƒì„±.
        
        Args:
            plan: ì‹¤í–‰ ê³„íš
            analysis: ê³„íš ë¶„ì„ ê²°ê³¼
            context: í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            íƒœìŠ¤í¬ ëª©ë¡
        """
        tasks = []
        plan_tasks = plan.get("tasks", [])
        
        for i, plan_task in enumerate(plan_tasks):
            # AIë¡œ ìƒì„¸ íƒœìŠ¤í¬ ìƒì„±
            prompt = f"""
            Create an executable task from this plan task:
            
            Plan Task:
            {json.dumps(plan_task, indent=2)}
            
            Context:
            {json.dumps(context, indent=2)[:500]}
            
            Create a detailed executable task with:
            - Clear execution instructions
            - Specific agent to execute
            - Task type (analysis/development/testing/etc)
            - Estimated time in minutes
            - Priority (1-10)
            
            Response format (JSON):
            {{
                "name": "...",
                "description": "...",
                "type": "analysis|development|testing|deployment|documentation|review|monitoring",
                "agent": "agent_name",
                "estimated_time": 30,
                "priority": 5,
                "instructions": "Step by step instructions..."
            }}
            """
            
            response = await self.ai_provider.complete(prompt)
            
            try:
                task_data = json.loads(response)
            except:
                task_data = {
                    "name": plan_task.get("name", f"Task {i+1}"),
                    "description": plan_task.get("description", ""),
                    "type": "analysis",
                    "agent": plan_task.get("agent", "orchestrator"),
                    "estimated_time": 30,
                    "priority": 5,
                    "instructions": "Execute task as planned"
                }
            
            # ExecutableTask ìƒì„±
            exec_task = ExecutableTask(
                id=plan_task.get("id", f"task_{i+1:03d}"),
                name=task_data["name"],
                description=task_data["description"],
                type=TaskType(task_data.get("type", "analysis")),
                agent=task_data["agent"],
                estimated_time=task_data["estimated_time"],
                priority=task_data["priority"],
                dependencies=plan_task.get("dependencies", []),
                metadata={
                    "instructions": task_data.get("instructions", ""),
                    "phase": plan_task.get("phase", ""),
                    "critical": plan_task.get("id") in analysis.get("critical_tasks", [])
                }
            )
            
            tasks.append(exec_task)
        
        return tasks
    
    async def _define_io_specs(self, tasks: List[ExecutableTask], requirement: str) -> List[ExecutableTask]:
        """ì…ì¶œë ¥ ìŠ¤í™ ì •ì˜.
        
        Args:
            tasks: íƒœìŠ¤í¬ ëª©ë¡
            requirement: ìš”êµ¬ì‚¬í•­
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ íƒœìŠ¤í¬ ëª©ë¡
        """
        for task in tasks:
            prompt = f"""
            Define input/output specifications for this task:
            
            Task: {task.name}
            Type: {task.type.value}
            Agent: {task.agent}
            Description: {task.description}
            
            Requirement context:
            {requirement[:300]}
            
            Define:
            1. Required inputs (with types and validation)
            2. Expected outputs (with structure)
            3. Data formats
            
            Response format (JSON):
            {{
                "inputs": {{
                    "param1": {{"type": "string", "required": true, "description": "..."}},
                    ...
                }},
                "outputs": {{
                    "result1": {{"type": "object", "structure": {{...}}}},
                    ...
                }}
            }}
            """
            
            response = await self.ai_provider.complete(prompt)
            
            try:
                io_spec = json.loads(response)
                task.inputs = io_spec.get("inputs", {})
                task.expected_outputs = io_spec.get("outputs", {})
            except:
                task.inputs = {"requirement": {"type": "string", "required": True}}
                task.expected_outputs = {"result": {"type": "object"}}
        
        return tasks
    
    async def _set_validation_criteria(self, tasks: List[ExecutableTask]) -> List[ExecutableTask]:
        """ê²€ì¦ ê¸°ì¤€ ì„¤ì •.
        
        Args:
            tasks: íƒœìŠ¤í¬ ëª©ë¡
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ íƒœìŠ¤í¬ ëª©ë¡
        """
        for task in tasks:
            prompt = f"""
            Set validation criteria for this task:
            
            Task: {task.name}
            Type: {task.type.value}
            Expected Outputs: {json.dumps(task.expected_outputs, indent=2)[:500]}
            
            Create 3-5 specific validation criteria that must be met for success.
            
            Response format (JSON):
            {{
                "criteria": [
                    "Criterion 1",
                    "Criterion 2",
                    ...
                ]
            }}
            """
            
            response = await self.ai_provider.complete(prompt)
            
            try:
                data = json.loads(response)
                task.validation_criteria = data.get("criteria", [])
            except:
                task.validation_criteria = [
                    "Task completes without errors",
                    "Output matches expected structure",
                    "Performance within acceptable limits"
                ]
        
        return tasks
    
    async def _generate_execution_scripts(self, tasks: List[ExecutableTask], context: Dict[str, Any]) -> List[ExecutableTask]:
        """ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±.
        
        Args:
            tasks: íƒœìŠ¤í¬ ëª©ë¡
            context: í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ íƒœìŠ¤í¬ ëª©ë¡
        """
        for task in tasks:
            prompt = f"""
            Generate execution script for this task:
            
            Task: {task.name}
            Agent: {task.agent}
            Inputs: {json.dumps(task.inputs, indent=2)[:500]}
            
            Create a Python script snippet that:
            1. Prepares inputs
            2. Calls the agent
            3. Processes outputs
            4. Handles errors
            
            Response format: Python code as string
            """
            
            response = await self.ai_provider.complete(prompt)
            
            # ì½”ë“œ ì¶”ì¶œ
            if "```python" in response:
                code = response.split("```python")[1].split("```")[0].strip()
            elif "```" in response:
                code = response.split("```")[1].split("```")[0].strip()
            else:
                code = response.strip()
            
            task.execution_script = code
        
        return tasks
    
    async def _create_rollback_plans(self, tasks: List[ExecutableTask]) -> List[ExecutableTask]:
        """ë¡¤ë°± ê³„íš ìˆ˜ë¦½.
        
        Args:
            tasks: íƒœìŠ¤í¬ ëª©ë¡
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ íƒœìŠ¤í¬ ëª©ë¡
        """
        for task in tasks:
            prompt = f"""
            Create a rollback plan for this task:
            
            Task: {task.name}
            Type: {task.type.value}
            
            Define steps to undo this task if it fails or needs to be reverted.
            
            Response: Rollback steps as text
            """
            
            response = await self.ai_provider.complete(prompt)
            task.rollback_plan = response.strip()
        
        return tasks
    
    async def _optimize_dependencies(self, tasks: List[ExecutableTask], optimization_goal: str) -> List[ExecutableTask]:
        """ì˜ì¡´ì„± ìµœì í™”.
        
        Args:
            tasks: íƒœìŠ¤í¬ ëª©ë¡
            optimization_goal: ìµœì í™” ëª©í‘œ (speed/cost/quality)
            
        Returns:
            ìµœì í™”ëœ íƒœìŠ¤í¬ ëª©ë¡
        """
        prompt = f"""
        Optimize task dependencies for {optimization_goal}:
        
        Current tasks: {len(tasks)}
        Current dependencies: {sum(len(t.dependencies) for t in tasks)}
        
        Optimization goal: {optimization_goal}
        - speed: Maximize parallelization
        - cost: Minimize resource usage
        - quality: Ensure thorough validation
        - balanced: Balance all factors
        
        Suggest dependency optimizations.
        
        Response format (JSON):
        {{
            "remove_dependencies": [["task1", "task2"], ...],
            "add_dependencies": [["task3", "task4"], ...],
            "parallel_groups": [["task5", "task6"], ...]
        }}
        """
        
        response = await self.ai_provider.complete(prompt)
        
        try:
            optimizations = json.loads(response)
            # Apply optimizations (simplified)
            # In real implementation, would modify task dependencies
        except:
            pass
        
        return tasks
    
    async def _adjust_priorities(self, tasks: List[ExecutableTask], requirement: str) -> List[ExecutableTask]:
        """ìš°ì„ ìˆœìœ„ ì¡°ì •.
        
        Args:
            tasks: íƒœìŠ¤í¬ ëª©ë¡
            requirement: ìš”êµ¬ì‚¬í•­
            
        Returns:
            ìš°ì„ ìˆœìœ„ ì¡°ì •ëœ íƒœìŠ¤í¬ ëª©ë¡
        """
        # Critical tasks get higher priority
        for task in tasks:
            if task.metadata.get("critical"):
                task.priority = min(task.priority + 2, 10)
            
            # Adjust based on type
            if task.type == TaskType.TESTING:
                task.priority = min(task.priority + 1, 10)
            elif task.type == TaskType.DOCUMENTATION:
                task.priority = max(task.priority - 1, 1)
        
        return tasks
    
    async def _set_retry_strategies(self, tasks: List[ExecutableTask]) -> List[ExecutableTask]:
        """ë¦¬íŠ¸ë¼ì´ ì „ëµ ì„¤ì •.
        
        Args:
            tasks: íƒœìŠ¤í¬ ëª©ë¡
            
        Returns:
            ì—…ë°ì´íŠ¸ëœ íƒœìŠ¤í¬ ëª©ë¡
        """
        for task in tasks:
            # Default retry strategy
            task.retry_strategy = {
                "max_attempts": 3 if task.metadata.get("critical") else 2,
                "backoff": "exponential",
                "initial_delay": 5,  # seconds
                "max_delay": 60,  # seconds
                "retry_on": ["timeout", "api_error", "resource_unavailable"]
            }
        
        return tasks
    
    async def _validate_tasks(self, tasks: List[ExecutableTask]) -> Dict[str, Any]:
        """íƒœìŠ¤í¬ ê²€ì¦.
        
        Args:
            tasks: íƒœìŠ¤í¬ ëª©ë¡
            
        Returns:
            ê²€ì¦ ê²°ê³¼
        """
        validation = {
            "valid": True,
            "issues": [],
            "warnings": [],
            "stats": {
                "total_tasks": len(tasks),
                "by_type": {},
                "by_agent": {},
                "critical_count": sum(1 for t in tasks if t.metadata.get("critical"))
            }
        }
        
        # Typeë³„ ì¹´ìš´íŠ¸
        for task in tasks:
            task_type = task.type.value
            validation["stats"]["by_type"][task_type] = validation["stats"]["by_type"].get(task_type, 0) + 1
            
            # Agentë³„ ì¹´ìš´íŠ¸
            agent = task.agent
            validation["stats"]["by_agent"][agent] = validation["stats"]["by_agent"].get(agent, 0) + 1
            
            # ê²€ì¦ ì²´í¬
            if not task.inputs:
                validation["warnings"].append(f"Task {task.id} has no inputs defined")
            if not task.validation_criteria:
                validation["warnings"].append(f"Task {task.id} has no validation criteria")
            if not task.execution_script:
                validation["warnings"].append(f"Task {task.id} has no execution script")
        
        # Circular dependency check
        if self._has_circular_dependencies(tasks):
            validation["valid"] = False
            validation["issues"].append("Circular dependencies detected")
        
        return validation
    
    def _has_circular_dependencies(self, tasks: List[ExecutableTask]) -> bool:
        """ìˆœí™˜ ì˜ì¡´ì„± ì²´í¬.
        
        Args:
            tasks: íƒœìŠ¤í¬ ëª©ë¡
            
        Returns:
            ìˆœí™˜ ì˜ì¡´ì„± ì¡´ì¬ ì—¬ë¶€
        """
        # Simplified check - in real implementation would use graph algorithms
        task_ids = {t.id for t in tasks}
        for task in tasks:
            for dep in task.dependencies:
                if dep not in task_ids:
                    return True  # Invalid dependency
        return False
    
    def _get_execution_order(self, tasks: List[ExecutableTask]) -> List[str]:
        """ì‹¤í–‰ ìˆœì„œ ê²°ì •.
        
        Args:
            tasks: íƒœìŠ¤í¬ ëª©ë¡
            
        Returns:
            ì‹¤í–‰ ìˆœì„œ (íƒœìŠ¤í¬ ID ëª©ë¡)
        """
        # Topological sort (simplified)
        ordered = []
        remaining = tasks.copy()
        
        while remaining:
            # Find tasks with no dependencies
            ready = [t for t in remaining if not t.dependencies or all(d in ordered for d in t.dependencies)]
            
            if not ready:
                # Circular dependency or error
                break
            
            # Sort by priority
            ready.sort(key=lambda t: t.priority, reverse=True)
            
            # Add to ordered list
            for task in ready:
                ordered.append(task.id)
                remaining.remove(task)
        
        return ordered
    
    def _identify_critical_path(self, tasks: List[ExecutableTask]) -> List[str]:
        """í¬ë¦¬í‹°ì»¬ íŒ¨ìŠ¤ ì‹ë³„.
        
        Args:
            tasks: íƒœìŠ¤í¬ ëª©ë¡
            
        Returns:
            í¬ë¦¬í‹°ì»¬ íŒ¨ìŠ¤ íƒœìŠ¤í¬ ID ëª©ë¡
        """
        # Simplified critical path - tasks marked as critical
        critical_path = [t.id for t in tasks if t.metadata.get("critical")]
        
        if not critical_path:
            # Fallback: longest dependency chain
            critical_path = [tasks[0].id] if tasks else []
        
        return critical_path