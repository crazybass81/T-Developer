import asyncio
import time
import statistics
from typing import List, Dict, Any
from .nl_input_agent import NLInputAgent
from .nl_input_multimodal import MultimodalInputProcessor
from .requirement_clarification import RequirementClarificationSystem

class NLAgentBenchmark:
    """NL Agent ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self):
        self.nl_agent = NLInputAgent()
        self.multimodal_processor = MultimodalInputProcessor(self.nl_agent)
        self.clarification_system = RequirementClarificationSystem()
        
    async def run_performance_benchmark(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        
        test_cases = [
            "ê°„ë‹¨í•œ í• ì¼ ê´€ë¦¬ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
            "Reactì™€ Node.jsë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ì±„íŒ… ì•±",
            "Python Djangoë¡œ REST API ì„œë²„ êµ¬ì¶•",
            "ëª¨ë°”ì¼ ì•± - iOS/Android ì§€ì›, ì‚¬ì§„ ê³µìœ  ê¸°ëŠ¥",
            "ì „ììƒê±°ë˜ í”Œë«í¼ - ê²°ì œ, ì£¼ë¬¸ê´€ë¦¬, ì¬ê³ ê´€ë¦¬"
        ]
        
        results = {
            'total_tests': len(test_cases),
            'individual_times': [],
            'accuracy_scores': [],
            'memory_usage': [],
            'error_count': 0
        }
        
        for i, test_case in enumerate(test_cases):
            print(f"Running test {i+1}/{len(test_cases)}: {test_case[:50]}...")
            
            try:
                # ì‹œê°„ ì¸¡ì •
                start_time = time.perf_counter()
                
                # NL ì²˜ë¦¬ ì‹¤í–‰
                requirements = await self.nl_agent.process_description(test_case)
                
                end_time = time.perf_counter()
                execution_time = end_time - start_time
                
                results['individual_times'].append(execution_time)
                
                # ì •í™•ë„ í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                accuracy = self._evaluate_accuracy(test_case, requirements)
                results['accuracy_scores'].append(accuracy)
                
                print(f"  Time: {execution_time:.3f}s, Accuracy: {accuracy:.2f}")
                
            except Exception as e:
                print(f"  Error: {e}")
                results['error_count'] += 1
        
        # í†µê³„ ê³„ì‚°
        if results['individual_times']:
            results['avg_time'] = statistics.mean(results['individual_times'])
            results['max_time'] = max(results['individual_times'])
            results['min_time'] = min(results['individual_times'])
            results['std_dev'] = statistics.stdev(results['individual_times']) if len(results['individual_times']) > 1 else 0
        
        if results['accuracy_scores']:
            results['avg_accuracy'] = statistics.mean(results['accuracy_scores'])
            
        results['success_rate'] = (len(test_cases) - results['error_count']) / len(test_cases)
        
        return results
    
    def _evaluate_accuracy(self, input_text: str, requirements) -> float:
        """ì •í™•ë„ í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)"""
        score = 0.0
        
        # í”„ë¡œì íŠ¸ íƒ€ì… ì •í™•ë„
        if self._check_project_type_accuracy(input_text, requirements.project_type):
            score += 0.3
            
        # ê¸°ìˆ  ìŠ¤íƒ ì¶”ì¶œ ì •í™•ë„
        if self._check_tech_stack_accuracy(input_text, requirements.technology_preferences):
            score += 0.3
            
        # ìš”êµ¬ì‚¬í•­ ì¶”ì¶œ ì •í™•ë„
        if len(requirements.technical_requirements) > 0:
            score += 0.2
            
        # êµ¬ì¡°í™” ì •í™•ë„
        if requirements.description and requirements.project_type:
            score += 0.2
            
        return min(score, 1.0)
    
    def _check_project_type_accuracy(self, input_text: str, project_type: str) -> bool:
        """í”„ë¡œì íŠ¸ íƒ€ì… ì •í™•ë„ í™•ì¸"""
        input_lower = input_text.lower()
        
        type_keywords = {
            'web_application': ['ì›¹', 'web', 'ì›¹ì•±'],
            'mobile_application': ['ëª¨ë°”ì¼', 'mobile', 'ì•±', 'ios', 'android'],
            'api_service': ['api', 'rest', 'ì„œë²„'],
            'cli_tool': ['cli', 'command', 'ëª…ë ¹ì–´']
        }
        
        expected_keywords = type_keywords.get(project_type, [])
        return any(keyword in input_lower for keyword in expected_keywords)
    
    def _check_tech_stack_accuracy(self, input_text: str, tech_prefs: Dict) -> bool:
        """ê¸°ìˆ  ìŠ¤íƒ ì •í™•ë„ í™•ì¸"""
        input_lower = input_text.lower()
        
        # ì…ë ¥ì— ëª…ì‹œëœ ê¸°ìˆ ì´ ì¶”ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸
        mentioned_techs = []
        if 'react' in input_lower:
            mentioned_techs.append('react')
        if 'node' in input_lower or 'nodejs' in input_lower:
            mentioned_techs.append('node')
        if 'python' in input_lower:
            mentioned_techs.append('python')
        if 'django' in input_lower:
            mentioned_techs.append('django')
            
        if not mentioned_techs:
            return True  # ê¸°ìˆ ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì •í™•ë„ í†µê³¼
            
        # ì¶”ì¶œëœ ê¸°ìˆ ê³¼ ë¹„êµ
        extracted_techs = []
        for category, techs in tech_prefs.items():
            if isinstance(techs, list):
                extracted_techs.extend([t.lower() for t in techs])
            elif isinstance(techs, str):
                extracted_techs.append(techs.lower())
                
        return any(tech in extracted_techs for tech in mentioned_techs)
    
    async def run_load_test(self, concurrent_requests: int = 10) -> Dict[str, Any]:
        """ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        test_case = "Reactì™€ Node.jsë¥¼ ì‚¬ìš©í•œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”"
        
        async def single_request():
            start_time = time.perf_counter()
            try:
                await self.nl_agent.process_description(test_case)
                return time.perf_counter() - start_time, True
            except Exception:
                return time.perf_counter() - start_time, False
        
        print(f"Running load test with {concurrent_requests} concurrent requests...")
        
        # ë™ì‹œ ìš”ì²­ ì‹¤í–‰
        start_time = time.perf_counter()
        tasks = [single_request() for _ in range(concurrent_requests)]
        results = await asyncio.gather(*tasks)
        total_time = time.perf_counter() - start_time
        
        # ê²°ê³¼ ë¶„ì„
        times = [r[0] for r in results]
        successes = [r[1] for r in results]
        
        return {
            'concurrent_requests': concurrent_requests,
            'total_time': total_time,
            'avg_response_time': statistics.mean(times),
            'max_response_time': max(times),
            'min_response_time': min(times),
            'success_rate': sum(successes) / len(successes),
            'requests_per_second': concurrent_requests / total_time
        }

async def main():
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    benchmark = NLAgentBenchmark()
    
    print("ğŸš€ Starting NL Agent Performance Benchmark...")
    
    # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    perf_results = await benchmark.run_performance_benchmark()
    
    print("\nğŸ“Š Performance Results:")
    print(f"  Average Time: {perf_results.get('avg_time', 0):.3f}s")
    print(f"  Max Time: {perf_results.get('max_time', 0):.3f}s")
    print(f"  Average Accuracy: {perf_results.get('avg_accuracy', 0):.2f}")
    print(f"  Success Rate: {perf_results.get('success_rate', 0):.2f}")
    
    # ë¶€í•˜ í…ŒìŠ¤íŠ¸
    load_results = await benchmark.run_load_test(concurrent_requests=5)
    
    print("\nâš¡ Load Test Results:")
    print(f"  Concurrent Requests: {load_results['concurrent_requests']}")
    print(f"  Requests/Second: {load_results['requests_per_second']:.2f}")
    print(f"  Average Response Time: {load_results['avg_response_time']:.3f}s")
    print(f"  Success Rate: {load_results['success_rate']:.2f}")
    
    # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
    print("\nâœ… Target Achievement:")
    print(f"  Response Time < 2s: {'âœ…' if perf_results.get('avg_time', 0) < 2.0 else 'âŒ'}")
    print(f"  Accuracy > 95%: {'âœ…' if perf_results.get('avg_accuracy', 0) > 0.95 else 'âŒ'}")
    print(f"  Success Rate > 99%: {'âœ…' if perf_results.get('success_rate', 0) > 0.99 else 'âŒ'}")

if __name__ == "__main__":
    asyncio.run(main())