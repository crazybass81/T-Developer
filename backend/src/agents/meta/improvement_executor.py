"""
Improvement Executor - Automated improvement application with A/B testing
Size: < 6.5KB | Performance: < 3Î¼s
Day 29: Phase 2 - ServiceImproverAgent
"""

import asyncio
import hashlib
import json
import time
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple


@dataclass
class ImprovementPlan:
    """Improvement execution plan"""
    
    id: str
    type: str  # performance, quality, security, usability
    description: str
    changes: List[Dict[str, Any]]
    estimated_impact: float
    risk_level: str
    rollback_plan: Dict[str, Any]


@dataclass
class ABTestConfig:
    """A/B test configuration"""
    
    test_id: str
    variant_a: str  # Control
    variant_b: str  # Treatment
    traffic_split: float  # % to variant B
    duration_hours: int
    success_metrics: List[str]
    minimum_sample_size: int


@dataclass
class ExecutionResult:
    """Improvement execution result"""
    
    success: bool
    improvement_id: str
    metrics_before: Dict[str, float]
    metrics_after: Dict[str, float]
    actual_impact: float
    ab_test_results: Optional[Dict[str, Any]]
    rollback_performed: bool
    execution_time: float


class ImprovementExecutor:
    """Execute improvements with A/B testing and rollback"""
    
    def __init__(self):
        self.active_tests = {}
        self.execution_history = []
        self.rollback_stack = []
        
    async def execute(self,
                     plan: ImprovementPlan,
                     enable_ab_test: bool = False,
                     auto_rollback: bool = True) -> ExecutionResult:
        """Execute improvement plan"""
        
        start_time = time.time()
        
        # Capture baseline metrics
        metrics_before = await self._capture_metrics()
        
        # Setup A/B test if enabled
        ab_test_results = None
        if enable_ab_test:
            ab_config = self._create_ab_config(plan)
            ab_test_results = await self._run_ab_test(plan, ab_config)
            
            # Check if treatment wins
            if not self._is_treatment_winner(ab_test_results):
                return ExecutionResult(
                    success=False,
                    improvement_id=plan.id,
                    metrics_before=metrics_before,
                    metrics_after=metrics_before,
                    actual_impact=0.0,
                    ab_test_results=ab_test_results,
                    rollback_performed=False,
                    execution_time=time.time() - start_time
                )
        
        # Apply improvements
        success = await self._apply_changes(plan)
        
        if not success:
            return ExecutionResult(
                success=False,
                improvement_id=plan.id,
                metrics_before=metrics_before,
                metrics_after=metrics_before,
                actual_impact=0.0,
                ab_test_results=ab_test_results,
                rollback_performed=False,
                execution_time=time.time() - start_time
            )
        
        # Capture post-improvement metrics
        metrics_after = await self._capture_metrics()
        
        # Calculate actual impact
        actual_impact = self._calculate_impact(metrics_before, metrics_after)
        
        # Check if improvement meets expectations
        rollback_performed = False
        if auto_rollback and actual_impact < plan.estimated_impact * 0.5:
            # Rollback if impact is less than 50% of estimate
            rollback_performed = await self._rollback(plan)
            if rollback_performed:
                metrics_after = await self._capture_metrics()
                actual_impact = 0.0
        
        # Record execution
        result = ExecutionResult(
            success=success and not rollback_performed,
            improvement_id=plan.id,
            metrics_before=metrics_before,
            metrics_after=metrics_after,
            actual_impact=actual_impact,
            ab_test_results=ab_test_results,
            rollback_performed=rollback_performed,
            execution_time=time.time() - start_time
        )
        
        self.execution_history.append(result)
        
        return result
    
    def _create_ab_config(self, plan: ImprovementPlan) -> ABTestConfig:
        """Create A/B test configuration"""
        
        test_id = hashlib.md5(f"{plan.id}_{time.time()}".encode()).hexdigest()[:8]
        
        # Determine traffic split based on risk
        traffic_split = {
            "low": 0.5,
            "medium": 0.3,
            "high": 0.1
        }.get(plan.risk_level, 0.2)
        
        return ABTestConfig(
            test_id=test_id,
            variant_a="control",
            variant_b=f"treatment_{plan.id}",
            traffic_split=traffic_split,
            duration_hours=24,
            success_metrics=["performance", "error_rate", "user_satisfaction"],
            minimum_sample_size=100
        )
    
    async def _run_ab_test(self,
                          plan: ImprovementPlan,
                          config: ABTestConfig) -> Dict[str, Any]:
        """Run A/B test"""
        
        # Simplified A/B test simulation
        # In production, this would integrate with experimentation platform
        
        self.active_tests[config.test_id] = {
            "plan": plan,
            "config": config,
            "start_time": time.time(),
            "variant_a_metrics": {},
            "variant_b_metrics": {}
        }
        
        # Simulate test results
        await asyncio.sleep(0.1)  # Simulate test duration
        
        # Generate mock results
        variant_a_performance = 1.0
        variant_b_performance = 1.0 + plan.estimated_impact
        
        # Add some randomness
        import random
        variant_b_performance *= random.uniform(0.8, 1.2)
        
        results = {
            "test_id": config.test_id,
            "duration": 0.1,
            "sample_size": 1000,
            "variant_a": {
                "performance": variant_a_performance,
                "error_rate": 0.01,
                "user_satisfaction": 0.75
            },
            "variant_b": {
                "performance": variant_b_performance,
                "error_rate": 0.008,
                "user_satisfaction": 0.78
            },
            "statistical_significance": variant_b_performance > variant_a_performance * 1.05,
            "winner": "variant_b" if variant_b_performance > variant_a_performance else "variant_a"
        }
        
        del self.active_tests[config.test_id]
        
        return results
    
    def _is_treatment_winner(self, ab_results: Dict[str, Any]) -> bool:
        """Check if treatment variant wins A/B test"""
        
        if not ab_results:
            return True  # No test, proceed
        
        return (ab_results.get("winner") == "variant_b" and
                ab_results.get("statistical_significance", False))
    
    async def _apply_changes(self, plan: ImprovementPlan) -> bool:
        """Apply improvement changes"""
        
        # Store rollback information
        self.rollback_stack.append({
            "plan": plan,
            "timestamp": time.time(),
            "state_before": self._capture_state()
        })
        
        # Apply each change
        for change in plan.changes:
            success = await self._apply_single_change(change)
            if not success:
                return False
        
        return True
    
    async def _apply_single_change(self, change: Dict[str, Any]) -> bool:
        """Apply single change"""
        
        change_type = change.get("type")
        
        if change_type == "config":
            return self._update_config(change.get("config", {}))
        elif change_type == "code":
            return await self._deploy_code(change.get("code", {}))
        elif change_type == "infrastructure":
            return await self._update_infrastructure(change.get("infra", {}))
        else:
            return True  # Unknown type, skip
    
    def _update_config(self, config: Dict[str, Any]) -> bool:
        """Update configuration"""
        # Simplified config update
        return True
    
    async def _deploy_code(self, code: Dict[str, Any]) -> bool:
        """Deploy code changes"""
        # Simplified code deployment
        await asyncio.sleep(0.01)
        return True
    
    async def _update_infrastructure(self, infra: Dict[str, Any]) -> bool:
        """Update infrastructure"""
        # Simplified infrastructure update
        await asyncio.sleep(0.01)
        return True
    
    async def _capture_metrics(self) -> Dict[str, float]:
        """Capture current metrics"""
        
        # Simplified metrics capture
        # In production, would integrate with monitoring systems
        
        return {
            "response_time": 100.0,  # ms
            "error_rate": 0.01,
            "throughput": 1000.0,  # req/s
            "cpu_usage": 50.0,  # %
            "memory_usage": 60.0,  # %
            "user_satisfaction": 0.75
        }
    
    def _calculate_impact(self,
                         before: Dict[str, float],
                         after: Dict[str, float]) -> float:
        """Calculate improvement impact"""
        
        # Calculate improvement for each metric
        improvements = []
        
        for metric in ["response_time", "error_rate"]:
            if metric in before and metric in after:
                # Lower is better for these metrics
                improvement = (before[metric] - after[metric]) / max(0.01, before[metric])
                improvements.append(improvement)
        
        for metric in ["throughput", "user_satisfaction"]:
            if metric in before and metric in after:
                # Higher is better for these metrics
                improvement = (after[metric] - before[metric]) / max(0.01, before[metric])
                improvements.append(improvement)
        
        # Average improvement
        if improvements:
            return sum(improvements) / len(improvements)
        return 0.0
    
    async def _rollback(self, plan: ImprovementPlan) -> bool:
        """Rollback improvements"""
        
        if not self.rollback_stack:
            return False
        
        # Get rollback info
        rollback_info = None
        for item in reversed(self.rollback_stack):
            if item["plan"].id == plan.id:
                rollback_info = item
                break
        
        if not rollback_info:
            return False
        
        # Execute rollback plan
        if plan.rollback_plan:
            for step in plan.rollback_plan.get("steps", []):
                await self._execute_rollback_step(step)
        
        # Restore state
        self._restore_state(rollback_info["state_before"])
        
        # Remove from stack
        self.rollback_stack.remove(rollback_info)
        
        return True
    
    async def _execute_rollback_step(self, step: Dict[str, Any]) -> bool:
        """Execute single rollback step"""
        await asyncio.sleep(0.01)  # Simulate rollback
        return True
    
    def _capture_state(self) -> Dict[str, Any]:
        """Capture current state for rollback"""
        return {
            "timestamp": time.time(),
            "config": {},  # Current config
            "version": "1.0.0"  # Current version
        }
    
    def _restore_state(self, state: Dict[str, Any]) -> bool:
        """Restore previous state"""
        # Simplified state restoration
        return True
    
    def get_active_tests(self) -> List[Dict[str, Any]]:
        """Get active A/B tests"""
        return [
            {
                "test_id": test_id,
                "plan_id": data["plan"].id,
                "duration": time.time() - data["start_time"]
            }
            for test_id, data in self.active_tests.items()
        ]
    
    def get_execution_history(self) -> List[ExecutionResult]:
        """Get execution history"""
        return self.execution_history[-10:]  # Last 10 executions
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get executor metrics"""
        
        successful = sum(1 for r in self.execution_history if r.success)
        total = len(self.execution_history)
        
        return {
            "total_executions": total,
            "successful_executions": successful,
            "success_rate": successful / max(1, total),
            "active_tests": len(self.active_tests),
            "rollback_stack_size": len(self.rollback_stack)
        }


# Global instance
executor = None


def get_executor() -> ImprovementExecutor:
    """Get or create executor instance"""
    global executor
    if not executor:
        executor = ImprovementExecutor()
    return executor


async def main():
    """Test improvement executor"""
    executor = get_executor()
    
    # Create improvement plan
    plan = ImprovementPlan(
        id="imp_001",
        type="performance",
        description="Optimize database queries",
        changes=[
            {"type": "config", "config": {"cache_enabled": True}},
            {"type": "code", "code": {"optimization": "query_cache"}}
        ],
        estimated_impact=0.3,
        risk_level="low",
        rollback_plan={
            "steps": [
                {"action": "disable_cache"},
                {"action": "restore_queries"}
            ]
        }
    )
    
    # Execute with A/B testing
    result = await executor.execute(plan, enable_ab_test=True)
    
    print("Execution Result:")
    print(f"  Success: {result.success}")
    print(f"  Actual Impact: {result.actual_impact:.2%}")
    print(f"  Rollback: {result.rollback_performed}")
    print(f"  Execution Time: {result.execution_time:.2f}s")
    
    if result.ab_test_results:
        print("\nA/B Test Results:")
        print(f"  Winner: {result.ab_test_results['winner']}")
        print(f"  Significant: {result.ab_test_results['statistical_significance']}")
    
    print("\nMetrics Change:")
    for metric in result.metrics_before:
        before = result.metrics_before[metric]
        after = result.metrics_after.get(metric, before)
        change = ((after - before) / before * 100) if before else 0
        print(f"  {metric}: {before:.2f} â {after:.2f} ({change:+.1f}%)")


if __name__ == "__main__":
    asyncio.run(main())