"""
T-Developer Evolution System - Performance Benchmark
ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë„êµ¬

ëª©í‘œ:
- ì¸ìŠ¤í„´ìŠ¤í™” ì‹œê°„: < 3Î¼s
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: < 6.5KB
- ì‘ë‹µ ì‹œê°„: < 100ms
"""

import gc
import importlib.util
import json
import os
import statistics
import sys
import time
import tracemalloc
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import psutil


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""

    agent_name: str
    instantiation_us: float
    memory_kb: float
    response_time_ms: float
    cpu_percent: float
    passes_constraints: bool
    iterations: int
    std_dev_us: float
    min_us: float
    max_us: float
    percentile_95_us: float


class AgentBenchmark:
    """
    ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ
    ì •ë°€í•œ ì„±ëŠ¥ ì¸¡ì • ë° ì œì•½ ì¡°ê±´ ê²€ì¦
    """

    # ì„±ëŠ¥ ëª©í‘œ
    TARGET_INSTANTIATION_US = 3.0
    TARGET_MEMORY_KB = 6.5
    TARGET_RESPONSE_MS = 100.0

    # ë²¤ì¹˜ë§ˆí¬ ì„¤ì •
    WARMUP_ITERATIONS = 10
    BENCHMARK_ITERATIONS = 100

    def __init__(self, results_path: str = None):
        """ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.results_path = Path(results_path or "backend/data/benchmarks/results.json")
        self.results_path.parent.mkdir(parents=True, exist_ok=True)

        self.results: List[BenchmarkResult] = []
        self.current_process = psutil.Process()

    def benchmark_agent(self, agent_path: str, iterations: int = None) -> BenchmarkResult:
        """
        ë‹¨ì¼ ì—ì´ì „íŠ¸ ë²¤ì¹˜ë§ˆí‚¹

        Args:
            agent_path: ì—ì´ì „íŠ¸ íŒŒì¼ ê²½ë¡œ
            iterations: ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸: BENCHMARK_ITERATIONS)

        Returns:
            ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        agent_file = Path(agent_path)
        if not agent_file.exists():
            raise FileNotFoundError(f"Agent file not found: {agent_path}")

        iterations = iterations or self.BENCHMARK_ITERATIONS
        agent_name = agent_file.stem

        print(f"\nâš¡ Benchmarking: {agent_name}")
        print(f"   Iterations: {iterations}")

        # ì›Œë°ì—…
        self._warmup(agent_path)

        # ì¸ìŠ¤í„´ìŠ¤í™” ì†ë„ ì¸¡ì •
        instantiation_times = self._measure_instantiation_speed(agent_path, iterations)

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        memory_kb = self._measure_memory_usage(agent_path)

        # ì‘ë‹µ ì‹œê°„ ì¸¡ì •
        response_time_ms = self._measure_response_time(agent_path)

        # CPU ì‚¬ìš©ë¥  ì¸¡ì •
        cpu_percent = self._measure_cpu_usage(agent_path)

        # í†µê³„ ê³„ì‚°
        avg_instantiation = statistics.mean(instantiation_times)
        std_dev = statistics.stdev(instantiation_times) if len(instantiation_times) > 1 else 0
        min_time = min(instantiation_times)
        max_time = max(instantiation_times)
        percentile_95 = self._calculate_percentile(instantiation_times, 95)

        # ì œì•½ ì¡°ê±´ í™•ì¸
        passes = (
            avg_instantiation <= self.TARGET_INSTANTIATION_US
            and memory_kb <= self.TARGET_MEMORY_KB
            and response_time_ms <= self.TARGET_RESPONSE_MS
        )

        result = BenchmarkResult(
            agent_name=agent_name,
            instantiation_us=round(avg_instantiation, 3),
            memory_kb=round(memory_kb, 2),
            response_time_ms=round(response_time_ms, 2),
            cpu_percent=round(cpu_percent, 1),
            passes_constraints=passes,
            iterations=iterations,
            std_dev_us=round(std_dev, 3),
            min_us=round(min_time, 3),
            max_us=round(max_time, 3),
            percentile_95_us=round(percentile_95, 3),
        )

        self.results.append(result)
        self._print_result(result)

        return result

    def _warmup(self, agent_path: str):
        """JIT ì»´íŒŒì¼ëŸ¬ ì›Œë°ì—…"""
        try:
            for _ in range(self.WARMUP_ITERATIONS):
                spec = importlib.util.spec_from_file_location("warmup_agent", agent_path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                del module
                gc.collect()
        except Exception:
            pass  # ì›Œë°ì—… ì‹¤íŒ¨ëŠ” ë¬´ì‹œ

    def _measure_instantiation_speed(self, agent_path: str, iterations: int) -> List[float]:
        """
        ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤í™” ì†ë„ ì •ë°€ ì¸¡ì •

        Returns:
            ê° ë°˜ë³µì˜ ì¸ìŠ¤í„´ìŠ¤í™” ì‹œê°„ (ë§ˆì´í¬ë¡œì´ˆ)
        """
        times = []

        for i in range(iterations):
            try:
                # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
                gc.collect()
                gc.disable()  # ì¸¡ì • ì¤‘ GC ë¹„í™œì„±í™”

                # ëª¨ë“ˆ ë¡œë“œ ë° ì¸ìŠ¤í„´ìŠ¤í™” ì‹œê°„ ì¸¡ì •
                start = time.perf_counter_ns()

                spec = importlib.util.spec_from_file_location(f"test_agent_{i}", agent_path)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)

                # ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ ì°¾ê¸° ë° ì¸ìŠ¤í„´ìŠ¤í™”
                for name in dir(module):
                    obj = getattr(module, name)
                    if isinstance(obj, type) and "Agent" in name:
                        instance = obj()
                        break

                end = time.perf_counter_ns()

                # ë‚˜ë…¸ì´ˆë¥¼ ë§ˆì´í¬ë¡œì´ˆë¡œ ë³€í™˜
                elapsed_us = (end - start) / 1000
                times.append(elapsed_us)

                # ì •ë¦¬
                del module
                if "instance" in locals():
                    del instance

            except Exception as e:
                print(f"âš ï¸ Iteration {i} failed: {e}")
                times.append(float("inf"))

            finally:
                gc.enable()
                gc.collect()

        # ì´ìƒì¹˜ ì œê±° (ìƒìœ„/í•˜ìœ„ 5%)
        times.sort()
        trim_count = max(1, len(times) // 20)
        if len(times) > 20:
            times = times[trim_count:-trim_count]

        return times

    def _measure_memory_usage(self, agent_path: str) -> float:
        """
        ì—ì´ì „íŠ¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •

        Returns:
            ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (KB)
        """
        # íŒŒì¼ í¬ê¸° (ì •ì  ë©”ëª¨ë¦¬)
        file_size_kb = Path(agent_path).stat().st_size / 1024

        # ëŸ°íƒ€ì„ ë©”ëª¨ë¦¬ ì¸¡ì •
        tracemalloc.start()
        gc.collect()

        try:
            spec = importlib.util.spec_from_file_location("memory_test", agent_path)
            module = importlib.util.module_from_spec(spec)

            snapshot1 = tracemalloc.take_snapshot()
            spec.loader.exec_module(module)

            # ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            for name in dir(module):
                obj = getattr(module, name)
                if isinstance(obj, type) and "Agent" in name:
                    instance = obj()
                    break

            snapshot2 = tracemalloc.take_snapshot()

            # ë©”ëª¨ë¦¬ ì°¨ì´ ê³„ì‚°
            stats = snapshot2.compare_to(snapshot1, "lineno")
            runtime_memory_kb = sum(stat.size_diff for stat in stats) / 1024

        except Exception as e:
            print(f"âš ï¸ Memory measurement failed: {e}")
            runtime_memory_kb = 0

        finally:
            tracemalloc.stop()

        # ì •ì  + ë™ì  ë©”ëª¨ë¦¬
        total_memory_kb = file_size_kb + max(0, runtime_memory_kb)

        return total_memory_kb

    def _measure_response_time(self, agent_path: str) -> float:
        """
        ì—ì´ì „íŠ¸ ì‘ë‹µ ì‹œê°„ ì¸¡ì •

        Returns:
            í‰ê·  ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
        """
        try:
            spec = importlib.util.spec_from_file_location("response_test", agent_path)
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)

            # ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            agent_instance = None
            for name in dir(module):
                obj = getattr(module, name)
                if isinstance(obj, type) and "Agent" in name:
                    agent_instance = obj()
                    break

            if not agent_instance:
                return 0.0

            # process ë©”ì„œë“œê°€ ìˆìœ¼ë©´ í˜¸ì¶œ
            if hasattr(agent_instance, "process"):
                test_input = {"test": "data"}

                times = []
                for _ in range(10):
                    start = time.perf_counter()
                    agent_instance.process(test_input)
                    end = time.perf_counter()
                    times.append((end - start) * 1000)  # ë°€ë¦¬ì´ˆ

                return statistics.mean(times)

        except Exception:
            pass

        return 0.0

    def _measure_cpu_usage(self, agent_path: str) -> float:
        """
        ì—ì´ì „íŠ¸ CPU ì‚¬ìš©ë¥  ì¸¡ì •

        Returns:
            í‰ê·  CPU ì‚¬ìš©ë¥  (%)
        """
        try:
            # ì´ˆê¸° CPU ìƒíƒœ
            self.current_process.cpu_percent()
            time.sleep(0.1)

            # ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ CPU ì¸¡ì •
            spec = importlib.util.spec_from_file_location("cpu_test", agent_path)
            module = importlib.util.module_from_spec(spec)

            cpu_samples = []
            for _ in range(5):
                spec.loader.exec_module(module)
                cpu_samples.append(self.current_process.cpu_percent())
                time.sleep(0.01)

            return statistics.mean(cpu_samples)

        except Exception:
            return 0.0

    def _calculate_percentile(self, values: List[float], percentile: int) -> float:
        """ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°"""
        if not values:
            return 0.0

        sorted_values = sorted(values)
        index = int(len(sorted_values) * percentile / 100)
        return sorted_values[min(index, len(sorted_values) - 1)]

    def _print_result(self, result: BenchmarkResult):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¶œë ¥"""
        status = "âœ… PASS" if result.passes_constraints else "âŒ FAIL"

        print(f"\n{status} Benchmark Results for {result.agent_name}")
        print("=" * 50)
        print(f"Instantiation Speed:")
        print(f"  Average: {result.instantiation_us}Î¼s (target: <{self.TARGET_INSTANTIATION_US}Î¼s)")
        print(f"  Std Dev: {result.std_dev_us}Î¼s")
        print(f"  Min/Max: {result.min_us}Î¼s / {result.max_us}Î¼s")
        print(f"  95th percentile: {result.percentile_95_us}Î¼s")
        print(f"\nMemory Usage: {result.memory_kb}KB (target: <{self.TARGET_MEMORY_KB}KB)")
        print(f"Response Time: {result.response_time_ms}ms (target: <{self.TARGET_RESPONSE_MS}ms)")
        print(f"CPU Usage: {result.cpu_percent}%")

    def benchmark_directory(self, directory: str) -> List[BenchmarkResult]:
        """
        ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  ì—ì´ì „íŠ¸ ë²¤ì¹˜ë§ˆí‚¹

        Args:
            directory: ì—ì´ì „íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ

        Returns:
            ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        agent_dir = Path(directory)
        if not agent_dir.exists():
            raise FileNotFoundError(f"Directory not found: {directory}")

        agent_files = list(agent_dir.rglob("*agent*.py"))
        print(f"\nğŸš€ Found {len(agent_files)} agents to benchmark")

        results = []
        for agent_file in agent_files:
            try:
                result = self.benchmark_agent(str(agent_file))
                results.append(result)
            except Exception as e:
                print(f"âŒ Failed to benchmark {agent_file.name}: {e}")

        self._save_results()
        self._print_summary()

        return results

    def _save_results(self):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥"""
        data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "results": [asdict(r) for r in self.results],
            "summary": self._calculate_summary(),
        }

        with open(self.results_path, "w") as f:
            json.dump(data, f, indent=2)

    def _calculate_summary(self) -> Dict[str, Any]:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ìš”ì•½"""
        if not self.results:
            return {}

        passing = sum(1 for r in self.results if r.passes_constraints)
        total = len(self.results)

        return {
            "total_agents": total,
            "passing_agents": passing,
            "pass_rate": f"{(passing/total)*100:.1f}%",
            "avg_instantiation_us": round(
                statistics.mean(r.instantiation_us for r in self.results), 3
            ),
            "avg_memory_kb": round(statistics.mean(r.memory_kb for r in self.results), 2),
            "avg_response_ms": round(statistics.mean(r.response_time_ms for r in self.results), 2),
            "fastest_agent": min(self.results, key=lambda r: r.instantiation_us).agent_name,
            "smallest_agent": min(self.results, key=lambda r: r.memory_kb).agent_name,
        }

    def _print_summary(self):
        """ë²¤ì¹˜ë§ˆí¬ ìš”ì•½ ì¶œë ¥"""
        summary = self._calculate_summary()
        if not summary:
            return

        print("\n" + "=" * 60)
        print("ğŸ“Š BENCHMARK SUMMARY")
        print("=" * 60)

        for key, value in summary.items():
            print(f"{key}: {value}")

        print("\nğŸ† Top Performers:")

        # ì†ë„ ìˆœìœ„
        speed_sorted = sorted(self.results, key=lambda r: r.instantiation_us)[:3]
        print("\nFastest Agents:")
        for i, r in enumerate(speed_sorted, 1):
            print(f"  {i}. {r.agent_name}: {r.instantiation_us}Î¼s")

        # ë©”ëª¨ë¦¬ ìˆœìœ„
        memory_sorted = sorted(self.results, key=lambda r: r.memory_kb)[:3]
        print("\nSmallest Agents:")
        for i, r in enumerate(memory_sorted, 1):
            print(f"  {i}. {r.agent_name}: {r.memory_kb}KB")


# CLI ì¸í„°í˜ì´ìŠ¤
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Agent Performance Benchmark")
    parser.add_argument("path", help="Agent file or directory path")
    parser.add_argument("--iterations", type=int, default=100, help="Number of iterations")
    parser.add_argument(
        "--quick-test", action="store_true", help="Quick test with fewer iterations"
    )

    args = parser.parse_args()

    benchmark = AgentBenchmark()

    iterations = 10 if args.quick_test else args.iterations

    path = Path(args.path)
    if path.is_file():
        benchmark.benchmark_agent(str(path), iterations)
    elif path.is_dir():
        benchmark.benchmark_directory(str(path))
    else:
        print(f"âŒ Path not found: {args.path}")
        sys.exit(1)

    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼
    if args.quick_test:
        results = benchmark.results
        if results and results[0].passes_constraints:
            print("\nâœ… PASS - Agent meets performance constraints")
            sys.exit(0)
        else:
            print("\nâŒ FAIL - Agent violates performance constraints")
            sys.exit(1)
