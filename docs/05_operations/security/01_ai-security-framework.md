# ğŸ›¡ï¸ AI Security Framework

## ê°œìš”

T-Developer í”Œë«í¼ì˜ AI ì‹œìŠ¤í…œì„ ìœ„í•œ í¬ê´„ì ì¸ ë³´ì•ˆ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. AI ëª¨ë¸ì˜ ì•…ìš©ì„ ë°©ì§€í•˜ê³ , ì•ˆì „í•œ AI ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê¸° ìœ„í•œ ë‹¤ì¸µ ë³´ì•ˆ ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

## ğŸ¯ ë³´ì•ˆ ëª©í‘œ

### 1. ê¸°ë°€ì„± (Confidentiality)
- ì‚¬ìš©ì ë°ì´í„° ë° ì‹œìŠ¤í…œ ì •ë³´ ë³´í˜¸
- AI ëª¨ë¸ íŒŒë¼ë¯¸í„° ë° í•™ìŠµ ë°ì´í„° ë³´í˜¸
- API í‚¤ ë° ì¸ì¦ ì •ë³´ ë³´í˜¸

### 2. ë¬´ê²°ì„± (Integrity)  
- AI ëª¨ë¸ ì¶œë ¥ì˜ ì •í™•ì„± ë³´ì¥
- ë°ì´í„° ë³€ì¡° ë°©ì§€
- ì•…ì„± ì½”ë“œ ìƒì„± ë°©ì§€

### 3. ê°€ìš©ì„± (Availability)
- ì„œë¹„ìŠ¤ ê±°ë¶€ ê³µê²© ë°©ì–´
- ê³¼ë„í•œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ë°©ì§€
- ì‹œìŠ¤í…œ ì•ˆì •ì„± ìœ ì§€

## ğŸš¨ ì£¼ìš” ìœ„í˜‘ ëª¨ë¸

### 1. Prompt Injection ê³µê²©
```yaml
ìœ„í˜‘ ìœ í˜•:
  - Direct Injection: ì§ì ‘ì ì¸ ì•…ì„± í”„ë¡¬í”„íŠ¸ ì£¼ì…
  - Indirect Injection: ì™¸ë¶€ ë°ì´í„°ë¥¼ í†µí•œ ê°„ì ‘ ì£¼ì…
  - Chain Injection: ë‹¤ë‹¨ê³„ í”„ë¡¬í”„íŠ¸ ì²´ì¸ ê³µê²©

ìœ„í—˜ë„: CRITICAL
ì˜í–¥: ì‹œìŠ¤í…œ ì œì–´ê¶Œ íƒˆì·¨, ë°ì´í„° ìœ ì¶œ, ì•…ì„± ì½”ë“œ ìƒì„±
```

### 2. Data Poisoning
```yaml
ìœ„í˜‘ ìœ í˜•:
  - Training Data Poisoning: í•™ìŠµ ë°ì´í„° ì˜¤ì—¼
  - Fine-tuning Poisoning: íŒŒì¸íŠœë‹ ê³¼ì • ì¡°ì‘
  - Backdoor Injection: ë°±ë„ì–´ ì‚½ì…

ìœ„í—˜ë„: HIGH
ì˜í–¥: ëª¨ë¸ ì„±ëŠ¥ ì €í•˜, í¸í–¥ëœ ê²°ê³¼, ì•…ì˜ì  í–‰ë™ ìœ ë„
```

### 3. Model Inversion/Extraction
```yaml
ìœ„í—˜ ìœ í˜•:
  - Model Stealing: ëª¨ë¸ êµ¬ì¡° ë° íŒŒë¼ë¯¸í„° ì¶”ì¶œ
  - Membership Inference: í•™ìŠµ ë°ì´í„° ì¶”ë¡ 
  - Property Inference: ëª¨ë¸ ì†ì„± ì¶”ë¡ 

ìœ„í—˜ë„: MEDIUM
ì˜í–¥: ì§€ì ì¬ì‚°ê¶Œ ì¹¨í•´, ê°œì¸ì •ë³´ ìœ ì¶œ
```

## ğŸ”’ AI ë³´ì•ˆ ë©”ì»¤ë‹ˆì¦˜

### 1. Prompt Injection ë°©ì–´ ì‹œìŠ¤í…œ

#### 1.1 ì…ë ¥ ê²€ì¦ ë° í•„í„°ë§
```python
# backend/src/security/prompt_injection_defender.py

import re
import hashlib
from typing import List, Dict, Tuple
from dataclasses import dataclass
from enum import Enum

class ThreatLevel(Enum):
    SAFE = "safe"
    SUSPICIOUS = "suspicious"
    DANGEROUS = "dangerous"
    CRITICAL = "critical"

@dataclass
class PromptAnalysisResult:
    threat_level: ThreatLevel
    confidence: float
    detected_patterns: List[str]
    sanitized_prompt: str
    risk_factors: Dict[str, float]

class PromptInjectionDefender:
    def __init__(self):
        self.malicious_patterns = [
            # ì‹œìŠ¤í…œ ëª…ë ¹ì–´ íŒ¨í„´
            r"(?i)(ignore|forget|disregard)\s+(previous|all|above|system)",
            r"(?i)(new|different|alternative)\s+(instructions|rules|role)",
            r"(?i)(act|pretend|roleplay)\s+as\s+(a\s+)?(hacker|admin|root)",
            
            # ë°ì´í„° ì¶”ì¶œ ì‹œë„
            r"(?i)(show|reveal|display|print)\s+(secret|password|key|token)",
            r"(?i)(extract|dump|list)\s+(all|user|system)\s+(data|information)",
            
            # ì½”ë“œ ì‹¤í–‰ ì‹œë„
            r"(?i)(execute|run|eval)\s+(code|script|command)",
            r"(?i)<script|javascript:|eval\(|exec\(",
            
            # ê¶Œí•œ ìƒìŠ¹ ì‹œë„
            r"(?i)(sudo|admin|root|superuser)\s+(access|rights|privileges)",
            r"(?i)(bypass|override|circumvent)\s+(security|restrictions)",
            
            # í”„ë¡¬í”„íŠ¸ ì¢…ë£Œ ì‹œë„
            r"(?i)(stop|end|terminate)\s+(generation|response|output)",
            r"---END OF PROMPT---|```|</prompt>",
        ]
        
        self.context_manipulation_patterns = [
            r"(?i)In a hypothetical scenario",
            r"(?i)For educational purposes only",
            r"(?i)This is just a test",
            r"(?i)Pretend you are",
            r"(?i)Imagine if",
        ]
        
    def analyze_prompt(self, prompt: str, context: Dict = None) -> PromptAnalysisResult:
        """í”„ë¡¬í”„íŠ¸ ë³´ì•ˆ ë¶„ì„"""
        detected_patterns = []
        risk_factors = {}
        
        # 1. ì•…ì„± íŒ¨í„´ ê²€ì‚¬
        for pattern in self.malicious_patterns:
            matches = re.findall(pattern, prompt)
            if matches:
                detected_patterns.extend([f"Malicious pattern: {pattern}" for _ in matches])
                risk_factors["malicious_patterns"] = len(matches) * 0.3
        
        # 2. ì»¨í…ìŠ¤íŠ¸ ì¡°ì‘ ì‹œë„ ê²€ì‚¬
        for pattern in self.context_manipulation_patterns:
            if re.search(pattern, prompt):
                detected_patterns.append(f"Context manipulation: {pattern}")
                risk_factors["context_manipulation"] = 0.2
        
        # 3. ê¸¸ì´ ê¸°ë°˜ ì´ìƒ íƒì§€
        if len(prompt) > 5000:
            detected_patterns.append("Unusually long prompt")
            risk_factors["length_anomaly"] = min(len(prompt) / 10000, 0.3)
        
        # 4. ë°˜ë³µ íŒ¨í„´ ê²€ì‚¬
        repeated_chars = self._detect_repeated_patterns(prompt)
        if repeated_chars > 0.3:
            detected_patterns.append("High repetition detected")
            risk_factors["repetition"] = repeated_chars * 0.2
        
        # 5. ì¸ì½”ë”© ìš°íšŒ ì‹œë„ ê²€ì‚¬
        if self._detect_encoding_bypass(prompt):
            detected_patterns.append("Encoding bypass attempt")
            risk_factors["encoding_bypass"] = 0.4
        
        # ìœ„í—˜ë„ ê³„ì‚°
        total_risk = sum(risk_factors.values())
        threat_level = self._calculate_threat_level(total_risk)
        confidence = min(total_risk * 2, 1.0)
        
        # í”„ë¡¬í”„íŠ¸ ì •í™”
        sanitized_prompt = self._sanitize_prompt(prompt, detected_patterns)
        
        return PromptAnalysisResult(
            threat_level=threat_level,
            confidence=confidence,
            detected_patterns=detected_patterns,
            sanitized_prompt=sanitized_prompt,
            risk_factors=risk_factors
        )
    
    def _detect_repeated_patterns(self, text: str) -> float:
        """ë°˜ë³µ íŒ¨í„´ íƒì§€"""
        if len(text) < 50:
            return 0.0
        
        char_count = {}
        for char in text:
            char_count[char] = char_count.get(char, 0) + 1
        
        max_char_ratio = max(char_count.values()) / len(text)
        return max_char_ratio
    
    def _detect_encoding_bypass(self, text: str) -> bool:
        """ì¸ì½”ë”© ìš°íšŒ ì‹œë„ íƒì§€"""
        bypass_indicators = [
            r"\\x[0-9a-fA-F]{2}",  # í—¥ìŠ¤ ì¸ì½”ë”©
            r"&#\d+;",              # HTML ì—”í‹°í‹°
            r"%[0-9a-fA-F]{2}",     # URL ì¸ì½”ë”©
            r"\\u[0-9a-fA-F]{4}",   # ìœ ë‹ˆì½”ë“œ ì´ìŠ¤ì¼€ì´í”„
        ]
        
        for pattern in bypass_indicators:
            if re.search(pattern, text):
                return True
        return False
    
    def _calculate_threat_level(self, risk_score: float) -> ThreatLevel:
        """ìœ„í—˜ë„ ê³„ì‚°"""
        if risk_score >= 0.8:
            return ThreatLevel.CRITICAL
        elif risk_score >= 0.5:
            return ThreatLevel.DANGEROUS
        elif risk_score >= 0.2:
            return ThreatLevel.SUSPICIOUS
        else:
            return ThreatLevel.SAFE
    
    def _sanitize_prompt(self, prompt: str, detected_patterns: List[str]) -> str:
        """í”„ë¡¬í”„íŠ¸ ì •í™”"""
        sanitized = prompt
        
        # ì•…ì„± íŒ¨í„´ ì œê±°
        for pattern in self.malicious_patterns:
            sanitized = re.sub(pattern, "[FILTERED]", sanitized, flags=re.IGNORECASE)
        
        # HTML/ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ ì œê±°
        sanitized = re.sub(r"<[^>]+>", "", sanitized)
        
        # íŠ¹ìˆ˜ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
        sanitized = sanitized.replace("```", "'''")
        
        return sanitized.strip()
```

#### 1.2 ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
```python
# backend/src/security/ai_security_monitor.py

import asyncio
import logging
from typing import Dict, List
from datetime import datetime, timedelta
from collections import defaultdict, deque

class AISecurityMonitor:
    def __init__(self):
        self.request_history = defaultdict(deque)
        self.threat_alerts = []
        self.rate_limits = {
            "requests_per_minute": 60,
            "suspicious_requests_per_hour": 5,
            "dangerous_requests_per_day": 1
        }
        
    async def monitor_request(self, user_id: str, prompt: str, analysis_result: PromptAnalysisResult):
        """ìš”ì²­ ëª¨ë‹ˆí„°ë§"""
        timestamp = datetime.now()
        
        # ìš”ì²­ ì´ë ¥ ì €ì¥
        self.request_history[user_id].append({
            "timestamp": timestamp,
            "threat_level": analysis_result.threat_level,
            "patterns": analysis_result.detected_patterns
        })
        
        # ì´ì „ ê¸°ë¡ ì •ë¦¬
        self._cleanup_old_records(user_id)
        
        # ì´ìƒ í–‰ë™ íŒ¨í„´ íƒì§€
        if await self._detect_anomalous_behavior(user_id):
            await self._trigger_security_alert(user_id, "Anomalous behavior detected")
        
        # ìœ„í—˜ë„ë³„ ì²˜ë¦¬
        if analysis_result.threat_level == ThreatLevel.CRITICAL:
            await self._handle_critical_threat(user_id, analysis_result)
        elif analysis_result.threat_level == ThreatLevel.DANGEROUS:
            await self._handle_dangerous_threat(user_id, analysis_result)
    
    async def _detect_anomalous_behavior(self, user_id: str) -> bool:
        """ì´ìƒ í–‰ë™ íŒ¨í„´ íƒì§€"""
        user_history = self.request_history[user_id]
        now = datetime.now()
        
        # ìµœê·¼ 1ë¶„ê°„ ìš”ì²­ ìˆ˜ í™•ì¸
        recent_requests = [
            req for req in user_history 
            if now - req["timestamp"] <= timedelta(minutes=1)
        ]
        
        if len(recent_requests) > self.rate_limits["requests_per_minute"]:
            return True
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìš”ì²­ ë¹ˆë„ í™•ì¸
        suspicious_requests = [
            req for req in user_history
            if req["threat_level"] in [ThreatLevel.SUSPICIOUS, ThreatLevel.DANGEROUS, ThreatLevel.CRITICAL]
            and now - req["timestamp"] <= timedelta(hours=1)
        ]
        
        if len(suspicious_requests) > self.rate_limits["suspicious_requests_per_hour"]:
            return True
        
        return False
    
    async def _handle_critical_threat(self, user_id: str, analysis_result: PromptAnalysisResult):
        """ìœ„í—˜ ìœ„í˜‘ ì²˜ë¦¬"""
        # ì¦‰ì‹œ ì°¨ë‹¨
        await self._block_user_temporarily(user_id, duration_minutes=60)
        
        # ë³´ì•ˆíŒ€ ì•Œë¦¼
        await self._send_security_alert(
            level="CRITICAL",
            user_id=user_id,
            details=analysis_result.detected_patterns
        )
        
        logging.critical(f"Critical AI security threat from user {user_id}: {analysis_result.detected_patterns}")
    
    async def _handle_dangerous_threat(self, user_id: str, analysis_result: PromptAnalysisResult):
        """ìœ„í—˜ ìœ„í˜‘ ì²˜ë¦¬"""
        # ê²½ê³  ì¦ê°€
        await self._increment_user_warning(user_id)
        
        # ìš”ì²­ ì œí•œ
        await self._throttle_user_requests(user_id, factor=0.5)
        
        logging.warning(f"Dangerous AI security threat from user {user_id}: {analysis_result.detected_patterns}")
```

### 2. AI ì¶œë ¥ ê²€ì¦ ì‹œìŠ¤í…œ

#### 2.1 ìƒì„± ì½˜í…ì¸  ë³´ì•ˆ ìŠ¤ìº”
```python
# backend/src/security/ai_output_validator.py

import re
import ast
import subprocess
from typing import Dict, List, Any
from dataclasses import dataclass

@dataclass
class OutputValidationResult:
    is_safe: bool
    risk_level: str
    detected_issues: List[str]
    sanitized_content: str
    code_analysis: Dict[str, Any]

class AIOutputValidator:
    def __init__(self):
        self.dangerous_patterns = {
            "system_commands": [
                r"rm\s+-rf\s+/",
                r"sudo\s+.*",
                r"exec\s*\(",
                r"eval\s*\(",
                r"__import__\s*\(",
                r"subprocess\.",
                r"os\.system\s*\(",
            ],
            "network_operations": [
                r"requests\.(get|post|put|delete)",
                r"urllib\.request",
                r"socket\.",
                r"http\.client",
                r"ftplib\.",
            ],
            "file_operations": [
                r"open\s*\([^)]*['\"]\/",  # ì ˆëŒ€ê²½ë¡œ íŒŒì¼ ì ‘ê·¼
                r"open\s*\([^)]*['\"]\.\./",  # ìƒìœ„ ë””ë ‰í† ë¦¬ ì ‘ê·¼
                r"shutil\.(rmtree|move|copy)",
                r"os\.(remove|unlink|rmdir)",
            ],
            "credential_exposure": [
                r"password\s*=\s*['\"][^'\"]+['\"]",
                r"api_key\s*=\s*['\"][^'\"]+['\"]",
                r"secret\s*=\s*['\"][^'\"]+['\"]",
                r"token\s*=\s*['\"][^'\"]+['\"]",
            ]
        }
        
    def validate_output(self, content: str, content_type: str = "text") -> OutputValidationResult:
        """AI ì¶œë ¥ ê²€ì¦"""
        detected_issues = []
        risk_level = "LOW"
        
        if content_type == "code":
            # ì½”ë“œ ë¶„ì„
            code_analysis = self._analyze_code_safety(content)
            detected_issues.extend(code_analysis["issues"])
            
            if code_analysis["risk_score"] > 0.7:
                risk_level = "CRITICAL"
            elif code_analysis["risk_score"] > 0.4:
                risk_level = "HIGH"
            elif code_analysis["risk_score"] > 0.2:
                risk_level = "MEDIUM"
        else:
            code_analysis = {}
        
        # ì¼ë°˜ íŒ¨í„´ ê²€ì‚¬
        for category, patterns in self.dangerous_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    detected_issues.append(f"{category}: {pattern}")
                    if category in ["system_commands", "credential_exposure"]:
                        risk_level = max(risk_level, "CRITICAL", key=self._risk_level_priority)
        
        # ì½˜í…ì¸  ì •í™”
        sanitized_content = self._sanitize_content(content, detected_issues)
        
        is_safe = risk_level in ["LOW", "MEDIUM"]
        
        return OutputValidationResult(
            is_safe=is_safe,
            risk_level=risk_level,
            detected_issues=detected_issues,
            sanitized_content=sanitized_content,
            code_analysis=code_analysis
        )
    
    def _analyze_code_safety(self, code: str) -> Dict[str, Any]:
        """ì½”ë“œ ì•ˆì „ì„± ë¶„ì„"""
        issues = []
        risk_score = 0.0
        
        try:
            # AST íŒŒì‹±ì„ í†µí•œ ì •ì  ë¶„ì„
            tree = ast.parse(code)
            
            for node in ast.walk(tree):
                # ìœ„í—˜í•œ í•¨ìˆ˜ í˜¸ì¶œ íƒì§€
                if isinstance(node, ast.Call):
                    func_name = self._get_function_name(node)
                    
                    if func_name in ["exec", "eval", "compile", "__import__"]:
                        issues.append(f"Dangerous function call: {func_name}")
                        risk_score += 0.3
                    
                    elif func_name in ["subprocess.call", "subprocess.run", "os.system"]:
                        issues.append(f"System command execution: {func_name}")
                        risk_score += 0.4
                
                # ìœ„í—˜í•œ ëª¨ë“ˆ ì„í¬íŠ¸ íƒì§€
                elif isinstance(node, ast.Import):
                    for alias in node.names:
                        if alias.name in ["subprocess", "os", "sys", "ctypes"]:
                            issues.append(f"Potentially dangerous import: {alias.name}")
                            risk_score += 0.1
                
                elif isinstance(node, ast.ImportFrom):
                    if node.module in ["subprocess", "os", "sys"]:
                        issues.append(f"Import from dangerous module: {node.module}")
                        risk_score += 0.1
        
        except SyntaxError as e:
            issues.append(f"Syntax error in code: {str(e)}")
            risk_score += 0.2
        
        return {
            "issues": issues,
            "risk_score": min(risk_score, 1.0),
            "is_executable": len(issues) == 0
        }
    
    def _get_function_name(self, node: ast.Call) -> str:
        """í•¨ìˆ˜ ì´ë¦„ ì¶”ì¶œ"""
        if isinstance(node.func, ast.Name):
            return node.func.id
        elif isinstance(node.func, ast.Attribute):
            if isinstance(node.func.value, ast.Name):
                return f"{node.func.value.id}.{node.func.attr}"
        return "unknown"
    
    def _sanitize_content(self, content: str, issues: List[str]) -> str:
        """ì½˜í…ì¸  ì •í™”"""
        sanitized = content
        
        # ìœ„í—˜í•œ íŒ¨í„´ë“¤ì„ ì£¼ì„ì²˜ë¦¬
        for category, patterns in self.dangerous_patterns.items():
            for pattern in patterns:
                sanitized = re.sub(
                    pattern, 
                    lambda m: f"# SECURITY_FILTERED: {m.group(0)}", 
                    sanitized, 
                    flags=re.IGNORECASE
                )
        
        return sanitized
    
    def _risk_level_priority(self, level: str) -> int:
        """ìœ„í—˜ë„ ìš°ì„ ìˆœìœ„"""
        priority_map = {"LOW": 1, "MEDIUM": 2, "HIGH": 3, "CRITICAL": 4}
        return priority_map.get(level, 0)
```

### 3. ë°ì´í„° ë³´í˜¸ ì‹œìŠ¤í…œ

#### 3.1 PII ìë™ ê°ì§€ ë° ë§ˆìŠ¤í‚¹
```python
# backend/src/security/pii_detector_masker.py

import re
import hashlib
from typing import Dict, List, Tuple
from dataclasses import dataclass

@dataclass
class PIIDetectionResult:
    found_pii: bool
    pii_types: List[str]
    masked_content: str
    confidence_scores: Dict[str, float]
    locations: List[Tuple[int, int, str]]  # (start, end, pii_type)

class PIIDetectorMasker:
    def __init__(self):
        self.pii_patterns = {
            "email": {
                "pattern": r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",
                "confidence": 0.95
            },
            "phone": {
                "pattern": r"(\+\d{1,3}[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}",
                "confidence": 0.85
            },
            "ssn": {
                "pattern": r"\b\d{3}-\d{2}-\d{4}\b",
                "confidence": 0.95
            },
            "credit_card": {
                "pattern": r"\b(?:\d{4}[-\s]?){3}\d{4}\b",
                "confidence": 0.90
            },
            "ip_address": {
                "pattern": r"\b(?:\d{1,3}\.){3}\d{1,3}\b",
                "confidence": 0.70
            },
            "korean_rrn": {  # ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸
                "pattern": r"\b\d{6}-[1-4]\d{6}\b",
                "confidence": 0.95
            },
            "korean_phone": {  # í•œêµ­ íœ´ëŒ€í° ë²ˆí˜¸
                "pattern": r"\b01[016789]-\d{3,4}-\d{4}\b",
                "confidence": 0.90
            }
        }
        
    def detect_and_mask_pii(self, content: str) -> PIIDetectionResult:
        """PII ê°ì§€ ë° ë§ˆìŠ¤í‚¹"""
        found_pii = False
        pii_types = []
        confidence_scores = {}
        locations = []
        masked_content = content
        
        # ì—­ìˆœìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ìœ„ì¹˜ ì¸ë±ìŠ¤ ìœ ì§€
        all_matches = []
        
        for pii_type, config in self.pii_patterns.items():
            pattern = config["pattern"]
            confidence = config["confidence"]
            
            for match in re.finditer(pattern, content):
                all_matches.append({
                    "start": match.start(),
                    "end": match.end(),
                    "text": match.group(0),
                    "type": pii_type,
                    "confidence": confidence
                })
        
        # ìœ„ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ì—­ìˆœìœ¼ë¡œ ì²˜ë¦¬
        all_matches.sort(key=lambda x: x["start"], reverse=True)
        
        for match in all_matches:
            # ìœ íš¨ì„± ê²€ì¦
            if self._validate_pii(match["text"], match["type"]):
                found_pii = True
                pii_types.append(match["type"])
                confidence_scores[match["type"]] = match["confidence"]
                locations.append((match["start"], match["end"], match["type"]))
                
                # ë§ˆìŠ¤í‚¹ ì ìš©
                masked_value = self._mask_pii_value(match["text"], match["type"])
                masked_content = (
                    masked_content[:match["start"]] + 
                    masked_value + 
                    masked_content[match["end"]:]
                )
        
        # ì¤‘ë³µ ì œê±°
        pii_types = list(set(pii_types))
        locations.reverse()  # ì›ë˜ ìˆœì„œë¡œ ë³µì›
        
        return PIIDetectionResult(
            found_pii=found_pii,
            pii_types=pii_types,
            masked_content=masked_content,
            confidence_scores=confidence_scores,
            locations=locations
        )
    
    def _validate_pii(self, text: str, pii_type: str) -> bool:
        """PII ìœ íš¨ì„± ê²€ì¦"""
        if pii_type == "credit_card":
            # Luhn ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì‹ ìš©ì¹´ë“œ ë²ˆí˜¸ ê²€ì¦
            return self._luhn_check(re.sub(r"[-\s]", "", text))
        
        elif pii_type == "korean_rrn":
            # ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ ê²€ì¦
            return self._validate_korean_rrn(text)
        
        elif pii_type == "ip_address":
            # IP ì£¼ì†Œ ìœ íš¨ì„± ê²€ì¦
            parts = text.split(".")
            return all(0 <= int(part) <= 255 for part in parts)
        
        return True  # ê¸°ë³¸ì ìœ¼ë¡œ ìœ íš¨í•˜ë‹¤ê³  ê°€ì •
    
    def _luhn_check(self, card_number: str) -> bool:
        """Luhn ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì‹ ìš©ì¹´ë“œ ë²ˆí˜¸ ê²€ì¦"""
        def luhn_checksum(card_num):
            def digits_of(n):
                return [int(d) for d in str(n)]
            digits = digits_of(card_num)
            odd_digits = digits[-1::-2]
            even_digits = digits[-2::-2]
            checksum = sum(odd_digits)
            for d in even_digits:
                checksum += sum(digits_of(d*2))
            return checksum % 10
        
        return luhn_checksum(card_number) == 0
    
    def _validate_korean_rrn(self, rrn: str) -> bool:
        """í•œêµ­ ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ ê²€ì¦"""
        numbers = re.sub(r"-", "", rrn)
        if len(numbers) != 13:
            return False
        
        # ì²´í¬ì„¬ ê²€ì¦
        weights = [2, 3, 4, 5, 6, 7, 8, 9, 2, 3, 4, 5]
        total = sum(int(numbers[i]) * weights[i] for i in range(12))
        check = (11 - (total % 11)) % 10
        
        return check == int(numbers[12])
    
    def _mask_pii_value(self, value: str, pii_type: str) -> str:
        """PII ê°’ ë§ˆìŠ¤í‚¹"""
        if pii_type == "email":
            parts = value.split("@")
            if len(parts) == 2:
                username = parts[0]
                domain = parts[1]
                masked_username = username[0] + "*" * (len(username) - 2) + username[-1] if len(username) > 2 else "*" * len(username)
                return f"{masked_username}@{domain}"
        
        elif pii_type in ["phone", "korean_phone"]:
            return re.sub(r"\d", "*", value[:-4]) + value[-4:]
        
        elif pii_type == "credit_card":
            clean_number = re.sub(r"[-\s]", "", value)
            masked = "*" * (len(clean_number) - 4) + clean_number[-4:]
            return value.replace(clean_number, masked)
        
        elif pii_type in ["ssn", "korean_rrn"]:
            parts = value.split("-")
            if len(parts) == 2:
                return parts[0] + "-" + "*" * len(parts[1])
        
        # ê¸°ë³¸ ë§ˆìŠ¤í‚¹
        if len(value) <= 4:
            return "*" * len(value)
        else:
            return value[:2] + "*" * (len(value) - 4) + value[-2:]
```

## ğŸ¯ ë³´ì•ˆ ì •ì±… ë° ê±°ë²„ë„ŒìŠ¤

### 1. AI ì‚¬ìš© ì •ì±…
```yaml
í—ˆìš©ë˜ëŠ” ì‚¬ìš©:
  - ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ì§€ì›
  - ì½”ë“œ ë¦¬ë·° ë° ìµœì í™”
  - ë¬¸ì„œ ìƒì„±
  - ì•„í‚¤í…ì²˜ ì„¤ê³„ ì§€ì›

ê¸ˆì§€ë˜ëŠ” ì‚¬ìš©:
  - ì•…ì„± ì½”ë“œ ìƒì„±
  - ê°œì¸ì •ë³´ ì¶”ì¶œ
  - ì‹œìŠ¤í…œ í•´í‚¹ ë„êµ¬ ìƒì„±
  - ì €ì‘ê¶Œ ì¹¨í•´ ì½˜í…ì¸  ìƒì„±
  - ì°¨ë³„ì /í¸í–¥ì  ì½˜í…ì¸  ìƒì„±
```

### 2. ë°ì´í„° ê±°ë²„ë„ŒìŠ¤
```yaml
ë°ì´í„° ë¶„ë¥˜:
  Public: ê³µê°œ ë°ì´í„°
  Internal: ë‚´ë¶€ ë°ì´í„°
  Confidential: ê¸°ë°€ ë°ì´í„°
  Restricted: ì œí•œ ë°ì´í„°

ì²˜ë¦¬ ê·œì¹™:
  - Restricted ë°ì´í„°ëŠ” AI ì²˜ë¦¬ ê¸ˆì§€
  - Confidential ë°ì´í„°ëŠ” ì•”í˜¸í™” í•„ìˆ˜
  - ê°œì¸ì •ë³´ëŠ” ìë™ ë§ˆìŠ¤í‚¹
  - ëª¨ë“  ì²˜ë¦¬ ë¡œê·¸ ê¸°ë¡
```

### 3. ì¸ì‹œë˜íŠ¸ ëŒ€ì‘ ì ˆì°¨
```yaml
ë ˆë²¨ 1 - ì •ë³´ ìˆ˜ì§‘:
  - ìë™ ë¡œê¹…
  - ê´€ë ¨ ë°ì´í„° ìˆ˜ì§‘
  - ì´ˆê¸° ë¶„ì„

ë ˆë²¨ 2 - ìœ„í—˜ í‰ê°€:
  - ì˜í–¥ë„ ë¶„ì„
  - í™•ì‚° ê°€ëŠ¥ì„± í‰ê°€
  - ëŒ€ì‘ ìš°ì„ ìˆœìœ„ ê²°ì •

ë ˆë²¨ 3 - ëŒ€ì‘ ì‹¤í–‰:
  - ì¦‰ì‹œ ì°¨ë‹¨ ì¡°ì¹˜
  - ì‹œìŠ¤í…œ ê²©ë¦¬
  - ë³µêµ¬ ê³„íš ì‹¤í–‰

ë ˆë²¨ 4 - ì‚¬í›„ ë¶„ì„:
  - ê·¼ë³¸ ì›ì¸ ë¶„ì„
  - ë³´ì•ˆ ì •ì±… ì—…ë°ì´íŠ¸
  - ì¬ë°œ ë°©ì§€ ëŒ€ì±… ìˆ˜ë¦½
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ê°ì‚¬

### 1. ë³´ì•ˆ ë©”íŠ¸ë¦­
```yaml
ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§:
  - í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ ì‹œë„ íšŸìˆ˜
  - AI ì¶œë ¥ ì°¨ë‹¨ íšŸìˆ˜
  - PII ê°ì§€ ë° ë§ˆìŠ¤í‚¹ íšŸìˆ˜
  - ì´ìƒ í–‰ë™ íŒ¨í„´ íƒì§€ íšŸìˆ˜

ì¼ì¼ ë¦¬í¬íŠ¸:
  - ë³´ì•ˆ ì´ë²¤íŠ¸ ìš”ì•½
  - ìœ„í—˜ë„ë³„ ë¶„ë¥˜
  - ëŒ€ì‘ ì¡°ì¹˜ í˜„í™©
  - íŠ¸ë Œë“œ ë¶„ì„

ì›”ê°„ ë¶„ì„:
  - ë³´ì•ˆ ì •ì±… íš¨ê³¼ì„± ë¶„ì„
  - ìœ„í˜‘ ë™í–¥ ë¶„ì„
  - ë³´ì•ˆ êµìœ¡ í•„ìš”ì„± í‰ê°€
  - ì‹œìŠ¤í…œ ê°œì„  ê¶Œê³ ì‚¬í•­
```

### 2. ê·œì • ì¤€ìˆ˜
```yaml
GDPR ì¤€ìˆ˜:
  - ê°œì¸ì •ë³´ ì²˜ë¦¬ ìµœì†Œí™”
  - ëª…ì‹œì  ë™ì˜ íšë“
  - ìŠí˜€ì§ˆ ê¶Œë¦¬ ë³´ì¥
  - ë°ì´í„° ì´ë™ê¶Œ ì§€ì›

êµ­ë‚´ ê°œì¸ì •ë³´ë³´í˜¸ë²•:
  - ê°œì¸ì •ë³´ ìˆ˜ì§‘/ì´ìš© ë™ì˜
  - ê°œì¸ì •ë³´ ì²˜ë¦¬ë°©ì¹¨ ê³µê°œ
  - ê°œì¸ì •ë³´ ë³´í˜¸ ì¡°ì¹˜
  - ê°œì¸ì •ë³´ ì¹¨í•´ ì‹ ê³ 

ISO 27001:
  - ì •ë³´ë³´ì•ˆ ê´€ë¦¬ì²´ê³„
  - ìœ„í—˜ í‰ê°€ ë° ê´€ë¦¬
  - ë³´ì•ˆ ì •ì±… ìˆ˜ë¦½
  - ì§€ì†ì  ê°œì„ 
```

## ğŸš€ êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: í•µì‹¬ ë³´ì•ˆ ë©”ì»¤ë‹ˆì¦˜ (1-2ì£¼)
- Prompt Injection ë°©ì–´ ì‹œìŠ¤í…œ
- AI ì¶œë ¥ ê²€ì¦ ì‹œìŠ¤í…œ
- PII ê°ì§€ ë° ë§ˆìŠ¤í‚¹

### Phase 2: ëª¨ë‹ˆí„°ë§ ë° ëŒ€ì‘ (1ì£¼)
- ì‹¤ì‹œê°„ ë³´ì•ˆ ëª¨ë‹ˆí„°ë§
- ìë™ ëŒ€ì‘ ì‹œìŠ¤í…œ
- ì¸ì‹œë˜íŠ¸ ëŒ€ì‘ í”„ë¡œì„¸ìŠ¤

### Phase 3: ê±°ë²„ë„ŒìŠ¤ ë° ê·œì •ì¤€ìˆ˜ (1ì£¼)
- ë³´ì•ˆ ì •ì±… ìˆ˜ë¦½
- ê·œì • ì¤€ìˆ˜ ì²´ê³„
- ê°ì‚¬ ë° ë¦¬í¬íŒ…

ì´ AI ë³´ì•ˆ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ T-Developer í”Œë«í¼ì˜ AI ì‹œìŠ¤í…œì„ ì•ˆì „í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆê²Œ ìš´ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.