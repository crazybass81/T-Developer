# âš¡ Performance Optimization Strategy

## ê°œìš”

T-Developer í”Œë«í¼ì˜ ëŒ€ê·œëª¨ í™•ì¥ì„±ê³¼ ê³ ì„±ëŠ¥ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ ì¢…í•©ì ì¸ ì„±ëŠ¥ ìµœì í™” ì „ëµì…ë‹ˆë‹¤. ìºì‹±, ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”, ë„¤íŠ¸ì›Œí¬ ìµœì í™”, ì• í”Œë¦¬ì¼€ì´ì…˜ ìµœì í™”ë¥¼ í¬í•¨í•œ ë‹¤ì¸µ ì ‘ê·¼ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ ì„±ëŠ¥ ëª©í‘œ

### 1. ì‘ë‹µ ì‹œê°„ ëª©í‘œ
```yaml
API ì‘ë‹µ ì‹œê°„:
  P50: < 100ms
  P95: < 200ms  
  P99: < 500ms
  P99.9: < 1000ms

Agent ì‹¤í–‰ ì‹œê°„:
  Cold Start: < 2ì´ˆ
  Warm Start: < 100ms
  
Database ì¿¼ë¦¬:
  Simple Queries: < 10ms
  Complex Queries: < 100ms
  Analytics Queries: < 1ì´ˆ
```

### 2. ì²˜ë¦¬ëŸ‰ ëª©í‘œ
```yaml
API ì²˜ë¦¬ëŸ‰:
  Peak Load: 10,000 req/sec
  Sustained Load: 5,000 req/sec
  
Agent ì‹¤í–‰:
  Concurrent Executions: 50,000+
  Agent Instantiation: 1,000/sec
  
Database:
  Read Operations: 100,000 ops/sec
  Write Operations: 10,000 ops/sec
```

### 3. ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„± ëª©í‘œ
```yaml
ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:
  Agent Memory: < 6.5KB (Agno Framework)
  API Server: < 2GB per instance
  Database: < 80% of allocated memory

CPU ì‚¬ìš©ëŸ‰:
  Normal Load: < 60%
  Peak Load: < 80%
  
Network:
  Bandwidth Utilization: < 70%
  Connection Pool: 90%+ efficiency
```

## ğŸ—ï¸ ë‹¤ì¸µ ìºì‹± ì•„í‚¤í…ì²˜

### Layer 1: In-Memory Caching (L1)

#### 1.1 Application-Level ìºì‹œ
```python
# backend/src/cache/l1_memory_cache.py

import asyncio
import time
from typing import Any, Dict, Optional, Callable
from dataclasses import dataclass
from collections import OrderedDict
import threading

@dataclass
class CacheEntry:
    value: Any
    expires_at: float
    hit_count: int = 0
    last_accessed: float = 0.0

class L1MemoryCache:
    def __init__(self, max_size: int = 10000, default_ttl: int = 300):
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self.lock = threading.RLock()
        self.stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "sets": 0
        }
        
        # Background cleanup task
        self.cleanup_task = asyncio.create_task(self._cleanup_expired())
    
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        with self.lock:
            if key not in self.cache:
                self.stats["misses"] += 1
                return None
            
            entry = self.cache[key]
            
            # ë§Œë£Œ í™•ì¸
            if time.time() > entry.expires_at:
                del self.cache[key]
                self.stats["misses"] += 1
                return None
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            entry.hit_count += 1
            entry.last_accessed = time.time()
            self.stats["hits"] += 1
            
            # LRU ì—…ë°ì´íŠ¸
            self.cache.move_to_end(key)
            
            return entry.value
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:
        """ìºì‹œì— ê°’ ì €ì¥"""
        with self.lock:
            expires_at = time.time() + (ttl or self.default_ttl)
            
            # ìš©ëŸ‰ ì´ˆê³¼ ì‹œ LRU ì œê±°
            if len(self.cache) >= self.max_size and key not in self.cache:
                self._evict_lru()
            
            self.cache[key] = CacheEntry(
                value=value,
                expires_at=expires_at,
                last_accessed=time.time()
            )
            self.cache.move_to_end(key)
            self.stats["sets"] += 1
    
    def _evict_lru(self) -> None:
        """LRU ì œê±°"""
        if self.cache:
            self.cache.popitem(last=False)
            self.stats["evictions"] += 1
    
    async def _cleanup_expired(self) -> None:
        """ë§Œë£Œëœ í•­ëª© ì •ë¦¬"""
        while True:
            await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì •ë¦¬
            
            with self.lock:
                current_time = time.time()
                expired_keys = [
                    key for key, entry in self.cache.items()
                    if current_time > entry.expires_at
                ]
                
                for key in expired_keys:
                    del self.cache[key]
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        with self.lock:
            total_requests = self.stats["hits"] + self.stats["misses"]
            hit_rate = self.stats["hits"] / total_requests if total_requests > 0 else 0
            
            return {
                "size": len(self.cache),
                "hit_rate": hit_rate,
                "stats": self.stats.copy()
            }
```

#### 1.2 Agent Code ìºì‹±
```python
# backend/src/cache/agent_code_cache.py

import hashlib
import pickle
from typing import Dict, Any, Optional
from dataclasses import dataclass

@dataclass
class CompiledAgent:
    bytecode: bytes
    metadata: Dict[str, Any]
    compiled_at: float
    dependencies: Dict[str, str]  # dependency -> version

class AgentCodeCache:
    def __init__(self):
        self.compiled_agents: Dict[str, CompiledAgent] = {}
        self.code_hashes: Dict[str, str] = {}  # agent_id -> code_hash
    
    def get_compiled_agent(self, agent_id: str, code: str) -> Optional[CompiledAgent]:
        """ì»´íŒŒì¼ëœ ì—ì´ì „íŠ¸ ì¡°íšŒ"""
        code_hash = self._calculate_code_hash(code)
        
        # ì½”ë“œ ë³€ê²½ í™•ì¸
        if agent_id in self.code_hashes:
            if self.code_hashes[agent_id] != code_hash:
                # ì½”ë“œê°€ ë³€ê²½ë¨, ìºì‹œ ë¬´íš¨í™”
                self._invalidate_agent(agent_id)
                return None
        
        return self.compiled_agents.get(agent_id)
    
    def cache_compiled_agent(self, agent_id: str, code: str, 
                           compiled_agent: CompiledAgent) -> None:
        """ì»´íŒŒì¼ëœ ì—ì´ì „íŠ¸ ìºì‹±"""
        code_hash = self._calculate_code_hash(code)
        self.code_hashes[agent_id] = code_hash
        self.compiled_agents[agent_id] = compiled_agent
    
    def _calculate_code_hash(self, code: str) -> str:
        """ì½”ë“œ í•´ì‹œ ê³„ì‚°"""
        return hashlib.sha256(code.encode()).hexdigest()
    
    def _invalidate_agent(self, agent_id: str) -> None:
        """ì—ì´ì „íŠ¸ ìºì‹œ ë¬´íš¨í™”"""
        if agent_id in self.compiled_agents:
            del self.compiled_agents[agent_id]
        if agent_id in self.code_hashes:
            del self.code_hashes[agent_id]
```

### Layer 2: Distributed Cache (L2)

#### 2.1 Redis í´ëŸ¬ìŠ¤í„° ìºì‹±
```python
# backend/src/cache/l2_redis_cache.py

import asyncio
import json
import pickle
import redis.asyncio as redis
from typing import Any, Dict, List, Optional, Union
from dataclasses import dataclass

@dataclass
class RedisClusterConfig:
    nodes: List[Dict[str, Any]]
    password: Optional[str] = None
    ssl: bool = True
    max_connections: int = 100
    retry_on_timeout: bool = True

class L2RedisCache:
    def __init__(self, config: RedisClusterConfig):
        self.config = config
        self.redis_cluster = None
        self.serialization_methods = {
            "json": (json.dumps, json.loads),
            "pickle": (pickle.dumps, pickle.loads)
        }
    
    async def initialize(self):
        """Redis í´ëŸ¬ìŠ¤í„° ì´ˆê¸°í™”"""
        startup_nodes = [
            redis.RedisCluster.ClusterNode(
                host=node["host"], 
                port=node["port"]
            ) for node in self.config.nodes
        ]
        
        self.redis_cluster = redis.RedisCluster(
            startup_nodes=startup_nodes,
            password=self.config.password,
            ssl=self.config.ssl,
            max_connections=self.config.max_connections,
            retry_on_timeout=self.config.retry_on_timeout,
            decode_responses=False  # ë°”ì´ë„ˆë¦¬ ë°ì´í„° ì§€ì›
        )
    
    async def get(self, key: str, serialization: str = "json") -> Optional[Any]:
        """ë¶„ì‚° ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        try:
            raw_value = await self.redis_cluster.get(key)
            if raw_value is None:
                return None
            
            serializer, deserializer = self.serialization_methods[serialization]
            return deserializer(raw_value)
            
        except Exception as e:
            # ìºì‹œ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜ (graceful degradation)
            print(f"Redis cache get error: {e}")
            return None
    
    async def set(self, key: str, value: Any, ttl: int = 3600, 
                 serialization: str = "json") -> bool:
        """ë¶„ì‚° ìºì‹œì— ê°’ ì €ì¥"""
        try:
            serializer, deserializer = self.serialization_methods[serialization]
            serialized_value = serializer(value)
            
            await self.redis_cluster.setex(key, ttl, serialized_value)
            return True
            
        except Exception as e:
            print(f"Redis cache set error: {e}")
            return False
    
    async def mget(self, keys: List[str], serialization: str = "json") -> Dict[str, Any]:
        """ë‹¤ì¤‘ í‚¤ ì¡°íšŒ"""
        try:
            raw_values = await self.redis_cluster.mget(keys)
            serializer, deserializer = self.serialization_methods[serialization]
            
            result = {}
            for key, raw_value in zip(keys, raw_values):
                if raw_value is not None:
                    result[key] = deserializer(raw_value)
            
            return result
            
        except Exception as e:
            print(f"Redis cache mget error: {e}")
            return {}
    
    async def invalidate_pattern(self, pattern: str) -> int:
        """íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìºì‹œ ë¬´íš¨í™”"""
        try:
            # ëª¨ë“  ë…¸ë“œì—ì„œ íŒ¨í„´ ë§¤ì¹­ í‚¤ ê²€ìƒ‰
            keys_to_delete = []
            for node in self.redis_cluster.get_nodes():
                async for key in node.scan_iter(match=pattern):
                    keys_to_delete.append(key)
            
            if keys_to_delete:
                deleted = await self.redis_cluster.delete(*keys_to_delete)
                return deleted
            
            return 0
            
        except Exception as e:
            print(f"Redis cache invalidation error: {e}")
            return 0
```

#### 2.2 ì§€ëŠ¥í˜• ìºì‹œ ì „ëµ
```python
# backend/src/cache/intelligent_cache_strategy.py

import time
import asyncio
from typing import Dict, Any, List, Optional, Callable
from dataclasses import dataclass
from enum import Enum

class CacheStrategy(Enum):
    WRITE_THROUGH = "write_through"
    WRITE_BEHIND = "write_behind"
    CACHE_ASIDE = "cache_aside"
    REFRESH_AHEAD = "refresh_ahead"

@dataclass
class CachePolicy:
    strategy: CacheStrategy
    ttl: int
    refresh_threshold: float = 0.8  # TTLì˜ 80%ì—ì„œ ë°±ê·¸ë¼ìš´ë“œ ìƒˆë¡œê³ ì¹¨
    max_stale_time: int = 300  # ìµœëŒ€ 5ë¶„ ë™ì•ˆ stale ë°ì´í„° í—ˆìš©

class IntelligentCacheManager:
    def __init__(self, l1_cache: L1MemoryCache, l2_cache: L2RedisCache):
        self.l1_cache = l1_cache
        self.l2_cache = l2_cache
        self.policies: Dict[str, CachePolicy] = {}
        self.refresh_queue = asyncio.Queue()
        
        # ë°±ê·¸ë¼ìš´ë“œ ìƒˆë¡œê³ ì¹¨ ì›Œì»¤ ì‹œì‘
        asyncio.create_task(self._refresh_worker())
    
    def set_policy(self, key_pattern: str, policy: CachePolicy):
        """í‚¤ íŒ¨í„´ë³„ ìºì‹œ ì •ì±… ì„¤ì •"""
        self.policies[key_pattern] = policy
    
    async def get(self, key: str, fetch_func: Callable = None) -> Optional[Any]:
        """ë‹¤ì¸µ ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        # L1 ìºì‹œ í™•ì¸
        value = self.l1_cache.get(key)
        if value is not None:
            await self._check_refresh_ahead(key, fetch_func)
            return value
        
        # L2 ìºì‹œ í™•ì¸
        value = await self.l2_cache.get(key)
        if value is not None:
            # L1ì— ìŠ¹ê²©
            policy = self._get_policy(key)
            self.l1_cache.set(key, value, ttl=policy.ttl // 4)  # L1ì€ ë” ì§§ì€ TTL
            
            await self._check_refresh_ahead(key, fetch_func)
            return value
        
        # ìºì‹œ ë¯¸ìŠ¤ - ì›ë³¸ ë°ì´í„° ì¡°íšŒ
        if fetch_func:
            value = await fetch_func()
            if value is not None:
                await self.set(key, value)
            return value
        
        return None
    
    async def set(self, key: str, value: Any) -> None:
        """ë‹¤ì¸µ ìºì‹œì— ê°’ ì €ì¥"""
        policy = self._get_policy(key)
        
        if policy.strategy == CacheStrategy.WRITE_THROUGH:
            # L1, L2ì— ë™ì‹œ ì €ì¥
            self.l1_cache.set(key, value, ttl=policy.ttl // 4)
            await self.l2_cache.set(key, value, ttl=policy.ttl)
        
        elif policy.strategy == CacheStrategy.WRITE_BEHIND:
            # L1ì— ì¦‰ì‹œ ì €ì¥, L2ëŠ” ë¹„ë™ê¸°
            self.l1_cache.set(key, value, ttl=policy.ttl // 4)
            asyncio.create_task(self.l2_cache.set(key, value, ttl=policy.ttl))
        
        elif policy.strategy == CacheStrategy.CACHE_ASIDE:
            # L1ë§Œ ì €ì¥
            self.l1_cache.set(key, value, ttl=policy.ttl // 4)
    
    async def _check_refresh_ahead(self, key: str, fetch_func: Callable):
        """Refresh-ahead ì „ëµ í™•ì¸"""
        if not fetch_func:
            return
        
        policy = self._get_policy(key)
        if policy.strategy == CacheStrategy.REFRESH_AHEAD:
            # TTL ì„ê³„ê°’ í™•ì¸í•˜ì—¬ ë°±ê·¸ë¼ìš´ë“œ ìƒˆë¡œê³ ì¹¨ ìŠ¤ì¼€ì¤„
            await self.refresh_queue.put((key, fetch_func))
    
    async def _refresh_worker(self):
        """ë°±ê·¸ë¼ìš´ë“œ ìƒˆë¡œê³ ì¹¨ ì›Œì»¤"""
        while True:
            try:
                key, fetch_func = await self.refresh_queue.get()
                
                # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°ì´í„° ìƒˆë¡œê³ ì¹¨
                fresh_value = await fetch_func()
                if fresh_value is not None:
                    await self.set(key, fresh_value)
                
                self.refresh_queue.task_done()
                
            except Exception as e:
                print(f"Refresh worker error: {e}")
                await asyncio.sleep(1)
    
    def _get_policy(self, key: str) -> CachePolicy:
        """í‚¤ì— ëŒ€í•œ ìºì‹œ ì •ì±… ì¡°íšŒ"""
        for pattern, policy in self.policies.items():
            if pattern in key:  # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­
                return policy
        
        # ê¸°ë³¸ ì •ì±…
        return CachePolicy(
            strategy=CacheStrategy.WRITE_THROUGH,
            ttl=3600
        )
```

### Layer 3: CDN ë° Edge ìºì‹± (L3)

#### 3.1 CloudFront ìµœì í™”
```yaml
# infrastructure/cdn/cloudfront-config.yaml

CloudFrontDistribution:
  Type: AWS::CloudFront::Distribution
  Properties:
    DistributionConfig:
      Comment: "T-Developer API and Static Assets CDN"
      Enabled: true
      HttpVersion: http2
      IPV6Enabled: true
      
      # Origin êµ¬ì„±
      Origins:
        - Id: "api-origin"
          DomainName: "api.t-developer.com"
          CustomOriginConfig:
            HTTPPort: 443
            HTTPSPort: 443
            OriginProtocolPolicy: "https-only"
            OriginSSLProtocols: ["TLSv1.2"]
        
        - Id: "static-origin"
          DomainName: "static.t-developer.com"
          S3OriginConfig:
            OriginAccessIdentity: !Sub "${OriginAccessIdentity}"
      
      # ìºì‹œ ë™ì‘ ì„¤ì •
      DefaultCacheBehavior:
        TargetOriginId: "api-origin"
        ViewerProtocolPolicy: "redirect-to-https"
        CachePolicyId: "4135ea2d-6df8-44a3-9df3-4b5a84be39ad"  # CachingDisabled
        OriginRequestPolicyId: "88a5eaf4-2fd4-4709-b370-b4c650ea3fcf"  # CORS-S3Origin
        ResponseHeadersPolicyId: "67f7725c-6f97-4210-82d7-5512b31e9d03"  # SecurityHeaders
        
        # Edge Lambda í•¨ìˆ˜
        LambdaFunctionAssociations:
          - EventType: "origin-request"
            LambdaFunctionARN: !Sub "${EdgeOptimizer}:${EdgeOptimizerVersion}"
      
      # ì¶”ê°€ ìºì‹œ ë™ì‘
      CacheBehaviors:
        # API ì—”ë“œí¬ì¸íŠ¸ë³„ ìºì‹œ ì„¤ì •
        - PathPattern: "/api/v1/agents/*/status"
          TargetOriginId: "api-origin"
          ViewerProtocolPolicy: "https-only"
          CachePolicyId: "658327ea-f89d-4fab-a63d-7e88639e58f6"  # CachingOptimized
          TTL:
            DefaultTTL: 300      # 5ë¶„
            MaxTTL: 3600         # 1ì‹œê°„
            MinTTL: 0
        
        - PathPattern: "/api/v1/agents"
          TargetOriginId: "api-origin"  
          ViewerProtocolPolicy: "https-only"
          CachePolicyId: "4135ea2d-6df8-44a3-9df3-4b5a84be39ad"  # CachingDisabled
        
        # ì •ì  íŒŒì¼ ìµœì í™”
        - PathPattern: "*.js"
          TargetOriginId: "static-origin"
          ViewerProtocolPolicy: "https-only"
          TTL:
            DefaultTTL: 86400    # 1ì¼
            MaxTTL: 31536000     # 1ë…„
            MinTTL: 0
          Compress: true
        
        - PathPattern: "*.css"
          TargetOriginId: "static-origin"
          ViewerProtocolPolicy: "https-only"
          TTL:
            DefaultTTL: 86400    # 1ì¼
            MaxTTL: 31536000     # 1ë…„
            MinTTL: 0
          Compress: true
      
      # ì§€ì—­ë³„ Edge Location ìµœì í™”
      PriceClass: "PriceClass_All"  # ì „ ì„¸ê³„ ëª¨ë“  ì—£ì§€ ë¡œì¼€ì´ì…˜ ì‚¬ìš©
      
      # HTTP/2 Push ìµœì í™”
      ViewerCertificate:
        AcmCertificateArn: !Ref SSLCertificate
        SslSupportMethod: "sni-only"
        MinimumProtocolVersion: "TLSv1.2_2021"
```

#### 3.2 Edge Computing ìµœì í™”
```javascript
// Edge Lambda Function for Request Optimization
export const handler = async (event) => {
    const request = event.Records[0].cf.request;
    const headers = request.headers;
    
    // User-Agent ê¸°ë°˜ ìµœì í™”
    const userAgent = headers['user-agent'][0].value.toLowerCase();
    if (userAgent.includes('mobile')) {
        // ëª¨ë°”ì¼ ì‚¬ìš©ìë¥¼ ìœ„í•œ ì••ì¶•ëœ ì‘ë‹µ
        headers['accept-encoding'] = [{ key: 'Accept-Encoding', value: 'gzip, br' }];
    }
    
    // ì§€ì—­ë³„ ë¼ìš°íŒ… ìµœì í™”
    const cloudFrontRegion = headers['cloudfront-viewer-country'][0].value;
    if (cloudFrontRegion === 'KR') {
        // í•œêµ­ ì‚¬ìš©ìë¥¼ ìœ„í•œ ì„œìš¸ ë¦¬ì „ ë¼ìš°íŒ…
        request.origin.custom.domainName = 'api-ap-northeast-2.t-developer.com';
    }
    
    // API í‚¤ ê¸°ë°˜ ìºì‹± ì „ëµ
    const apiKey = headers['x-api-key'];
    if (apiKey) {
        const tier = await getTierFromApiKey(apiKey[0].value);
        if (tier === 'premium') {
            // í”„ë¦¬ë¯¸ì—„ ì‚¬ìš©ìëŠ” ë” ê¸´ ìºì‹œ TTL
            headers['cache-control'] = [{ key: 'Cache-Control', value: 'max-age=3600' }];
        }
    }
    
    return request;
};
```

## ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

### 1. ë°ì´í„°ë² ì´ìŠ¤ ìƒ¤ë”© ì „ëµ

#### 1.1 ìˆ˜í‰ì  ìƒ¤ë”© êµ¬í˜„
```python
# backend/src/database/sharding_manager.py

import hashlib
import asyncio
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum

class ShardingStrategy(Enum):
    HASH_BASED = "hash"
    RANGE_BASED = "range"
    DIRECTORY_BASED = "directory"

@dataclass
class ShardConfig:
    shard_id: str
    connection_string: str
    weight: float = 1.0
    is_active: bool = True
    read_only: bool = False

class DatabaseShardingManager:
    def __init__(self, sharding_strategy: ShardingStrategy = ShardingStrategy.HASH_BASED):
        self.strategy = sharding_strategy
        self.shards: Dict[str, ShardConfig] = {}
        self.shard_ring: List[str] = []  # Consistent hashing ring
        self.connections: Dict[str, Any] = {}  # DB connection pools
        
    def add_shard(self, config: ShardConfig) -> None:
        """ìƒ¤ë“œ ì¶”ê°€"""
        self.shards[config.shard_id] = config
        
        if self.strategy == ShardingStrategy.HASH_BASED:
            self._rebuild_hash_ring()
    
    def _rebuild_hash_ring(self) -> None:
        """Consistent Hashing Ring ì¬êµ¬ì„±"""
        self.shard_ring.clear()
        
        for shard_id, config in self.shards.items():
            if config.is_active:
                # ê°€ì¤‘ì¹˜ì— ë”°ë¼ ë” ë§ì€ ê°€ìƒ ë…¸ë“œ ìƒì„±
                virtual_nodes = int(config.weight * 100)
                
                for i in range(virtual_nodes):
                    virtual_key = f"{shard_id}:{i}"
                    hash_value = hashlib.md5(virtual_key.encode()).hexdigest()
                    self.shard_ring.append((hash_value, shard_id))
        
        # í•´ì‹œê°’ìœ¼ë¡œ ì •ë ¬
        self.shard_ring.sort(key=lambda x: x[0])
    
    def get_shard_for_key(self, key: str) -> str:
        """í‚¤ì— ëŒ€í•œ ì ì ˆí•œ ìƒ¤ë“œ ë°˜í™˜"""
        if self.strategy == ShardingStrategy.HASH_BASED:
            return self._get_shard_by_hash(key)
        elif self.strategy == ShardingStrategy.RANGE_BASED:
            return self._get_shard_by_range(key)
        elif self.strategy == ShardingStrategy.DIRECTORY_BASED:
            return self._get_shard_by_directory(key)
    
    def _get_shard_by_hash(self, key: str) -> str:
        """í•´ì‹œ ê¸°ë°˜ ìƒ¤ë“œ ì„ íƒ"""
        if not self.shard_ring:
            raise ValueError("No active shards available")
        
        key_hash = hashlib.md5(key.encode()).hexdigest()
        
        # Ringì—ì„œ key_hashë³´ë‹¤ í° ì²« ë²ˆì§¸ ë…¸ë“œ ì°¾ê¸°
        for ring_hash, shard_id in self.shard_ring:
            if ring_hash >= key_hash:
                return shard_id
        
        # ë§ˆì§€ë§‰ê¹Œì§€ ì°¾ì§€ ëª»í•˜ë©´ ì²« ë²ˆì§¸ ë…¸ë“œ ë°˜í™˜ (ringì˜ íŠ¹ì„±)
        return self.shard_ring[0][1]
    
    async def execute_query(self, key: str, query: str, params: Dict = None, 
                          read_only: bool = False) -> Any:
        """ìƒ¤ë“œë³„ ì¿¼ë¦¬ ì‹¤í–‰"""
        if read_only:
            # ì½ê¸° ì „ìš© ì¿¼ë¦¬ëŠ” ì½ê¸° ë³µì œë³¸ ì‚¬ìš©
            shard_id = self._get_read_replica(key)
        else:
            shard_id = self.get_shard_for_key(key)
        
        connection = await self._get_connection(shard_id)
        
        try:
            if params:
                result = await connection.execute(query, params)
            else:
                result = await connection.execute(query)
            
            return result
            
        except Exception as e:
            # ìƒ¤ë“œ ì¥ì•  ì‹œ ìë™ í˜ì¼ì˜¤ë²„
            if not read_only:
                backup_shard = self._get_backup_shard(shard_id)
                if backup_shard:
                    connection = await self._get_connection(backup_shard)
                    return await connection.execute(query, params or {})
            raise e
    
    async def execute_distributed_query(self, query: str, 
                                      params: Dict = None) -> List[Any]:
        """ëª¨ë“  ìƒ¤ë“œì— ë¶„ì‚° ì¿¼ë¦¬ ì‹¤í–‰"""
        tasks = []
        
        for shard_id in self.shards.keys():
            if self.shards[shard_id].is_active:
                task = self._execute_on_shard(shard_id, query, params)
                tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì„±ê³µí•œ ê²°ê³¼ë§Œ ìˆ˜ì§‘
        successful_results = [
            result for result in results 
            if not isinstance(result, Exception)
        ]
        
        return successful_results
    
    def _get_read_replica(self, key: str) -> str:
        """ì½ê¸° ë³µì œë³¸ ì„ íƒ"""
        primary_shard = self.get_shard_for_key(key)
        
        # ì½ê¸° ì „ìš© ë³µì œë³¸ì´ ìˆìœ¼ë©´ ì‚¬ìš©
        for shard_id, config in self.shards.items():
            if (config.is_active and config.read_only and 
                shard_id.startswith(f"{primary_shard}_replica")):
                return shard_id
        
        # ë³µì œë³¸ì´ ì—†ìœ¼ë©´ ì›ë³¸ ìƒ¤ë“œ ì‚¬ìš©
        return primary_shard
```

#### 1.2 ìë™ ìƒ¤ë“œ ë¦¬ë°¸ëŸ°ì‹±
```python
# backend/src/database/shard_rebalancer.py

import asyncio
import time
from typing import Dict, List, Tuple
from dataclasses import dataclass

@dataclass
class ShardMetrics:
    shard_id: str
    data_size: int  # bytes
    query_count: int
    cpu_usage: float
    memory_usage: float
    connection_count: int
    last_updated: float

class ShardRebalancer:
    def __init__(self, sharding_manager: DatabaseShardingManager):
        self.sharding_manager = sharding_manager
        self.metrics: Dict[str, ShardMetrics] = {}
        self.rebalancing_threshold = 0.8  # 80% ì„ê³„ê°’
        self.monitoring_interval = 300  # 5ë¶„ë§ˆë‹¤ ëª¨ë‹ˆí„°ë§
    
    async def start_monitoring(self):
        """ìƒ¤ë“œ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        while True:
            await self._collect_metrics()
            await self._analyze_and_rebalance()
            await asyncio.sleep(self.monitoring_interval)
    
    async def _collect_metrics(self):
        """ê° ìƒ¤ë“œì˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        for shard_id in self.sharding_manager.shards.keys():
            try:
                metrics = await self._get_shard_metrics(shard_id)
                self.metrics[shard_id] = metrics
            except Exception as e:
                print(f"Failed to collect metrics for shard {shard_id}: {e}")
    
    async def _get_shard_metrics(self, shard_id: str) -> ShardMetrics:
        """íŠ¹ì • ìƒ¤ë“œì˜ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        connection = await self.sharding_manager._get_connection(shard_id)
        
        # ë°ì´í„° í¬ê¸° ì¡°íšŒ
        size_query = "SELECT pg_database_size(current_database())"
        data_size = await connection.fetchval(size_query)
        
        # ì—°ê²° ìˆ˜ ì¡°íšŒ
        conn_query = "SELECT count(*) FROM pg_stat_activity"
        connection_count = await connection.fetchval(conn_query)
        
        # CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ CloudWatchë‚˜ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ì—ì„œ ê°€ì ¸ì˜´
        cpu_usage = await self._get_cpu_usage(shard_id)
        memory_usage = await self._get_memory_usage(shard_id)
        
        return ShardMetrics(
            shard_id=shard_id,
            data_size=data_size,
            query_count=0,  # ì¶”í›„ êµ¬í˜„
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            connection_count=connection_count,
            last_updated=time.time()
        )
    
    async def _analyze_and_rebalance(self):
        """ë©”íŠ¸ë¦­ ë¶„ì„ ë° ë¦¬ë°¸ëŸ°ì‹± ì‹¤í–‰"""
        if len(self.metrics) < 2:
            return
        
        # ë¶€í•˜ê°€ ë†’ì€ ìƒ¤ë“œ ì‹ë³„
        overloaded_shards = self._identify_overloaded_shards()
        
        if overloaded_shards:
            print(f"Overloaded shards detected: {overloaded_shards}")
            
            for shard_id in overloaded_shards:
                await self._rebalance_shard(shard_id)
    
    def _identify_overloaded_shards(self) -> List[str]:
        """ê³¼ë¶€í•˜ ìƒ¤ë“œ ì‹ë³„"""
        overloaded = []
        
        for shard_id, metrics in self.metrics.items():
            if (metrics.cpu_usage > self.rebalancing_threshold or
                metrics.memory_usage > self.rebalancing_threshold):
                overloaded.append(shard_id)
        
        return overloaded
    
    async def _rebalance_shard(self, overloaded_shard_id: str):
        """íŠ¹ì • ìƒ¤ë“œ ë¦¬ë°¸ëŸ°ì‹±"""
        # 1. ìƒˆë¡œìš´ ìƒ¤ë“œ ìƒì„±
        new_shard_id = f"{overloaded_shard_id}_split_{int(time.time())}"
        await self._create_new_shard(new_shard_id)
        
        # 2. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš ìˆ˜ë¦½
        migration_plan = await self._create_migration_plan(
            overloaded_shard_id, new_shard_id
        )
        
        # 3. ì ì§„ì  ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
        await self._execute_migration(migration_plan)
        
        # 4. ë¼ìš°íŒ… í…Œì´ë¸” ì—…ë°ì´íŠ¸
        await self._update_routing_table(overloaded_shard_id, new_shard_id)
        
        print(f"Rebalancing completed: {overloaded_shard_id} -> {new_shard_id}")
```

### 2. ì¿¼ë¦¬ ìµœì í™”

#### 2.1 ì§€ëŠ¥í˜• ì¿¼ë¦¬ ìºì‹œ
```python
# backend/src/database/query_cache.py

import hashlib
import time
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass

@dataclass
class QueryCacheEntry:
    result: Any
    cached_at: float
    ttl: int
    hit_count: int = 0
    execution_time: float = 0.0
    query_complexity: int = 1

class IntelligentQueryCache:
    def __init__(self, max_size: int = 10000):
        self.max_size = max_size
        self.cache: Dict[str, QueryCacheEntry] = {}
        self.query_stats: Dict[str, Dict[str, Any]] = {}
        
    def get_cache_key(self, query: str, params: Dict = None) -> str:
        """ì¿¼ë¦¬ ìºì‹œ í‚¤ ìƒì„±"""
        query_normalized = self._normalize_query(query)
        params_str = str(sorted(params.items())) if params else ""
        
        combined = f"{query_normalized}:{params_str}"
        return hashlib.sha256(combined.encode()).hexdigest()[:32]
    
    def _normalize_query(self, query: str) -> str:
        """ì¿¼ë¦¬ ì •ê·œí™”"""
        # ê³µë°± ì •ë¦¬
        normalized = " ".join(query.split())
        
        # ëŒ€ì†Œë¬¸ì í†µì¼
        normalized = normalized.lower()
        
        # SQL í‚¤ì›Œë“œ ì •ê·œí™”
        normalized = normalized.replace("  ", " ")
        
        return normalized
    
    async def get_or_execute(self, query: str, params: Dict = None, 
                           executor_func=None, ttl: int = 300) -> Any:
        """ìºì‹œ ì¡°íšŒ ë˜ëŠ” ì¿¼ë¦¬ ì‹¤í–‰"""
        cache_key = self.get_cache_key(query, params)
        
        # ìºì‹œ í™•ì¸
        cached_entry = self._get_from_cache(cache_key)
        if cached_entry:
            return cached_entry.result
        
        # ìºì‹œ ë¯¸ìŠ¤ - ì¿¼ë¦¬ ì‹¤í–‰
        start_time = time.time()
        result = await executor_func(query, params)
        execution_time = time.time() - start_time
        
        # ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ì„
        complexity = self._analyze_query_complexity(query)
        
        # ìºì‹œì— ì €ì¥ (ë³µì¡í•œ ì¿¼ë¦¬ì¼ìˆ˜ë¡ ë” ì˜¤ë˜ ìºì‹œ)
        dynamic_ttl = self._calculate_dynamic_ttl(execution_time, complexity, ttl)
        self._store_in_cache(cache_key, result, dynamic_ttl, execution_time, complexity)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_query_stats(cache_key, execution_time, complexity)
        
        return result
    
    def _get_from_cache(self, cache_key: str) -> Optional[QueryCacheEntry]:
        """ìºì‹œì—ì„œ ì¡°íšŒ"""
        if cache_key not in self.cache:
            return None
        
        entry = self.cache[cache_key]
        
        # TTL í™•ì¸
        if time.time() - entry.cached_at > entry.ttl:
            del self.cache[cache_key]
            return None
        
        # íˆíŠ¸ ì¹´ìš´íŠ¸ ì¦ê°€
        entry.hit_count += 1
        
        return entry
    
    def _store_in_cache(self, cache_key: str, result: Any, ttl: int, 
                       execution_time: float, complexity: int):
        """ìºì‹œì— ì €ì¥"""
        # ìš©ëŸ‰ ì´ˆê³¼ ì‹œ LRU + ë³µì¡ë„ ê¸°ë°˜ ì œê±°
        if len(self.cache) >= self.max_size:
            self._evict_entries()
        
        self.cache[cache_key] = QueryCacheEntry(
            result=result,
            cached_at=time.time(),
            ttl=ttl,
            execution_time=execution_time,
            query_complexity=complexity
        )
    
    def _analyze_query_complexity(self, query: str) -> int:
        """ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ì„"""
        complexity_score = 1
        
        query_lower = query.lower()
        
        # JOIN ê°œìˆ˜
        join_count = query_lower.count("join")
        complexity_score += join_count * 2
        
        # ì„œë¸Œì¿¼ë¦¬ ê°œìˆ˜
        subquery_count = query_lower.count("(select")
        complexity_score += subquery_count * 3
        
        # ì§‘ê³„ í•¨ìˆ˜
        aggregation_keywords = ["group by", "order by", "having", "window"]
        for keyword in aggregation_keywords:
            if keyword in query_lower:
                complexity_score += 2
        
        # CTE (Common Table Expression)
        if "with" in query_lower:
            complexity_score += 3
        
        return min(complexity_score, 10)  # ìµœëŒ€ 10ì 
    
    def _calculate_dynamic_ttl(self, execution_time: float, 
                             complexity: int, base_ttl: int) -> int:
        """ë™ì  TTL ê³„ì‚°"""
        # ì‹¤í–‰ ì‹œê°„ì´ ê¸¸ìˆ˜ë¡ ë” ì˜¤ë˜ ìºì‹œ
        time_factor = min(execution_time / 1.0, 5.0)  # ìµœëŒ€ 5ë°°
        
        # ë³µì¡ë„ê°€ ë†’ì„ìˆ˜ë¡ ë” ì˜¤ë˜ ìºì‹œ
        complexity_factor = complexity / 5.0
        
        dynamic_ttl = int(base_ttl * (1 + time_factor + complexity_factor))
        
        return min(dynamic_ttl, 3600)  # ìµœëŒ€ 1ì‹œê°„
    
    def _evict_entries(self):
        """ìºì‹œ í•­ëª© ì œê±°"""
        # LRU + ë³µì¡ë„ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
        scored_entries = []
        
        for cache_key, entry in self.cache.items():
            # ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ìœ ì§€ (hit_countì™€ complexityê°€ ë†’ê³ , ì˜¤ë˜ë˜ì§€ ì•Šì€ ê²ƒ)
            age = time.time() - entry.cached_at
            score = (entry.hit_count * entry.query_complexity) / (age + 1)
            scored_entries.append((score, cache_key))
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ í•˜ìœ„ 25% ì œê±°
        scored_entries.sort()
        to_remove = len(scored_entries) // 4
        
        for _, cache_key in scored_entries[:to_remove]:
            del self.cache[cache_key]
```

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„

### 1. Real-time Performance Monitoring
```python
# backend/src/monitoring/performance_monitor.py

import asyncio
import time
import psutil
from typing import Dict, List, Any, Callable
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta

@dataclass
class PerformanceMetrics:
    timestamp: float
    cpu_usage: float
    memory_usage: float
    disk_io: Dict[str, float]
    network_io: Dict[str, float]
    active_connections: int
    response_times: Dict[str, List[float]]  # endpoint -> [response_times]
    error_rates: Dict[str, float]
    cache_hit_rates: Dict[str, float]

class PerformanceMonitor:
    def __init__(self, collection_interval: int = 5):
        self.collection_interval = collection_interval
        self.metrics_history: List[PerformanceMetrics] = []
        self.alert_callbacks: List[Callable] = []
        self.thresholds = {
            "cpu_usage": 80.0,
            "memory_usage": 85.0,
            "response_time_p95": 1000.0,  # ms
            "error_rate": 0.05,  # 5%
            "cache_hit_rate_min": 0.8  # 80%
        }
        
    async def start_monitoring(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        while True:
            try:
                metrics = await self._collect_metrics()
                self.metrics_history.append(metrics)
                
                # ìµœê·¼ 1ì‹œê°„ ë°ì´í„°ë§Œ ìœ ì§€
                cutoff = time.time() - 3600
                self.metrics_history = [
                    m for m in self.metrics_history 
                    if m.timestamp > cutoff
                ]
                
                # ì„ê³„ê°’ í™•ì¸ ë° ì•Œë¦¼
                await self._check_thresholds(metrics)
                
            except Exception as e:
                print(f"Monitoring error: {e}")
                
            await asyncio.sleep(self.collection_interval)
    
    async def _collect_metrics(self) -> PerformanceMetrics:
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        # CPU ì‚¬ìš©ë¥ 
        cpu_usage = psutil.cpu_percent(interval=1)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        memory = psutil.virtual_memory()
        memory_usage = memory.percent
        
        # ë””ìŠ¤í¬ I/O
        disk_io = psutil.disk_io_counters()
        disk_metrics = {
            "read_bytes_per_sec": disk_io.read_bytes,
            "write_bytes_per_sec": disk_io.write_bytes,
            "read_ops_per_sec": disk_io.read_count,
            "write_ops_per_sec": disk_io.write_count
        }
        
        # ë„¤íŠ¸ì›Œí¬ I/O
        network_io = psutil.net_io_counters()
        network_metrics = {
            "bytes_sent_per_sec": network_io.bytes_sent,
            "bytes_recv_per_sec": network_io.bytes_recv,
            "packets_sent_per_sec": network_io.packets_sent,
            "packets_recv_per_sec": network_io.packets_recv
        }
        
        # í™œì„± ì—°ê²° ìˆ˜
        active_connections = len(psutil.net_connections())
        
        # ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ (ì™¸ë¶€ì—ì„œ ì£¼ì…)
        response_times = await self._get_response_times()
        error_rates = await self._get_error_rates()
        cache_hit_rates = await self._get_cache_hit_rates()
        
        return PerformanceMetrics(
            timestamp=time.time(),
            cpu_usage=cpu_usage,
            memory_usage=memory_usage,
            disk_io=disk_metrics,
            network_io=network_metrics,
            active_connections=active_connections,
            response_times=response_times,
            error_rates=error_rates,
            cache_hit_rates=cache_hit_rates
        )
    
    async def _check_thresholds(self, metrics: PerformanceMetrics):
        """ì„ê³„ê°’ í™•ì¸ ë° ì•Œë¦¼"""
        alerts = []
        
        # CPU ì‚¬ìš©ë¥  í™•ì¸
        if metrics.cpu_usage > self.thresholds["cpu_usage"]:
            alerts.append({
                "type": "HIGH_CPU_USAGE",
                "value": metrics.cpu_usage,
                "threshold": self.thresholds["cpu_usage"],
                "severity": "WARNING" if metrics.cpu_usage < 90 else "CRITICAL"
            })
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  í™•ì¸
        if metrics.memory_usage > self.thresholds["memory_usage"]:
            alerts.append({
                "type": "HIGH_MEMORY_USAGE", 
                "value": metrics.memory_usage,
                "threshold": self.thresholds["memory_usage"],
                "severity": "WARNING" if metrics.memory_usage < 95 else "CRITICAL"
            })
        
        # ì‘ë‹µ ì‹œê°„ í™•ì¸
        for endpoint, times in metrics.response_times.items():
            if times:
                p95_time = self._calculate_percentile(times, 95)
                if p95_time > self.thresholds["response_time_p95"]:
                    alerts.append({
                        "type": "HIGH_RESPONSE_TIME",
                        "endpoint": endpoint,
                        "value": p95_time,
                        "threshold": self.thresholds["response_time_p95"],
                        "severity": "WARNING"
                    })
        
        # ì—ëŸ¬ìœ¨ í™•ì¸
        for endpoint, error_rate in metrics.error_rates.items():
            if error_rate > self.thresholds["error_rate"]:
                alerts.append({
                    "type": "HIGH_ERROR_RATE",
                    "endpoint": endpoint,
                    "value": error_rate,
                    "threshold": self.thresholds["error_rate"],
                    "severity": "CRITICAL" if error_rate > 0.1 else "WARNING"
                })
        
        # ìºì‹œ íˆíŠ¸ìœ¨ í™•ì¸
        for cache_name, hit_rate in metrics.cache_hit_rates.items():
            if hit_rate < self.thresholds["cache_hit_rate_min"]:
                alerts.append({
                    "type": "LOW_CACHE_HIT_RATE",
                    "cache": cache_name,
                    "value": hit_rate,
                    "threshold": self.thresholds["cache_hit_rate_min"],
                    "severity": "WARNING"
                })
        
        # ì•Œë¦¼ ì „ì†¡
        for alert in alerts:
            await self._send_alert(alert)
    
    def get_performance_summary(self, duration_minutes: int = 60) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì¡°íšŒ"""
        cutoff = time.time() - (duration_minutes * 60)
        recent_metrics = [
            m for m in self.metrics_history 
            if m.timestamp > cutoff
        ]
        
        if not recent_metrics:
            return {"error": "No metrics available"}
        
        # í‰ê· ê°’ ê³„ì‚°
        avg_cpu = sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics)
        avg_memory = sum(m.memory_usage for m in recent_metrics) / len(recent_metrics)
        
        # ì‘ë‹µ ì‹œê°„ í†µê³„
        all_response_times = []
        for metrics in recent_metrics:
            for endpoint_times in metrics.response_times.values():
                all_response_times.extend(endpoint_times)
        
        response_time_stats = {
            "p50": self._calculate_percentile(all_response_times, 50),
            "p95": self._calculate_percentile(all_response_times, 95),
            "p99": self._calculate_percentile(all_response_times, 99)
        } if all_response_times else {}
        
        return {
            "period": f"{duration_minutes} minutes",
            "avg_cpu_usage": round(avg_cpu, 2),
            "avg_memory_usage": round(avg_memory, 2),
            "response_time_stats": response_time_stats,
            "total_data_points": len(recent_metrics)
        }
```

### 2. Automated Performance Optimization
```python
# backend/src/optimization/auto_optimizer.py

import asyncio
from typing import Dict, Any, List
from dataclasses import dataclass

@dataclass
class OptimizationAction:
    action_type: str
    target: str
    parameters: Dict[str, Any]
    expected_improvement: float
    risk_level: str  # LOW, MEDIUM, HIGH

class AutoPerformanceOptimizer:
    def __init__(self, performance_monitor: PerformanceMonitor):
        self.monitor = performance_monitor
        self.optimization_rules = self._load_optimization_rules()
        self.applied_optimizations: List[OptimizationAction] = []
        
    def _load_optimization_rules(self) -> List[Dict[str, Any]]:
        """ìµœì í™” ê·œì¹™ ë¡œë“œ"""
        return [
            {
                "condition": "high_cpu_usage",
                "threshold": 80.0,
                "actions": [
                    {
                        "type": "scale_up_instances",
                        "parameters": {"scale_factor": 1.5},
                        "expected_improvement": 0.3,
                        "risk_level": "LOW"
                    },
                    {
                        "type": "enable_cpu_throttling",
                        "parameters": {"max_cpu_per_request": 50},
                        "expected_improvement": 0.2,
                        "risk_level": "MEDIUM"
                    }
                ]
            },
            {
                "condition": "high_memory_usage", 
                "threshold": 85.0,
                "actions": [
                    {
                        "type": "increase_cache_eviction",
                        "parameters": {"eviction_rate": 1.2},
                        "expected_improvement": 0.15,
                        "risk_level": "LOW"
                    },
                    {
                        "type": "scale_up_memory",
                        "parameters": {"memory_multiplier": 1.3},
                        "expected_improvement": 0.4,
                        "risk_level": "MEDIUM"
                    }
                ]
            },
            {
                "condition": "low_cache_hit_rate",
                "threshold": 0.7,
                "actions": [
                    {
                        "type": "increase_cache_size",
                        "parameters": {"size_multiplier": 1.5},
                        "expected_improvement": 0.25,
                        "risk_level": "LOW"
                    },
                    {
                        "type": "optimize_cache_policy",
                        "parameters": {"ttl_multiplier": 1.2},
                        "expected_improvement": 0.15,
                        "risk_level": "LOW"
                    }
                ]
            }
        ]
    
    async def run_optimization_cycle(self):
        """ìµœì í™” ì‚¬ì´í´ ì‹¤í–‰"""
        current_metrics = self.monitor.metrics_history[-1] if self.monitor.metrics_history else None
        
        if not current_metrics:
            return
        
        # ìµœì í™” ê¸°íšŒ ì‹ë³„
        optimization_actions = self._identify_optimization_opportunities(current_metrics)
        
        # ì•ˆì „í•œ ìµœì í™”ë§Œ ìë™ ì ìš©
        safe_actions = [
            action for action in optimization_actions 
            if action.risk_level == "LOW"
        ]
        
        for action in safe_actions:
            success = await self._apply_optimization(action)
            if success:
                self.applied_optimizations.append(action)
                print(f"Applied optimization: {action.action_type}")
        
        # ìœ„í—˜ë„ê°€ ë†’ì€ ìµœì í™”ëŠ” ìŠ¹ì¸ ìš”ì²­
        risky_actions = [
            action for action in optimization_actions 
            if action.risk_level in ["MEDIUM", "HIGH"]
        ]
        
        if risky_actions:
            await self._request_approval_for_risky_optimizations(risky_actions)
    
    def _identify_optimization_opportunities(self, 
                                          metrics: PerformanceMetrics) -> List[OptimizationAction]:
        """ìµœì í™” ê¸°íšŒ ì‹ë³„"""
        opportunities = []
        
        for rule in self.optimization_rules:
            condition = rule["condition"]
            threshold = rule["threshold"]
            
            should_optimize = False
            
            if condition == "high_cpu_usage" and metrics.cpu_usage > threshold:
                should_optimize = True
            elif condition == "high_memory_usage" and metrics.memory_usage > threshold:
                should_optimize = True
            elif condition == "low_cache_hit_rate":
                avg_hit_rate = sum(metrics.cache_hit_rates.values()) / len(metrics.cache_hit_rates)
                if avg_hit_rate < threshold:
                    should_optimize = True
            
            if should_optimize:
                for action_config in rule["actions"]:
                    action = OptimizationAction(
                        action_type=action_config["type"],
                        target="system",
                        parameters=action_config["parameters"],
                        expected_improvement=action_config["expected_improvement"],
                        risk_level=action_config["risk_level"]
                    )
                    opportunities.append(action)
        
        return opportunities
    
    async def _apply_optimization(self, action: OptimizationAction) -> bool:
        """ìµœì í™” ì ìš©"""
        try:
            if action.action_type == "scale_up_instances":
                return await self._scale_up_instances(action.parameters)
            elif action.action_type == "increase_cache_size":
                return await self._increase_cache_size(action.parameters)
            elif action.action_type == "optimize_cache_policy":
                return await self._optimize_cache_policy(action.parameters)
            elif action.action_type == "increase_cache_eviction":
                return await self._increase_cache_eviction(action.parameters)
            # ... ê¸°íƒ€ ìµœì í™” ì•¡ì…˜ë“¤
            
            return False
            
        except Exception as e:
            print(f"Optimization failed: {action.action_type}, Error: {e}")
            return False
```

## ğŸ¯ ì„±ëŠ¥ ìµœì í™” ë¡œë“œë§µ

### Phase 1: ê¸°ë³¸ ìµœì í™” (1-2ì£¼)
- L1/L2 ìºì‹± ì‹œìŠ¤í…œ êµ¬í˜„
- ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ìµœì í™”
- API ì‘ë‹µ ì••ì¶• ì„¤ì •

### Phase 2: ê³ ê¸‰ ìµœì í™” (2-3ì£¼)
- ë°ì´í„°ë² ì´ìŠ¤ ìƒ¤ë”© êµ¬í˜„
- CDN ë° Edge ìºì‹± ì„¤ì •
- ì§€ëŠ¥í˜• ì¿¼ë¦¬ ìºì‹±

### Phase 3: ìë™ ìµœì í™” (1-2ì£¼)
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ìë™ ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ
- ì˜ˆì¸¡ì  ìŠ¤ì¼€ì¼ë§

ì´ ì„±ëŠ¥ ìµœì í™” ì „ëµì„ í†µí•´ T-Developer í”Œë«í¼ì´ ëŒ€ê·œëª¨ íŠ¸ë˜í”½ê³¼ ë³µì¡í•œ AI ì›Œí¬ë¡œë“œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.