# AI Models Configuration
# Day 21: Phase 2 - Meta Agents
# Multi-model consensus configuration

models:
  claude:
    provider: anthropic
    model: claude-3-sonnet
    version: "20241022"
    endpoint: bedrock
    region: us-east-1
    capabilities:
      - reasoning
      - code_generation
      - analysis
      - pattern_recognition
    limits:
      max_tokens: 4096
      temperature: 0.7
      top_p: 0.95
    weight: 1.2  # Higher weight for consensus

  gpt4:
    provider: openai
    model: gpt-4-turbo
    version: "2024-04-09"
    endpoint: azure
    capabilities:
      - reasoning
      - code_generation
      - creativity
      - problem_solving
    limits:
      max_tokens: 4096
      temperature: 0.7
      top_p: 0.95
    weight: 1.0

  gemini:
    provider: google
    model: gemini-1.5-pro
    version: "001"
    endpoint: vertex-ai
    region: us-central1
    capabilities:
      - multimodal
      - reasoning
      - long_context
      - code_generation
    limits:
      max_tokens: 8192
      temperature: 0.7
      top_p: 0.95
    weight: 0.9

  llama:
    provider: meta
    model: llama-3-70b
    version: "instruct"
    endpoint: bedrock
    region: us-east-1
    capabilities:
      - reasoning
      - instruction_following
      - multilingual
    limits:
      max_tokens: 2048
      temperature: 0.7
      top_p: 0.95
    weight: 0.8

  mixtral:
    provider: mistral
    model: mixtral-8x7b
    version: "instruct"
    endpoint: bedrock
    region: us-east-1
    capabilities:
      - fast_inference
      - code_generation
      - reasoning
    limits:
      max_tokens: 4096
      temperature: 0.7
      top_p: 0.95
    weight: 0.7

consensus:
  min_models: 3  # Minimum models for consensus
  agreement_threshold: 0.7  # Minimum agreement score
  confidence_threshold: 0.75  # Minimum confidence
  timeout_seconds: 30  # Max wait time for all models
  retry_attempts: 2  # Retries on failure

  voting_strategy:
    method: weighted  # weighted | majority | unanimous
    tie_breaker: highest_confidence  # highest_confidence | claude_preference

  fallback:
    enabled: true
    primary_model: claude  # Fallback to Claude if consensus fails

requirement_analysis:
  models_to_use:
    - claude
    - gpt4
    - gemini
  prompts:
    explicit_extraction: |
      Extract all explicit requirements from the following text.
      Categorize as functional, non-functional, technical, or business.
      Format as structured JSON.

    implicit_inference: |
      Based on the explicit requirements, infer implicit requirements
      that are commonly needed but not stated. Consider security,
      scalability, performance, and user experience.

    pattern_matching: |
      Identify architectural patterns that best fit these requirements.
      Consider microservices, monolithic, event-driven, serverless, and layered.

agent_generation:
  models_to_use:
    - claude  # Best for code generation
    - gpt4
  prompts:
    architecture_design: |
      Design the architecture for an agent with these requirements.
      Follow the 6.5KB size constraint and 3Î¼s performance target.
      Use the Agno framework patterns.

    code_generation: |
      Generate production-ready Python code for the agent.
      Ensure size < 6.5KB, async support, error handling.
      Follow T-Developer coding standards.

    test_generation: |
      Generate comprehensive unit tests with >85% coverage.
      Include edge cases and error scenarios.

workflow_composition:
  models_to_use:
    - gemini  # Good at understanding complex flows
    - claude
    - gpt4
  prompts:
    workflow_analysis: |
      Analyze the agent interactions and design optimal workflow.
      Identify parallelization opportunities and dependencies.

    optimization: |
      Optimize the workflow for performance and resource usage.
      Suggest caching, batching, and async processing points.

performance:
  cache_enabled: true
  cache_ttl_seconds: 300
  batch_processing: true
  max_batch_size: 10
  parallel_queries: true
  max_parallel: 5

monitoring:
  track_consensus: true
  track_latency: true
  track_costs: true
  track_accuracy: true

  metrics_export:
    format: prometheus
    endpoint: /metrics
    interval_seconds: 60

cost_optimization:
  enable_smart_routing: true  # Route to cheaper models when possible
  enable_caching: true  # Cache repeated queries
  enable_batching: true  # Batch similar requests

  limits:
    daily_token_limit: 1000000
    hourly_token_limit: 100000
    per_request_limit: 10000

  model_selection:
    strategy: cost_quality_balanced  # cheapest | best | balanced
    quality_threshold: 0.8  # Min quality score

security:
  enable_input_validation: true
  enable_output_filtering: true
  enable_rate_limiting: true

  sensitive_data:
    detect: true
    redact: true
    patterns:
      - api_key
      - password
      - secret
      - token
      - credential
