# pip â†’ uv ì „í™˜ í”„ë¡œì íŠ¸ ìƒì„¸ ì‘ì—… ì§€ì‹œì„œ

## ğŸ“‹ Phase 1: ì¤€ë¹„ ë° ê²€ì¦ (Day 1-3)

### Task 1.1: í˜„ì¬ í™˜ê²½ ë¶„ì„ ë° ë°±ì—…

#### SubTask 1.1.1: ì˜ì¡´ì„± ëª©ë¡ ìˆ˜ì§‘ ë° ë¶„ì„
**ë‹´ë‹¹ì**: DevOps ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```bash
#!/bin/bash
# scripts/analyze_dependencies.sh

echo "ğŸ“Š ì˜ì¡´ì„± ë¶„ì„ ì‹œì‘..."

# 1. ì „ì²´ í”„ë¡œì íŠ¸ ì˜ì¡´ì„± ìˆ˜ì§‘
find . -name "requirements*.txt" -type f > requirements_files.txt

# 2. ê° íŒŒì¼ë³„ íŒ¨í‚¤ì§€ ë¶„ì„
while read -r req_file; do
    echo "ë¶„ì„ ì¤‘: $req_file"
    
    # íŒ¨í‚¤ì§€ ê°œìˆ˜
    package_count=$(grep -v "^#" "$req_file" | grep -v "^$" | wc -l)
    echo "  - íŒ¨í‚¤ì§€ ìˆ˜: $package_count"
    
    # ë²„ì „ ê³ ì • ì—¬ë¶€ í™•ì¸
    pinned=$(grep -E "==" "$req_file" | wc -l)
    echo "  - ë²„ì „ ê³ ì •: $pinned/$package_count"
    
    # íŠ¹ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸ (git, local ë“±)
    git_deps=$(grep -E "git\+|@" "$req_file" | wc -l)
    local_deps=$(grep -E "file://|-e \." "$req_file" | wc -l)
    echo "  - Git ì˜ì¡´ì„±: $git_deps"
    echo "  - ë¡œì»¬ ì˜ì¡´ì„±: $local_deps"
done < requirements_files.txt

# 3. ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
python3 << 'EOF'
import subprocess
import json
from collections import defaultdict

def analyze_package_sources():
    result = subprocess.run(['pip', 'list', '--format=json'], 
                          capture_output=True, text=True)
    packages = json.loads(result.stdout)
    
    sources = defaultdict(list)
    for pkg in packages:
        # PyPI ì™¸ ì†ŒìŠ¤ í™•ì¸
        info = subprocess.run(['pip', 'show', pkg['name']], 
                            capture_output=True, text=True)
        if 'Location' in info.stdout:
            if 'site-packages' not in info.stdout:
                sources['non-pypi'].append(pkg['name'])
        
    return sources

sources = analyze_package_sources()
print(f"\nğŸ“¦ Non-PyPI íŒ¨í‚¤ì§€: {len(sources['non-pypi'])}")
for pkg in sources['non-pypi']:
    print(f"  - {pkg}")
EOF
```

#### SubTask 1.1.2: í™˜ê²½ êµ¬ì„± ë°±ì—…
**ë‹´ë‹¹ì**: ì‹œìŠ¤í…œ ê´€ë¦¬ì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 1ì‹œê°„

```bash
#!/bin/bash
# scripts/backup_environment.sh

BACKUP_DIR="backup/$(date +%Y%m%d_%H%M%S)"
mkdir -p "$BACKUP_DIR"

echo "ğŸ’¾ í™˜ê²½ ë°±ì—… ì‹œì‘: $BACKUP_DIR"

# 1. Python í™˜ê²½ ì •ë³´
python3 -m pip freeze > "$BACKUP_DIR/pip_freeze.txt"
python3 -m pip list --format=json > "$BACKUP_DIR/pip_list.json"
python3 -V > "$BACKUP_DIR/python_version.txt"
pip -V >> "$BACKUP_DIR/python_version.txt"

# 2. ì‹œìŠ¤í…œ ì •ë³´
uname -a > "$BACKUP_DIR/system_info.txt"
cat /etc/os-release >> "$BACKUP_DIR/system_info.txt" 2>/dev/null || true

# 3. í™˜ê²½ ë³€ìˆ˜
env | grep -E "PYTHON|PIP|PATH" > "$BACKUP_DIR/env_vars.txt"

# 4. pip ì„¤ì •
pip config list > "$BACKUP_DIR/pip_config.txt"
cp ~/.pip/pip.conf "$BACKUP_DIR/" 2>/dev/null || true

# 5. ê°€ìƒí™˜ê²½ ë©”íƒ€ë°ì´í„°
if [ -d ".venv" ]; then
    cp .venv/pyvenv.cfg "$BACKUP_DIR/" 2>/dev/null || true
fi

# 6. ë°±ì—… ê²€ì¦
tar -czf "$BACKUP_DIR.tar.gz" "$BACKUP_DIR"
echo "âœ… ë°±ì—… ì™„ë£Œ: $BACKUP_DIR.tar.gz"
```

#### SubTask 1.1.3: ìœ„í—˜ ìš”ì†Œ ì‹ë³„
**ë‹´ë‹¹ì**: ì‹œë‹ˆì–´ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 1ì‹œê°„

```python
# scripts/identify_risks.py
import json
import subprocess
import re
from typing import List, Dict

class RiskAnalyzer:
    def __init__(self):
        self.risks = []
        
    def analyze_package_compatibility(self):
        """íŒ¨í‚¤ì§€ í˜¸í™˜ì„± ìœ„í—˜ ë¶„ì„"""
        with open('backup/pip_list.json', 'r') as f:
            packages = json.load(f)
        
        for pkg in packages:
            # ì˜¤ë˜ëœ íŒ¨í‚¤ì§€ í™•ì¸
            if self._is_legacy_package(pkg['name']):
                self.risks.append({
                    'type': 'compatibility',
                    'severity': 'high',
                    'package': pkg['name'],
                    'issue': 'Legacy package may not be compatible with uv',
                    'mitigation': f'Test {pkg["name"]} installation separately'
                })
            
            # íŠ¹ìˆ˜ ì„¤ì¹˜ ì˜µì…˜ ì‚¬ìš© íŒ¨í‚¤ì§€
            if self._has_special_install_options(pkg['name']):
                self.risks.append({
                    'type': 'installation',
                    'severity': 'medium',
                    'package': pkg['name'],
                    'issue': 'Package requires special installation options',
                    'mitigation': 'Document installation process'
                })
    
    def analyze_ci_dependencies(self):
        """CI/CD íŒŒì´í”„ë¼ì¸ ì˜ì¡´ì„± ë¶„ì„"""
        # GitHub Actions ì›Œí¬í”Œë¡œìš° ë¶„ì„
        workflows = self._find_github_workflows()
        
        for workflow in workflows:
            if 'pip install' in open(workflow).read():
                self.risks.append({
                    'type': 'ci/cd',
                    'severity': 'medium',
                    'file': workflow,
                    'issue': 'Workflow uses pip directly',
                    'mitigation': 'Update workflow to use uv'
                })
    
    def _is_legacy_package(self, name: str) -> bool:
        # ì•Œë ¤ì§„ ë ˆê±°ì‹œ íŒ¨í‚¤ì§€ ëª©ë¡
        legacy_packages = ['nose', 'distribute', 'PIL']
        return name in legacy_packages
    
    def _has_special_install_options(self, name: str) -> bool:
        # íŠ¹ìˆ˜ ì„¤ì¹˜ê°€ í•„ìš”í•œ íŒ¨í‚¤ì§€
        special_packages = ['mysqlclient', 'psycopg2', 'Pillow']
        return name in special_packages
    
    def _find_github_workflows(self) -> List[str]:
        import glob
        return glob.glob('.github/workflows/*.yml')
    
    def generate_report(self):
        """ìœ„í—˜ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            'total_risks': len(self.risks),
            'by_severity': {
                'high': len([r for r in self.risks if r['severity'] == 'high']),
                'medium': len([r for r in self.risks if r['severity'] == 'medium']),
                'low': len([r for r in self.risks if r['severity'] == 'low'])
            },
            'risks': self.risks
        }
        
        with open('risk_analysis_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"âš ï¸  ì´ {report['total_risks']}ê°œì˜ ìœ„í—˜ ìš”ì†Œ ë°œê²¬")
        print(f"   - ë†’ìŒ: {report['by_severity']['high']}")
        print(f"   - ì¤‘ê°„: {report['by_severity']['medium']}")
        print(f"   - ë‚®ìŒ: {report['by_severity']['low']}")

if __name__ == "__main__":
    analyzer = RiskAnalyzer()
    analyzer.analyze_package_compatibility()
    analyzer.analyze_ci_dependencies()
    analyzer.generate_report()
```

### Task 1.2: uv í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶•

#### SubTask 1.2.1: ê²©ë¦¬ëœ í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„±
**ë‹´ë‹¹ì**: DevOps ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```bash
#!/bin/bash
# scripts/create_test_environment.sh

echo "ğŸ§ª uv í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶• ì¤‘..."

# 1. í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
TEST_DIR="uv-compatibility-test"
rm -rf "$TEST_DIR"
mkdir -p "$TEST_DIR"/{results,logs,temp}

cd "$TEST_DIR"

# 2. Docker ê¸°ë°˜ ê²©ë¦¬ í™˜ê²½ ìƒì„±
cat > Dockerfile.test << 'EOF'
FROM python:3.11-slim

# í…ŒìŠ¤íŠ¸ì— í•„ìš”í•œ ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    curl \
    libpq-dev \
    libmysqlclient-dev \
    && rm -rf /var/lib/apt/lists/*

# uv ì„¤ì¹˜
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.cargo/bin:$PATH"

WORKDIR /test
EOF

# 3. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
cat > test_uv_installation.py << 'EOF'
import subprocess
import time
import json
import sys
from pathlib import Path

class UvCompatibilityTester:
    def __init__(self, requirements_file):
        self.requirements_file = requirements_file
        self.results = {
            'total_packages': 0,
            'successful': [],
            'failed': [],
            'warnings': [],
            'timing': {}
        }
    
    def test_individual_packages(self):
        """ê° íŒ¨í‚¤ì§€ë¥¼ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸"""
        with open(self.requirements_file, 'r') as f:
            packages = [line.strip() for line in f 
                       if line.strip() and not line.startswith('#')]
        
        self.results['total_packages'] = len(packages)
        
        for package in packages:
            print(f"Testing: {package}")
            start_time = time.time()
            
            # uvë¡œ ì„¤ì¹˜ ì‹œë„
            result = subprocess.run(
                ['uv', 'pip', 'install', '--dry-run', package],
                capture_output=True,
                text=True
            )
            
            elapsed = time.time() - start_time
            
            if result.returncode == 0:
                self.results['successful'].append(package)
                self.results['timing'][package] = elapsed
            else:
                self.results['failed'].append({
                    'package': package,
                    'error': result.stderr,
                    'stdout': result.stdout
                })
                
                # ëŒ€ì²´ ë°©ë²• ì‹œë„
                self._try_alternative_install(package)
    
    def _try_alternative_install(self, package):
        """ì‹¤íŒ¨í•œ íŒ¨í‚¤ì§€ì— ëŒ€í•œ ëŒ€ì²´ ì„¤ì¹˜ ë°©ë²• ì‹œë„"""
        # --pre ì˜µì…˜ìœ¼ë¡œ ì‹œë„
        result = subprocess.run(
            ['uv', 'pip', 'install', '--dry-run', '--pre', package],
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            self.results['warnings'].append({
                'package': package,
                'message': 'Requires --pre flag',
                'solution': f'uv pip install --pre {package}'
            })
    
    def test_bulk_installation(self):
        """ì „ì²´ requirements íŒŒì¼ë¡œ í•œë²ˆì— ì„¤ì¹˜ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ”„ Bulk installation test...")
        
        start_time = time.time()
        result = subprocess.run(
            ['uv', 'pip', 'install', '--dry-run', '-r', self.requirements_file],
            capture_output=True,
            text=True
        )
        elapsed = time.time() - start_time
        
        self.results['bulk_install'] = {
            'success': result.returncode == 0,
            'time': elapsed,
            'output': result.stdout,
            'error': result.stderr
        }
    
    def compare_with_pip(self):
        """pipê³¼ ì„±ëŠ¥ ë¹„êµ"""
        print("\nâš¡ Performance comparison...")
        
        # pip ì„¤ì¹˜ ì‹œê°„
        start_time = time.time()
        subprocess.run(
            ['pip', 'install', '--dry-run', '-r', self.requirements_file],
            capture_output=True
        )
        pip_time = time.time() - start_time
        
        # uv ì„¤ì¹˜ ì‹œê°„
        start_time = time.time()
        subprocess.run(
            ['uv', 'pip', 'install', '--dry-run', '-r', self.requirements_file],
            capture_output=True
        )
        uv_time = time.time() - start_time
        
        self.results['performance'] = {
            'pip_time': pip_time,
            'uv_time': uv_time,
            'speedup': pip_time / uv_time if uv_time > 0 else 0
        }
    
    def generate_report(self):
        """ìƒì„¸ ë³´ê³ ì„œ ìƒì„±"""
        success_rate = len(self.results['successful']) / self.results['total_packages'] * 100
        
        report = f"""
# uv í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ê²°ê³¼

## ìš”ì•½
- ì „ì²´ íŒ¨í‚¤ì§€: {self.results['total_packages']}
- ì„±ê³µ: {len(self.results['successful'])} ({success_rate:.1f}%)
- ì‹¤íŒ¨: {len(self.results['failed'])}
- ê²½ê³ : {len(self.results['warnings'])}

## ì„±ëŠ¥ ë¹„êµ
- pip ì„¤ì¹˜ ì‹œê°„: {self.results['performance']['pip_time']:.2f}ì´ˆ
- uv ì„¤ì¹˜ ì‹œê°„: {self.results['performance']['uv_time']:.2f}ì´ˆ
- ì†ë„ í–¥ìƒ: {self.results['performance']['speedup']:.1f}x

## ì‹¤íŒ¨í•œ íŒ¨í‚¤ì§€
"""
        for fail in self.results['failed']:
            report += f"\n### {fail['package']}\n"
            report += f"```\n{fail['error']}\n```\n"
        
        with open('results/compatibility_report.md', 'w') as f:
            f.write(report)
        
        # JSON í˜•ì‹ìœ¼ë¡œë„ ì €ì¥
        with open('results/compatibility_results.json', 'w') as f:
            json.dump(self.results, f, indent=2)

if __name__ == '__main__':
    tester = UvCompatibilityTester('../requirements.txt')
    tester.test_individual_packages()
    tester.test_bulk_installation()
    tester.compare_with_pip()
    tester.generate_report()
EOF

# 4. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
cat > run_tests.sh << 'EOF'
#!/bin/bash
echo "ğŸš€ Starting uv compatibility tests..."

# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -f Dockerfile.test -t uv-test .

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
docker run --rm \
    -v $(pwd)/../requirements.txt:/test/requirements.txt:ro \
    -v $(pwd)/results:/test/results \
    -v $(pwd)/logs:/test/logs \
    uv-test python test_uv_installation.py

echo "âœ… Tests completed. Check results/ directory for reports."
EOF

chmod +x run_tests.sh
```

#### SubTask 1.2.2: íŒ¨í‚¤ì§€ë³„ í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 3ì‹œê°„

```python
# scripts/create_compatibility_matrix.py
import json
import pandas as pd
from typing import Dict, List, Tuple
import subprocess
import concurrent.futures
from dataclasses import dataclass
import platform

@dataclass
class PackageTestResult:
    name: str
    version: str
    pip_compatible: bool
    uv_compatible: bool
    install_time_pip: float
    install_time_uv: float
    error_message: str = ""
    special_flags: List[str] = None
    platform: str = platform.system()

class CompatibilityMatrixBuilder:
    def __init__(self):
        self.results: List[PackageTestResult] = []
        self.test_environments = self._setup_test_environments()
    
    def _setup_test_environments(self):
        """pipê³¼ uv í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •"""
        return {
            'pip': {
                'venv_path': '.venv_pip_test',
                'command': ['pip', 'install']
            },
            'uv': {
                'venv_path': '.venv_uv_test',
                'command': ['uv', 'pip', 'install']
            }
        }
    
    def test_package(self, package_spec: str) -> PackageTestResult:
        """ë‹¨ì¼ íŒ¨í‚¤ì§€ í…ŒìŠ¤íŠ¸"""
        package_name, version = self._parse_package_spec(package_spec)
        
        result = PackageTestResult(
            name=package_name,
            version=version,
            pip_compatible=False,
            uv_compatible=False,
            install_time_pip=0,
            install_time_uv=0
        )
        
        # pip í…ŒìŠ¤íŠ¸
        pip_result = self._test_with_pip(package_spec)
        result.pip_compatible = pip_result['success']
        result.install_time_pip = pip_result['time']
        
        # uv í…ŒìŠ¤íŠ¸
        uv_result = self._test_with_uv(package_spec)
        result.uv_compatible = uv_result['success']
        result.install_time_uv = uv_result['time']
        result.error_message = uv_result.get('error', '')
        
        # íŠ¹ìˆ˜ í”Œë˜ê·¸ í•„ìš” ì—¬ë¶€ í™•ì¸
        if not result.uv_compatible:
            result.special_flags = self._check_special_flags(package_spec)
        
        return result
    
    def _test_with_pip(self, package: str) -> Dict:
        """pipìœ¼ë¡œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í…ŒìŠ¤íŠ¸"""
        import time
        
        start = time.time()
        try:
            subprocess.run(
                ['pip', 'install', '--dry-run', package],
                check=True,
                capture_output=True,
                timeout=60
            )
            return {'success': True, 'time': time.time() - start}
        except subprocess.CalledProcessError as e:
            return {
                'success': False, 
                'time': time.time() - start,
                'error': e.stderr.decode()
            }
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'time': 60,
                'error': 'Timeout'
            }
    
    def _test_with_uv(self, package: str) -> Dict:
        """uvë¡œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í…ŒìŠ¤íŠ¸"""
        import time
        
        start = time.time()
        try:
            subprocess.run(
                ['uv', 'pip', 'install', '--dry-run', package],
                check=True,
                capture_output=True,
                timeout=60
            )
            return {'success': True, 'time': time.time() - start}
        except subprocess.CalledProcessError as e:
            return {
                'success': False,
                'time': time.time() - start,
                'error': e.stderr.decode()
            }
    
    def _check_special_flags(self, package: str) -> List[str]:
        """íŠ¹ìˆ˜ í”Œë˜ê·¸ë¡œ ì¬ì‹œë„"""
        flags_to_try = ['--pre', '--no-binary :all:', '--only-binary :all:']
        working_flags = []
        
        for flag in flags_to_try:
            try:
                subprocess.run(
                    ['uv', 'pip', 'install', '--dry-run', flag, package],
                    check=True,
                    capture_output=True
                )
                working_flags.append(flag)
            except:
                pass
        
        return working_flags
    
    def _parse_package_spec(self, spec: str) -> Tuple[str, str]:
        """íŒ¨í‚¤ì§€ ëª…ì„¸ íŒŒì‹±"""
        if '==' in spec:
            name, version = spec.split('==')
            return name.strip(), version.strip()
        elif '>=' in spec or '<=' in spec:
            # ë²„ì „ ë²”ìœ„ ì§€ì •
            for op in ['>=', '<=', '>', '<', '~=']:
                if op in spec:
                    name, version = spec.split(op)
                    return name.strip(), f"{op}{version.strip()}"
        return spec.strip(), 'any'
    
    def test_all_packages(self, requirements_file: str):
        """ëª¨ë“  íŒ¨í‚¤ì§€ ë³‘ë ¬ í…ŒìŠ¤íŠ¸"""
        with open(requirements_file, 'r') as f:
            packages = [line.strip() for line in f 
                       if line.strip() and not line.startswith('#')]
        
        print(f"ğŸ§ª Testing {len(packages)} packages...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            future_to_package = {
                executor.submit(self.test_package, pkg): pkg 
                for pkg in packages
            }
            
            for future in concurrent.futures.as_completed(future_to_package):
                package = future_to_package[future]
                try:
                    result = future.result()
                    self.results.append(result)
                    self._print_progress(result)
                except Exception as exc:
                    print(f'âŒ {package} generated an exception: {exc}')
    
    def _print_progress(self, result: PackageTestResult):
        """ì§„í–‰ ìƒí™© ì¶œë ¥"""
        if result.uv_compatible:
            speedup = result.install_time_pip / result.install_time_uv if result.install_time_uv > 0 else 0
            print(f"âœ… {result.name}: {speedup:.1f}x faster")
        else:
            print(f"âŒ {result.name}: {result.error_message[:50]}...")
    
    def generate_matrix(self):
        """í˜¸í™˜ì„± ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
        # DataFrame ìƒì„±
        df = pd.DataFrame([
            {
                'Package': r.name,
                'Version': r.version,
                'pip': 'âœ…' if r.pip_compatible else 'âŒ',
                'uv': 'âœ…' if r.uv_compatible else 'âŒ',
                'pip_time': f"{r.install_time_pip:.2f}s",
                'uv_time': f"{r.install_time_uv:.2f}s",
                'Speedup': f"{r.install_time_pip/r.install_time_uv:.1f}x" if r.install_time_uv > 0 else 'N/A',
                'Special_Flags': ', '.join(r.special_flags) if r.special_flags else '',
                'Error': r.error_message[:50] if r.error_message else ''
            }
            for r in self.results
        ])
        
        # HTML ë³´ê³ ì„œ ìƒì„±
        html_report = f"""
<!DOCTYPE html>
<html>
<head>
    <title>uv Compatibility Matrix</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #4CAF50; color: white; }}
        tr:nth-child(even) {{ background-color: #f2f2f2; }}
        .success {{ color: green; }}
        .failure {{ color: red; }}
        .summary {{ background-color: #e7f3fe; padding: 15px; margin: 20px 0; }}
    </style>
</head>
<body>
    <h1>uv Compatibility Matrix</h1>
    <div class="summary">
        <h2>Summary</h2>
        <p>Total Packages: {len(self.results)}</p>
        <p>uv Compatible: {sum(1 for r in self.results if r.uv_compatible)} ({sum(1 for r in self.results if r.uv_compatible)/len(self.results)*100:.1f}%)</p>
        <p>Average Speedup: {sum(r.install_time_pip/r.install_time_uv for r in self.results if r.install_time_uv > 0)/len([r for r in self.results if r.install_time_uv > 0]):.1f}x</p>
    </div>
    {df.to_html(index=False, escape=False)}
</body>
</html>
"""
        
        # íŒŒì¼ ì €ì¥
        with open('results/compatibility_matrix.html', 'w') as f:
            f.write(html_report)
        
        df.to_csv('results/compatibility_matrix.csv', index=False)
        
        # ë¬¸ì œ íŒ¨í‚¤ì§€ ë³„ë„ ì €ì¥
        problem_packages = df[df['uv'] == 'âŒ']
        if not problem_packages.empty:
            problem_packages.to_csv('results/problem_packages.csv', index=False)
            
            # í•´ê²° ë°©ì•ˆ ë¬¸ì„œ ìƒì„±
            self._generate_solutions_doc(problem_packages)
    
    def _generate_solutions_doc(self, problem_packages):
        """ë¬¸ì œ íŒ¨í‚¤ì§€ í•´ê²° ë°©ì•ˆ ë¬¸ì„œ"""
        doc = """# uv í˜¸í™˜ì„± ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

## ë¬¸ì œ íŒ¨í‚¤ì§€ ë° í•´ê²° ë°©ì•ˆ

"""
        for _, pkg in problem_packages.iterrows():
            doc += f"""
### {pkg['Package']} {pkg['Version']}
- **ì˜¤ë¥˜**: {pkg['Error']}
- **í•´ê²° ë°©ì•ˆ**:
"""
            if pkg['Special_Flags']:
                doc += f"  - íŠ¹ìˆ˜ í”Œë˜ê·¸ ì‚¬ìš©: `uv pip install {pkg['Special_Flags']} {pkg['Package']}`\n"
            
            # ì•Œë ¤ì§„ í•´ê²° ë°©ì•ˆ ì¶”ê°€
            solutions = self._get_known_solutions(pkg['Package'])
            for solution in solutions:
                doc += f"  - {solution}\n"
        
        with open('results/problem_solutions.md', 'w') as f:
            f.write(doc)
    
    def _get_known_solutions(self, package: str) -> List[str]:
        """ì•Œë ¤ì§„ íŒ¨í‚¤ì§€ë³„ í•´ê²° ë°©ì•ˆ"""
        known_solutions = {
            'mysqlclient': [
                'MySQL ê°œë°œ í—¤ë” ì„¤ì¹˜ í•„ìš”: `apt-get install libmysqlclient-dev`',
                'ëŒ€ì•ˆ: `pymysql` ì‚¬ìš© ê³ ë ¤'
            ],
            'psycopg2': [
                'PostgreSQL ê°œë°œ í—¤ë” ì„¤ì¹˜ í•„ìš”: `apt-get install libpq-dev`',
                'ëŒ€ì•ˆ: `psycopg2-binary` ì‚¬ìš©'
            ],
            'Pillow': [
                'ì´ë¯¸ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”: `apt-get install libjpeg-dev zlib1g-dev`'
            ]
        }
        
        return known_solutions.get(package, ['ìˆ˜ë™ í™•ì¸ í•„ìš”'])

if __name__ == '__main__':
    builder = CompatibilityMatrixBuilder()
    builder.test_all_packages('requirements.txt')
    builder.generate_matrix()
    print("\nâœ… Compatibility matrix generated in results/")
```

#### SubTask 1.2.3: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰
**ë‹´ë‹¹ì**: ì„±ëŠ¥ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 1ì‹œê°„

```python
# scripts/performance_benchmark.py
import subprocess
import time
import statistics
import json
import matplotlib.pyplot as plt
from typing import Dict, List
import psutil
import os

class PerformanceBenchmark:
    def __init__(self):
        self.results = {
            'pip': {'times': [], 'memory': [], 'cpu': []},
            'uv': {'times': [], 'memory': [], 'cpu': []}
        }
        self.scenarios = [
            {
                'name': 'clean_install',
                'description': 'Clean installation of all dependencies',
                'setup': self._clean_cache,
                'requirements': 'requirements.txt'
            },
            {
                'name': 'cached_install',
                'description': 'Installation with populated cache',
                'setup': self._populate_cache,
                'requirements': 'requirements.txt'
            },
            {
                'name': 'single_package',
                'description': 'Single package installation',
                'setup': self._clean_cache,
                'package': 'requests'
            },
            {
                'name': 'large_package',
                'description': 'Large package with many dependencies',
                'setup': self._clean_cache,
                'package': 'pandas[all]'
            }
        ]
    
    def _clean_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        subprocess.run(['pip', 'cache', 'purge'], capture_output=True)
        # uv ìºì‹œ ì •ë¦¬
        cache_dir = os.path.expanduser('~/.cache/uv')
        if os.path.exists(cache_dir):
            import shutil
            shutil.rmtree(cache_dir)
    
    def _populate_cache(self):
        """ìºì‹œ ì‚¬ì „ ë¡œë“œ"""
        # pipìœ¼ë¡œ í•œ ë²ˆ ì„¤ì¹˜í•˜ì—¬ ìºì‹œ ìƒì„±
        subprocess.run(
            ['pip', 'install', '--dry-run', '-r', 'requirements.txt'],
            capture_output=True
        )
    
    def benchmark_scenario(self, scenario: Dict) -> Dict:
        """ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ ë²¤ì¹˜ë§ˆí¬"""
        results = {
            'scenario': scenario['name'],
            'description': scenario['description'],
            'pip': [],
            'uv': []
        }
        
        # 5íšŒ ë°˜ë³µ ì¸¡ì •
        for i in range(5):
            print(f"  Run {i+1}/5...")
            
            # Setup
            scenario['setup']()
            
            # pip ì¸¡ì •
            pip_result = self._measure_pip(scenario)
            results['pip'].append(pip_result)
            
            # Setup
            scenario['setup']()
            
            # uv ì¸¡ì •
            uv_result = self._measure_uv(scenario)
            results['uv'].append(uv_result)
        
        return results
    
    def _measure_pip(self, scenario: Dict) -> Dict:
        """pip ì„±ëŠ¥ ì¸¡ì •"""
        process = psutil.Popen(
            self._get_pip_command(scenario),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        start_time = time.time()
        cpu_percent = []
        memory_mb = []
        
        # í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§
        while process.poll() is None:
            try:
                cpu_percent.append(process.cpu_percent(interval=0.1))
                memory_mb.append(process.memory_info().rss / 1024 / 1024)
            except psutil.NoSuchProcess:
                break
        
        end_time = time.time()
        
        return {
            'time': end_time - start_time,
            'cpu_avg': statistics.mean(cpu_percent) if cpu_percent else 0,
            'memory_peak': max(memory_mb) if memory_mb else 0,
            'exit_code': process.returncode
        }
    
    def _measure_uv(self, scenario: Dict) -> Dict:
        """uv ì„±ëŠ¥ ì¸¡ì •"""
        process = psutil.Popen(
            self._get_uv_command(scenario),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        start_time = time.time()
        cpu_percent = []
        memory_mb = []
        
        while process.poll() is None:
            try:
                cpu_percent.append(process.cpu_percent(interval=0.1))
                memory_mb.append(process.memory_info().rss / 1024 / 1024)
            except psutil.NoSuchProcess:
                break
        
        end_time = time.time()
        
        return {
            'time': end_time - start_time,
            'cpu_avg': statistics.mean(cpu_percent) if cpu_percent else 0,
            'memory_peak': max(memory_mb) if memory_mb else 0,
            'exit_code': process.returncode
        }
    
    def _get_pip_command(self, scenario: Dict) -> List[str]:
        """pip ëª…ë ¹ ìƒì„±"""
        if 'requirements' in scenario:
            return ['pip', 'install', '-r', scenario['requirements']]
        else:
            return ['pip', 'install', scenario['package']]
    
    def _get_uv_command(self, scenario: Dict) -> List[str]:
        """uv ëª…ë ¹ ìƒì„±"""
        if 'requirements' in scenario:
            return ['uv', 'pip', 'install', '-r', scenario['requirements']]
        else:
            return ['uv', 'pip', 'install', scenario['package']]
    
    def run_all_benchmarks(self):
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        all_results = []
        
        for scenario in self.scenarios:
            print(f"\nğŸƒ Running benchmark: {scenario['name']}")
            result = self.benchmark_scenario(scenario)
            all_results.append(result)
            
            # ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
            pip_avg = statistics.mean([r['time'] for r in result['pip']])
            uv_avg = statistics.mean([r['time'] for r in result['uv']])
            speedup = pip_avg / uv_avg if uv_avg > 0 else 0
            
            print(f"  pip: {pip_avg:.2f}s")
            print(f"  uv:  {uv_avg:.2f}s")
            print(f"  Speedup: {speedup:.1f}x")
        
        self.generate_report(all_results)
    
    def generate_report(self, results: List[Dict]):
        """ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ ìƒì„±"""
        # ê·¸ë˜í”„ ìƒì„±
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('pip vs uv Performance Comparison')
        
        for idx, result in enumerate(results):
            ax = axes[idx // 2, idx % 2]
            
            # ì‹œê°„ ë¹„êµ
            pip_times = [r['time'] for r in result['pip']]
            uv_times = [r['time'] for r in result['uv']]
            
            ax.boxplot([pip_times, uv_times], labels=['pip', 'uv'])
            ax.set_title(result['scenario'])
            ax.set_ylabel('Time (seconds)')
            
            # í‰ê· ê³¼ ì†ë„ í–¥ìƒ í‘œì‹œ
            pip_avg = statistics.mean(pip_times)
            uv_avg = statistics.mean(uv_times)
            speedup = pip_avg / uv_avg
            
            ax.text(0.5, 0.95, f'Speedup: {speedup:.1f}x',
                   transform=ax.transAxes,
                   verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        plt.tight_layout()
        plt.savefig('results/performance_comparison.png')
        
        # ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
        report = {
            'summary': {
                'date': time.strftime('%Y-%m-%d %H:%M:%S'),
                'scenarios': len(results)
            },
            'results': []
        }
        
        for result in results:
            scenario_summary = {
                'scenario': result['scenario'],
                'description': result['description'],
                'pip': {
                    'avg_time': statistics.mean([r['time'] for r in result['pip']]),
                    'std_dev': statistics.stdev([r['time'] for r in result['pip']]),
                    'avg_cpu': statistics.mean([r['cpu_avg'] for r in result['pip']]),
                    'peak_memory': max([r['memory_peak'] for r in result['pip']])
                },
                'uv': {
                    'avg_time': statistics.mean([r['time'] for r in result['uv']]),
                    'std_dev': statistics.stdev([r['time'] for r in result['uv']]),
                    'avg_cpu': statistics.mean([r['cpu_avg'] for r in result['uv']]),
                    'peak_memory': max([r['memory_peak'] for r in result['uv']])
                }
            }
            
            scenario_summary['speedup'] = (
                scenario_summary['pip']['avg_time'] / 
                scenario_summary['uv']['avg_time']
            )
            
            report['results'].append(scenario_summary)
        
        # JSON ë³´ê³ ì„œ ì €ì¥
        with open('results/benchmark_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        # Markdown ë³´ê³ ì„œ ìƒì„±
        self._generate_markdown_report(report)
    
    def _generate_markdown_report(self, report: Dict):
        """Markdown í˜•ì‹ ë³´ê³ ì„œ ìƒì„±"""
        md = f"""# Performance Benchmark Report

**Date**: {report['summary']['date']}

## Executive Summary

uv demonstrates significant performance improvements over pip across all tested scenarios.

## Detailed Results

| Scenario | pip (avg) | uv (avg) | Speedup | Memory Savings |
|----------|-----------|----------|---------|----------------|
"""
        
        for result in report['results']:
            pip_time = result['pip']['avg_time']
            uv_time = result['uv']['avg_time']
            speedup = result['speedup']
            memory_savings = (
                (result['pip']['peak_memory'] - result['uv']['peak_memory']) / 
                result['pip']['peak_memory'] * 100
            )
            
            md += f"| {result['scenario']} | {pip_time:.2f}s | {uv_time:.2f}s | {speedup:.1f}x | {memory_savings:.0f}% |\n"
        
        md += """

## Recommendations

Based on the benchmark results:

1. **Immediate adoption recommended** - uv shows consistent performance improvements
2. **CI/CD optimization** - Expect 50-80% reduction in dependency installation time
3. **Developer experience** - Significant time savings in daily development workflow

## Test Environment

- Python Version: 3.11
- OS: Linux/macOS
- CPU: [System CPU]
- Memory: [System Memory]
"""
        
        with open('results/benchmark_report.md', 'w') as f:
            f.write(md)

if __name__ == '__main__':
    benchmark = PerformanceBenchmark()
    benchmark.run_all_benchmarks()
    print("\nâœ… Benchmark complete. Results saved to results/")
```

### Task 1.3: ë¬¸ì„œí™” ë° ê°€ì´ë“œ ì‘ì„±

#### SubTask 1.3.1: ê°œë°œì ê°€ì´ë“œ ì‘ì„±
**ë‹´ë‹¹ì**: ê¸°ìˆ  ë¬¸ì„œ ë‹´ë‹¹ì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```markdown
# docs/developer-guide-uv.md

# uv ê°œë°œì ê°€ì´ë“œ

## ëª©ì°¨
1. [ì‹œì‘í•˜ê¸°](#ì‹œì‘í•˜ê¸°)
2. [ê¸°ë³¸ ì‚¬ìš©ë²•](#ê¸°ë³¸-ì‚¬ìš©ë²•)
3. [ê³ ê¸‰ ê¸°ëŠ¥](#ê³ ê¸‰-ê¸°ëŠ¥)
4. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)
5. [ëª¨ë²” ì‚¬ë¡€](#ëª¨ë²”-ì‚¬ë¡€)

## ì‹œì‘í•˜ê¸°

### uvë€?
uvëŠ” Rustë¡œ ì‘ì„±ëœ ì´ˆê³ ì† Python íŒ¨í‚¤ì§€ ê´€ë¦¬ìì…ë‹ˆë‹¤. pip ëŒ€ë¹„ 10-100ë°° ë¹ ë¥¸ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

### ì„¤ì¹˜

#### macOS/Linux
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

#### Windows
```powershell
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

### ì„¤ì¹˜ í™•ì¸
```bash
uv --version
```

## ê¸°ë³¸ ì‚¬ìš©ë²•

### ê°€ìƒí™˜ê²½ ìƒì„±
```bash
# ê¸°ë³¸ ê°€ìƒí™˜ê²½ ìƒì„±
uv venv

# Python ë²„ì „ ì§€ì •
uv venv --python python3.11

# íŠ¹ì • ê²½ë¡œì— ìƒì„±
uv venv myenv
```

### íŒ¨í‚¤ì§€ ì„¤ì¹˜

#### ë‹¨ì¼ íŒ¨í‚¤ì§€
```bash
# ìµœì‹  ë²„ì „ ì„¤ì¹˜
uv pip install requests

# íŠ¹ì • ë²„ì „ ì„¤ì¹˜
uv pip install requests==2.31.0

# ë²„ì „ ë²”ìœ„ ì§€ì •
uv pip install "requests>=2.28,<3.0"
```

#### requirements.txt ì‚¬ìš©
```bash
# ì„¤ì¹˜
uv pip install -r requirements.txt

# ë™ê¸°í™” (ì •í™•íˆ ì¼ì¹˜í•˜ë„ë¡)
uv pip sync requirements.txt
```

#### ê°œë°œ ì˜ì¡´ì„±
```bash
# ê°œë°œ ì˜ì¡´ì„± í¬í•¨ ì„¤ì¹˜
uv pip install -e ".[dev]"
```

### íŒ¨í‚¤ì§€ ê´€ë¦¬

#### ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ í™•ì¸
```bash
uv pip list
uv pip freeze
```

#### íŒ¨í‚¤ì§€ ì œê±°
```bash
uv pip uninstall requests
```

#### íŒ¨í‚¤ì§€ ì—…ê·¸ë ˆì´ë“œ
```bash
uv pip install --upgrade requests
```

## ê³ ê¸‰ ê¸°ëŠ¥

### ì˜ì¡´ì„± í•´ê²°

uvëŠ” pipë³´ë‹¤ ì—„ê²©í•œ ì˜ì¡´ì„± í•´ê²°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

```bash
# ì˜ì¡´ì„± í•´ê²° ê³¼ì • í™•ì¸
uv pip install requests --verbose

# ì˜ì¡´ì„± ì¶©ëŒ ë¬´ì‹œ (ê¶Œì¥í•˜ì§€ ì•ŠìŒ)
uv pip install requests --no-deps
```

### ìºì‹±

uvëŠ” íš¨ìœ¨ì ì¸ ìºì‹±ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤:

```bash
# ìºì‹œ ìœ„ì¹˜ í™•ì¸
echo $HOME/.cache/uv

# ìºì‹œ ì •ë¦¬
rm -rf ~/.cache/uv
```

### ì¸ë±ìŠ¤ ì„¤ì •

#### ì»¤ìŠ¤í…€ ì¸ë±ìŠ¤ ì‚¬ìš©
```bash
# ë‹¨ì¼ ëª…ë ¹
uv pip install --index-url https://pypi.company.com/simple/ package

# í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •
export UV_INDEX_URL=https://pypi.company.com/simple/
```

#### ì¶”ê°€ ì¸ë±ìŠ¤
```bash
uv pip install --extra-index-url https://pypi.company.com/simple/ package
```

### í”„ë¡ì‹œ ì„¤ì •

```bash
# HTTP í”„ë¡ì‹œ
export HTTP_PROXY=http://proxy.company.com:8080
export HTTPS_PROXY=http://proxy.company.com:8080

# í”„ë¡ì‹œ ì˜ˆì™¸
export NO_PROXY=localhost,127.0.0.1,.company.com
```

## ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

#### 1. "Package not found" ì˜¤ë¥˜
```bash
# ì¸ë±ìŠ¤ ê°±ì‹ 
uv pip install --refresh package

# í”„ë¦¬ë¦´ë¦¬ì¦ˆ í¬í•¨
uv pip install --pre package
```

#### 2. ì»´íŒŒì¼ ì˜¤ë¥˜
```bash
# ë°”ì´ë„ˆë¦¬ íœ  ì‚¬ìš©
uv pip install --only-binary :all: package

# ì†ŒìŠ¤ì—ì„œ ë¹Œë“œ
uv pip install --no-binary :all: package
```

#### 3. ì˜ì¡´ì„± ì¶©ëŒ
```bash
# ìƒì„¸ ì •ë³´ í™•ì¸
uv pip install package --verbose

# ì˜ì¡´ì„± íŠ¸ë¦¬ í™•ì¸
uv pip install pipdeptree
pipdeptree
```

### ë””ë²„ê¹…

#### ìƒì„¸ ë¡œê·¸
```bash
uv pip install package -vvv
```

#### í™˜ê²½ ë³€ìˆ˜
```bash
# ë””ë²„ê·¸ ëª¨ë“œ
export UV_DEBUG=1

# ë¡œê·¸ íŒŒì¼
export UV_LOG_FILE=/tmp/uv.log
```

## ëª¨ë²” ì‚¬ë¡€

### 1. requirements íŒŒì¼ ë¶„ë¦¬

```
requirements/
â”œâ”€â”€ base.txt       # ê³µí†µ ì˜ì¡´ì„±
â”œâ”€â”€ dev.txt        # ê°œë°œ ì˜ì¡´ì„±
â”œâ”€â”€ prod.txt       # í”„ë¡œë•ì…˜ ì˜ì¡´ì„±
â””â”€â”€ test.txt       # í…ŒìŠ¤íŠ¸ ì˜ì¡´ì„±
```

```bash
# base.txt
fastapi==0.104.1
pydantic==2.5.0

# dev.txt
-r base.txt
pytest==7.4.3
black==23.11.0

# ì‚¬ìš©
uv pip install -r requirements/dev.txt
```

### 2. ë²„ì „ ê³ ì •

```bash
# í˜„ì¬ í™˜ê²½ ê³ ì •
uv pip freeze > requirements.lock

# ê³ ì •ëœ ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜
uv pip sync requirements.lock
```

### 3. CI/CD ìµœì í™”

```yaml
# .github/workflows/ci.yml
- name: Cache uv
  uses: actions/cache@v3
  with:
    path: ~/.cache/uv
    key: ${{ runner.os }}-uv-${{ hashFiles('**/requirements*.txt') }}

- name: Install dependencies
  run: |
    curl -LsSf https://astral.sh/uv/install.sh | sh
    uv pip install -r requirements.txt
```

### 4. Docker ìµœì í™”

```dockerfile
# ìºì‹œ í™œìš©ì„ ìœ„í•œ ë ˆì´ì–´ ë¶„ë¦¬
COPY requirements.txt .
RUN uv pip install -r requirements.txt

COPY . .
```

### 5. ê°œë°œ ì›Œí¬í”Œë¡œìš°

```bash
# Makefile
.PHONY: install
install:
	uv pip sync requirements.txt

.PHONY: update
update:
	uv pip install --upgrade -r requirements.txt
	uv pip freeze > requirements.lock
```

## ë§ˆì´ê·¸ë ˆì´ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] uv ì„¤ì¹˜ ì™„ë£Œ
- [ ] ê°€ìƒí™˜ê²½ ì¬ìƒì„±
- [ ] requirements.txt í˜¸í™˜ì„± í™•ì¸
- [ ] ê°œë°œ ìŠ¤í¬ë¦½íŠ¸ ì—…ë°ì´íŠ¸
- [ ] CI/CD íŒŒì´í”„ë¼ì¸ ìˆ˜ì •
- [ ] íŒ€ì› êµìœ¡ ì™„ë£Œ

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [uv ê³µì‹ ë¬¸ì„œ](https://github.com/astral-sh/uv)
- [ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬](./benchmark-results.md)
- [ë¬¸ì œ í•´ê²° ê°€ì´ë“œ](./troubleshooting.md)
- [íŒ€ Slack ì±„ë„](#uv-support)
```

#### SubTask 1.3.2: ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ ì‘ì„±
**ë‹´ë‹¹ì**: ì‹œë‹ˆì–´ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 1ì‹œê°„

```markdown
# docs/migration-guide.md

# pip â†’ uv ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

## ê°œìš”
ì´ ë¬¸ì„œëŠ” ê¸°ì¡´ pip ê¸°ë°˜ í”„ë¡œì íŠ¸ë¥¼ uvë¡œ ì•ˆì „í•˜ê²Œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ëŠ” ë‹¨ê³„ë³„ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ì‚¬ì „ ì¤€ë¹„

### 1. í˜„ì¬ ìƒíƒœ ë°±ì—…
```bash
# í˜„ì¬ íŒ¨í‚¤ì§€ ëª©ë¡ ì €ì¥
pip freeze > backup/pip_freeze_$(date +%Y%m%d).txt

# ê°€ìƒí™˜ê²½ ì •ë³´ ì €ì¥
pip list --format=json > backup/pip_list_$(date +%Y%m%d).json
```

### 2. uv ì„¤ì¹˜
```bash
# macOS/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# ì„¤ì¹˜ í™•ì¸
uv --version
```

## ë‹¨ê³„ë³„ ë§ˆì´ê·¸ë ˆì´ì…˜

### Phase 1: ê°œë°œ í™˜ê²½ (Day 1-3)

#### Step 1: í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶•
```bash
# ìƒˆ ë¸Œëœì¹˜ ìƒì„±
git checkout -b feature/uv-migration

# ê¸°ì¡´ ê°€ìƒí™˜ê²½ ë°±ì—…
mv .venv .venv_pip_backup

# uvë¡œ ìƒˆ ê°€ìƒí™˜ê²½ ìƒì„±
uv venv
source .venv/bin/activate
```

#### Step 2: ì˜ì¡´ì„± ì„¤ì¹˜ í…ŒìŠ¤íŠ¸
```bash
# ê¸°ë³¸ ì„¤ì¹˜ ì‹œë„
uv pip install -r requirements.txt

# ì‹¤íŒ¨ ì‹œ ê°œë³„ íŒ¨í‚¤ì§€ í™•ì¸
while read package; do
    echo "Testing: $package"
    uv pip install "$package" || echo "$package" >> failed_packages.txt
done < requirements.txt
```

#### Step 3: ë¬¸ì œ í•´ê²°
```bash
# ì‹¤íŒ¨í•œ íŒ¨í‚¤ì§€ ë¶„ì„
cat failed_packages.txt

# ì¼ë°˜ì ì¸ í•´ê²° ë°©ë²•
# 1. í”„ë¦¬ë¦´ë¦¬ì¦ˆ ë²„ì „ í—ˆìš©
uv pip install --pre problematic-package

# 2. ë°”ì´ë„ˆë¦¬ ì‚¬ìš©
uv pip install --only-binary :all: problematic-package

# 3. ëŒ€ì²´ íŒ¨í‚¤ì§€ ê²€í† 
# psycopg2 â†’ psycopg2-binary
# mysqlclient â†’ pymysql
```

### Phase 2: ìŠ¤í¬ë¦½íŠ¸ ì—…ë°ì´íŠ¸ (Day 4-5)

#### Step 1: ê°œë°œ ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì •

**setup.sh (ì´ì „)**
```bash
#!/bin/bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
pip install -r requirements-dev.txt
```

**setup.sh (ì´í›„)**
```bash
#!/bin/bash
if ! command -v uv &> /dev/null; then
    echo "Installing uv..."
    curl -LsSf https://astral.sh/uv/install.sh | sh
fi

uv venv
source .venv/bin/activate
uv pip sync requirements.txt
uv pip install -r requirements-dev.txt
```

#### Step 2: Makefile ì—…ë°ì´íŠ¸

**Makefile (ì´ì „)**
```makefile
install:
	pip install -r requirements.txt

update:
	pip install --upgrade -r requirements.txt
	pip freeze > requirements.txt
```

**Makefile (ì´í›„)**
```makefile
install:
	uv pip sync requirements.txt

update:
	uv pip install --upgrade -r requirements.txt
	uv pip freeze > requirements.lock

install-dev:
	uv pip install -r requirements-dev.txt
```

### Phase 3: CI/CD íŒŒì´í”„ë¼ì¸ (Day 6-7)

#### Step 1: GitHub Actions ìˆ˜ì •

**ì´ì „ (.github/workflows/test.yml)**
```yaml
- name: Install dependencies
  run: |
    pip install --upgrade pip
    pip install -r requirements.txt
```

**ì´í›„ (.github/workflows/test.yml)**
```yaml
- name: Install uv
  run: |
    curl -LsSf https://astral.sh/uv/install.sh | sh
    echo "$HOME/.cargo/bin" >> $GITHUB_PATH

- name: Cache uv
  uses: actions/cache@v3
  with:
    path: ~/.cache/uv
    key: ${{ runner.os }}-uv-${{ hashFiles('requirements.txt') }}

- name: Install dependencies
  run: |
    uv venv
    source .venv/bin/activate
    uv pip install -r requirements.txt
```

#### Step 2: Docker ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸

**Dockerfile (ì´ì „)**
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
```

**Dockerfile (ì´í›„)**
```dockerfile
FROM python:3.11-slim
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv
WORKDIR /app
COPY requirements.txt .
RUN uv venv && . .venv/bin/activate && uv pip install -r requirements.txt
ENV PATH="/app/.venv/bin:$PATH"
COPY . .
```

### Phase 4: í”„ë¡œë•ì…˜ ë°°í¬ (Day 8-10)

#### Step 1: ë‹¨ê³„ì  ë°°í¬
```bash
# 1. ìŠ¤í…Œì´ì§• í™˜ê²½ ë°°í¬
./deploy.sh staging uv-test

# 2. ëª¨ë‹ˆí„°ë§ (24ì‹œê°„)
# - ì„±ëŠ¥ ë©”íŠ¸ë¦­ í™•ì¸
# - ì˜¤ë¥˜ ë¡œê·¸ ëª¨ë‹ˆí„°ë§
# - ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì²´í¬

# 3. ì¹´ë‚˜ë¦¬ ë°°í¬ (10%)
kubectl set image deployment/api api=api:uv-version -n production
kubectl scale deployment/api --replicas=1 -n production

# 4. ì ì§„ì  í™•ëŒ€
# 25% â†’ 50% â†’ 100%
```

#### Step 2: ë¡¤ë°± ê³„íš
```bash
#!/bin/bash
# rollback.sh

echo "Rolling back to pip version..."

# 1. ì´ì „ ì´ë¯¸ì§€ë¡œ ë³µì›
kubectl set image deployment/api api=api:pip-version -n production

# 2. ê°€ìƒí™˜ê²½ ë³µì› (ê°œë°œ í™˜ê²½)
rm -rf .venv
mv .venv_pip_backup .venv

# 3. CI/CD ì„¤ì • ë³µì›
git revert --no-commit HEAD~3..HEAD
git commit -m "Rollback to pip"
```

## ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê°œë°œ í™˜ê²½
- [ ] ëª¨ë“  íŒ¨í‚¤ì§€ ì •ìƒ ì„¤ì¹˜
- [ ] í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ê°œë°œ ì„œë²„ ì •ìƒ ì‘ë™
- [ ] IDE í†µí•© í™•ì¸

### CI/CD
- [ ] ë¹Œë“œ ì‹œê°„ ë‹¨ì¶• í™•ì¸
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ì•„í‹°íŒ©íŠ¸ ìƒì„± ì •ìƒ

### í”„ë¡œë•ì…˜
- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê°œì„ 
- [ ] ì˜¤ë¥˜ìœ¨ ë³€í™” ì—†ìŒ
- [ ] ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì •ìƒ

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ: "Package not found"
```bash
# í•´ê²° 1: ì¸ë±ìŠ¤ URL í™•ì¸
uv pip install --index-url https://pypi.org/simple package

# í•´ê²° 2: í”„ë¦¬ë¦´ë¦¬ì¦ˆ í¬í•¨
uv pip install --pre package
```

### ë¬¸ì œ: ì»´íŒŒì¼ ì˜¤ë¥˜
```bash
# í•´ê²° 1: ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
apt-get install build-essential python3-dev

# í•´ê²° 2: ë°”ì´ë„ˆë¦¬ íŒ¨í‚¤ì§€ ì‚¬ìš©
uv pip install --only-binary :all: package
```

### ë¬¸ì œ: ì˜ì¡´ì„± ì¶©ëŒ
```bash
# í•´ê²°: ëª…ì‹œì  ë²„ì „ ì§€ì •
uv pip install "package1==1.0.0" "package2==2.0.0"
```

## ì„±ê³µ ì§€í‘œ

| ì§€í‘œ | ëª©í‘œ | ì¸¡ì • ë°©ë²• |
|------|------|-----------|
| ì„¤ì¹˜ ì‹œê°„ | 80% ê°ì†Œ | CI/CD ë¡œê·¸ |
| ìºì‹œ íš¨ìœ¨ | 90% íˆíŠ¸ìœ¨ | uv í†µê³„ |
| ê°œë°œì ë§Œì¡±ë„ | 4.5/5 | ì„¤ë¬¸ì¡°ì‚¬ |
| ì˜¤ë¥˜ìœ¨ | < 0.1% | ëª¨ë‹ˆí„°ë§ |

## ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ í›„

### 1. ë¬¸ì„œ ì—…ë°ì´íŠ¸
- README.mdì˜ ì„¤ì¹˜ ê°€ì´ë“œ
- ê¸°ì—¬ ê°€ì´ë“œ
- ì˜¨ë³´ë”© ë¬¸ì„œ

### 2. íŒ€ êµìœ¡
- uv ê¸°ë³¸ ì‚¬ìš©ë²• ì„¸ì…˜
- íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì›Œí¬ìƒµ
- Q&A ì„¸ì…˜

### 3. ì§€ì†ì  ê°œì„ 
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- í”¼ë“œë°± ìˆ˜ì§‘
- í”„ë¡œì„¸ìŠ¤ ìµœì í™”
```

#### SubTask 1.3.3: FAQ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ë¬¸ì„œ
**ë‹´ë‹¹ì**: ì§€ì› ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 1ì‹œê°„

```markdown
# docs/uv-faq-troubleshooting.md

# uv FAQ ë° íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

## ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)

### ì¼ë°˜ ì§ˆë¬¸

**Q: uvê°€ pipë³´ë‹¤ ë¹ ë¥¸ ì´ìœ ëŠ”?**
A: uvëŠ” Rustë¡œ ì‘ì„±ë˜ì–´ ìˆê³ , íš¨ìœ¨ì ì¸ ì˜ì¡´ì„± í•´ê²° ì•Œê³ ë¦¬ì¦˜ê³¼ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë˜í•œ ë” ë‚˜ì€ ìºì‹± ë©”ì»¤ë‹ˆì¦˜ì„ ì œê³µí•©ë‹ˆë‹¤.

**Q: pipì™€ uvë¥¼ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?**
A: ê¸°ìˆ ì ìœ¼ë¡œ ê°€ëŠ¥í•˜ì§€ë§Œ ê¶Œì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ë‚˜ì˜ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì¼ê´€ë˜ê²Œ í•˜ë‚˜ì˜ ë„êµ¬ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

**Q: uvê°€ ì§€ì›í•˜ëŠ” Python ë²„ì „ì€?**
A: Python 3.8 ì´ìƒì„ ì§€ì›í•©ë‹ˆë‹¤. ìµœì ì˜ ì„±ëŠ¥ì„ ìœ„í•´ Python 3.10+ ê¶Œì¥í•©ë‹ˆë‹¤.

### ì„¤ì¹˜ ê´€ë ¨

**Q: ê¸°ì—… í”„ë¡ì‹œ í™˜ê²½ì—ì„œ uv ì„¤ì¹˜ê°€ ì•ˆ ë©ë‹ˆë‹¤**
A:
```bash
# í”„ë¡ì‹œ ì„¤ì •
export HTTP_PROXY=http://proxy.company.com:8080
export HTTPS_PROXY=http://proxy.company.com:8080

# ì„¤ì¹˜ ì¬ì‹œë„
curl -LsSf https://astral.sh/uv/install.sh | sh
```

**Q: Windowsì—ì„œ "ëª…ë ¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ" ì˜¤ë¥˜**
A:
```powershell
# PATHì— ì¶”ê°€
$env:Path += ";$env:USERPROFILE\.cargo\bin"

# ì˜êµ¬ì ìœ¼ë¡œ ì¶”ê°€
[Environment]::SetEnvironmentVariable("Path", $env:Path + ";$env:USERPROFILE\.cargo\bin", [EnvironmentVariableTarget]::User)
```

### íŒ¨í‚¤ì§€ ì„¤ì¹˜

**Q: íŠ¹ì • íŒ¨í‚¤ì§€ë§Œ uvë¡œ ì„¤ì¹˜ê°€ ì•ˆ ë©ë‹ˆë‹¤**
A: ëª‡ ê°€ì§€ í•´ê²° ë°©ë²•ì„ ì‹œë„í•˜ì„¸ìš”:
```bash
# 1. í”„ë¦¬ë¦´ë¦¬ì¦ˆ ë²„ì „ í—ˆìš©
uv pip install --pre package

# 2. íŠ¹ì • ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜
uv pip install --index-url https://pypi.org/simple package

# 3. ì˜ì¡´ì„± ë¬´ì‹œ (ì£¼ì˜ í•„ìš”)
uv pip install --no-deps package
```

**Q: "No matching distribution" ì˜¤ë¥˜**
A:
```bash
# Python ë²„ì „ í™•ì¸
python --version

# í˜¸í™˜ ë²„ì „ ê²€ìƒ‰
uv pip install package --dry-run --verbose

# ë‹¤ë¥¸ ë²„ì „ ì‹œë„
uv pip install "package<2.0" ë˜ëŠ” "package>=1.0,<2.0"
```

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ğŸ”´ ì‹¬ê°í•œ ë¬¸ì œ

#### 1. uvê°€ ì „í˜€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ

**ì¦ìƒ**: `uv: command not found`

**í•´ê²°**:
```bash
# PATH í™•ì¸
echo $PATH | grep -q .cargo/bin || echo "PATH not set"

# ìˆ˜ë™ìœ¼ë¡œ PATH ì¶”ê°€
export PATH="$HOME/.cargo/bin:$PATH"

# shell ì„¤ì • íŒŒì¼ì— ì¶”ê°€
echo 'export PATH="$HOME/.cargo/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc
```

#### 2. ì„¸ê·¸ë©˜í…Œì´ì…˜ í´íŠ¸

**ì¦ìƒ**: `Segmentation fault (core dumped)`

**í•´ê²°**:
```bash
# uv ì¬ì„¤ì¹˜
rm -rf ~/.cargo/bin/uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# ì‹œìŠ¤í…œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—…ë°ì´íŠ¸
sudo apt-get update && sudo apt-get upgrade

# glibc ë²„ì „ í™•ì¸
ldd --version
```

### ğŸŸ¡ ì¼ë°˜ì ì¸ ë¬¸ì œ

#### 1. ì˜ì¡´ì„± ì¶©ëŒ

**ì¦ìƒ**: `ResolutionImpossible` ì˜¤ë¥˜

**í•´ê²°**:
```bash
# 1. ìƒì„¸ ì •ë³´ í™•ì¸
uv pip install package -vvv

# 2. ì˜ì¡´ì„± íŠ¸ë¦¬ ë¶„ì„
pip install pipdeptree
pipdeptree --packages package

# 3. ìˆ˜ë™ìœ¼ë¡œ ë²„ì „ ì¡°ì •
uv pip install "package1==1.0" "package2>=2.0,<3.0"
```

#### 2. ìºì‹œ ë¬¸ì œ

**ì¦ìƒ**: ì˜¤ë˜ëœ íŒ¨í‚¤ì§€ ë²„ì „ ì„¤ì¹˜

**í•´ê²°**:
```bash
# ìºì‹œ ìœ„ì¹˜ í™•ì¸
ls -la ~/.cache/uv

# íŠ¹ì • íŒ¨í‚¤ì§€ ìºì‹œ ì‚­ì œ
rm -rf ~/.cache/uv/wheels/package*

# ì „ì²´ ìºì‹œ ì‚­ì œ
rm -rf ~/.cache/uv

# ìºì‹œ ë¬´ì‹œí•˜ê³  ì„¤ì¹˜
uv pip install --refresh package
```

#### 3. SSL ì¸ì¦ì„œ ì˜¤ë¥˜

**ì¦ìƒ**: `SSL: CERTIFICATE_VERIFY_FAILED`

**í•´ê²°**:
```bash
# 1. ì‹œìŠ¤í…œ ì¸ì¦ì„œ ì—…ë°ì´íŠ¸
# Ubuntu/Debian
sudo apt-get update && sudo apt-get install ca-certificates

# macOS
brew install ca-certificates

# 2. ê¸°ì—… ì¸ì¦ì„œ ì¶”ê°€
export REQUESTS_CA_BUNDLE=/path/to/company-cert.pem
export SSL_CERT_FILE=/path/to/company-cert.pem

# 3. (ì„ì‹œ) SSL ê²€ì¦ ë¹„í™œì„±í™” (ë³´ì•ˆ ì£¼ì˜!)
export UV_INSECURE=true
```

### ğŸŸ¢ ì„±ëŠ¥ ë¬¸ì œ

#### 1. ì„¤ì¹˜ê°€ ì˜ˆìƒë³´ë‹¤ ëŠë¦¼

**ì›ì¸ ë¶„ì„**:
```bash
# ë„¤íŠ¸ì›Œí¬ ì†ë„ í™•ì¸
curl -o /dev/null http://pypi.org/simple/

# ìƒì„¸ ë¡œê·¸ë¡œ ë³‘ëª© í™•ì¸
uv pip install package -vvv

# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
htop
```

**ìµœì í™”**:
```bash
# ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¡°ì •
export UV_CONCURRENT_DOWNLOADS=10

# ê°€ê¹Œìš´ ë¯¸ëŸ¬ ì‚¬ìš©
uv pip install --index-url https://pypi.kr/simple package
```

#### 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ

**í•´ê²°**:
```bash
# ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
ulimit -v 2097152  # 2GB ì œí•œ

# ì‘ì€ ë°°ì¹˜ë¡œ ì„¤ì¹˜
uv pip install -r requirements1.txt
uv pip install -r requirements2.txt
```

### í”Œë«í¼ë³„ ë¬¸ì œ

#### macOS

**ë¬¸ì œ**: Apple Siliconì—ì„œ íŠ¹ì • íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤íŒ¨

```bash
# Rosettaë¡œ ì‹¤í–‰
arch -x86_64 uv pip install package

# ARM64 ë„¤ì´í‹°ë¸Œ ë¹Œë“œ
export ARCHFLAGS="-arch arm64"
uv pip install package
```

#### Windows

**ë¬¸ì œ**: ê¸´ ê²½ë¡œëª… ì˜¤ë¥˜

```powershell
# ê¸´ ê²½ë¡œ ì§€ì› í™œì„±í™”
New-ItemProperty -Path "HKLM:\SYSTEM\CurrentControlSet\Control\FileSystem" `
    -Name "LongPathsEnabled" -Value 1 -PropertyType DWORD -Force
```

#### Linux

**ë¬¸ì œ**: ê¶Œí•œ ì˜¤ë¥˜

```bash
# ì‚¬ìš©ì ì„¤ì¹˜ ì‚¬ìš©
uv pip install --user package

# ê°€ìƒí™˜ê²½ ì‚¬ìš© (ê¶Œì¥)
uv venv
source .venv/bin/activate
uv pip install package
```

## ë””ë²„ê¹… ë„êµ¬

### 1. ìƒì„¸ ë¡œê¹…
```bash
# ìµœëŒ€ ìƒì„¸ ë ˆë²¨
uv pip install package -vvv

# ë¡œê·¸ íŒŒì¼ë¡œ ì €ì¥
uv pip install package -vvv 2>&1 | tee install.log
```

### 2. í™˜ê²½ ì •ë³´ ìˆ˜ì§‘
```bash
#!/bin/bash
# debug-info.sh

echo "=== System Info ==="
uname -a
python --version
uv --version

echo -e "\n=== Environment ==="
env | grep -E "PYTHON|UV|PATH|PROXY"

echo -e "\n=== uv Config ==="
ls -la ~/.config/uv/

echo -e "\n=== Cache Info ==="
du -sh ~/.cache/uv/
```

### 3. ì˜ì¡´ì„± ë¶„ì„
```python
# analyze_deps.py
import subprocess
import json

def analyze_package(package):
    # uvë¡œ ì˜ì¡´ì„± ì •ë³´ ìˆ˜ì§‘
    result = subprocess.run(
        ['uv', 'pip', 'show', package],
        capture_output=True,
        text=True
    )
    
    if result.returncode == 0:
        print(f"âœ“ {package} ì •ë³´:")
        print(result.stdout)
    else:
        print(f"âœ— {package} ì˜¤ë¥˜:")
        print(result.stderr)

# ì‚¬ìš©
analyze_package('requests')
```

## ì§€ì› ì±„ë„

### ë‚´ë¶€ ì§€ì›
- Slack: #uv-support
- Wiki: https://wiki.company.com/uv
- í‹°ì¼“: https://helpdesk.company.com

### ì™¸ë¶€ ë¦¬ì†ŒìŠ¤
- GitHub Issues: https://github.com/astral-sh/uv/issues
- Discord: https://discord.gg/astral-sh
- ê³µì‹ ë¬¸ì„œ: https://github.com/astral-sh/uv

## ì‘ê¸‰ ì¡°ì¹˜

### uv ì™„ì „ ì œê±° ë° ì¬ì„¤ì¹˜
```bash
#!/bin/bash
# emergency-reset.sh

echo "âš ï¸  uv ì™„ì „ ì¬ì„¤ì • ì‹œì‘..."

# 1. uv ì œê±°
rm -rf ~/.cargo/bin/uv
rm -rf ~/.cache/uv
rm -rf ~/.config/uv

# 2. pipë¡œ ì„ì‹œ ì „í™˜
python -m venv .venv_temp
source .venv_temp/bin/activate
pip install -r requirements.txt

# 3. uv ì¬ì„¤ì¹˜
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.cargo/env

echo "âœ… ì¬ì„¤ì • ì™„ë£Œ"
```

---

ë¬¸ì œê°€ ì§€ì†ë˜ë©´ #uv-support ì±„ë„ì— ë‹¤ìŒ ì •ë³´ì™€ í•¨ê»˜ ë¬¸ì˜í•˜ì„¸ìš”:
1. ì˜¤ë¥˜ ë©”ì‹œì§€ ì „ë¬¸
2. `uv --version` ì¶œë ¥
3. `python --version` ì¶œë ¥
4. ì‹¤í–‰í•œ ëª…ë ¹ì–´
5. requirements.txt ê´€ë ¨ ë¶€ë¶„
```

---

## ğŸ“‹ Phase 2: ê°œë°œ í™˜ê²½ ì „í™˜ (Day 4-7)

### Task 2.1: ê°œë°œ í™˜ê²½ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

#### SubTask 2.1.1: í”Œë«í¼ë³„ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
**ë‹´ë‹¹ì**: DevOps ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```bash
#!/bin/bash
# scripts/install_uv_multiplatform.sh

set -e

# ìƒ‰ìƒ ì½”ë“œ
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# ë¡œê¹… í•¨ìˆ˜
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# OS ê°ì§€
detect_os() {
    if [[ "$OSTYPE" == "linux-gnu"* ]]; then
        if [ -f /etc/os-release ]; then
            . /etc/os-release
            OS=$ID
            VER=$VERSION_ID
        fi
    elif [[ "$OSTYPE" == "darwin"* ]]; then
        OS="macos"
        VER=$(sw_vers -productVersion)
    elif [[ "$OSTYPE" == "msys" ]] || [[ "$OSTYPE" == "cygwin" ]]; then
        OS="windows"
        VER=$(cmd //c ver 2>&1 | grep -oE '[0-9]+\.[0-9]+\.[0-9]+')
    else
        log_error "Unsupported OS: $OSTYPE"
        exit 1
    fi
    
    log_info "Detected OS: $OS $VER"
}

# ì•„í‚¤í…ì²˜ ê°ì§€
detect_arch() {
    ARCH=$(uname -m)
    case $ARCH in
        x86_64)
            ARCH="x86_64"
            ;;
        aarch64|arm64)
            ARCH="aarch64"
            ;;
        *)
            log_error "Unsupported architecture: $ARCH"
            exit 1
            ;;
    esac
    
    log_info "Detected architecture: $ARCH"
}

# Linux ì„¤ì¹˜
install_linux() {
    log_info "Installing uv for Linux..."
    
    # í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
    if command -v apt-get &> /dev/null; then
        log_info "Installing dependencies via apt..."
        sudo apt-get update
        sudo apt-get install -y curl ca-certificates
    elif command -v yum &> /dev/null; then
        log_info "Installing dependencies via yum..."
        sudo yum install -y curl ca-certificates
    fi
    
    # uv ì„¤ì¹˜
    curl -LsSf https://astral.sh/uv/install.sh | sh
    
    # PATH ì„¤ì •
    add_to_path_linux
}

# macOS ì„¤ì¹˜
install_macos() {
    log_info "Installing uv for macOS..."
    
    # Homebrew í™•ì¸
    if command -v brew &> /dev/null; then
        log_info "Installing via Homebrew..."
        brew install uv
    else
        log_info "Installing via install script..."
        curl -LsSf https://astral.sh/uv/install.sh | sh
        add_to_path_macos
    fi
    
    # Apple Silicon íŠ¹ë³„ ì²˜ë¦¬
    if [[ $ARCH == "aarch64" ]]; then
        log_warn "Apple Silicon detected. Some packages may require Rosetta."
        log_info "To install Rosetta: softwareupdate --install-rosetta"
    fi
}

# Windows ì„¤ì¹˜
install_windows() {
    log_info "Installing uv for Windows..."
    
    # PowerShell ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    cat > install_uv_windows.ps1 << 'EOF'
# Requires -RunAsAdministrator

Write-Host "Installing uv for Windows..." -ForegroundColor Green

# ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰
irm https://astral.sh/uv/install.ps1 | iex

# PATH ì—…ë°ì´íŠ¸
$uvPath = "$env:USERPROFILE\.cargo\bin"
$currentPath = [Environment]::GetEnvironmentVariable("Path", "User")

if ($currentPath -notlike "*$uvPath*") {
    [Environment]::SetEnvironmentVariable(
        "Path",
        "$currentPath;$uvPath",
        "User"
    )
    Write-Host "Added uv to PATH" -ForegroundColor Green
}

# ì„¤ì¹˜ í™•ì¸
$env:Path = [System.Environment]::GetEnvironmentVariable("Path","User")
uv --version

Write-Host "Installation complete!" -ForegroundColor Green
EOF

    log_info "Please run the following command in PowerShell as Administrator:"
    echo "powershell -ExecutionPolicy Bypass -File install_uv_windows.ps1"
}

# PATH ì¶”ê°€ - Linux
add_to_path_linux() {
    local shell_rc=""
    
    if [[ $SHELL == *"bash"* ]]; then
        shell_rc="$HOME/.bashrc"
    elif [[ $SHELL == *"zsh"* ]]; then
        shell_rc="$HOME/.zshrc"
    elif [[ $SHELL == *"fish"* ]]; then
        shell_rc="$HOME/.config/fish/config.fish"
    fi
    
    if [ -n "$shell_rc" ]; then
        echo 'export PATH="$HOME/.cargo/bin:$PATH"' >> "$shell_rc"
        log_info "Added uv to PATH in $shell_rc"
        log_warn "Please run: source $shell_rc"
    fi
}

# PATH ì¶”ê°€ - macOS
add_to_path_macos() {
    # macOSëŠ” ê¸°ë³¸ì ìœ¼ë¡œ zsh ì‚¬ìš©
    local shell_rc="$HOME/.zshrc"
    
    if [ -f "$HOME/.bash_profile" ]; then
        shell_rc="$HOME/.bash_profile"
    fi
    
    echo 'export PATH="$HOME/.cargo/bin:$PATH"' >> "$shell_rc"
    log_info "Added uv to PATH in $shell_rc"
    log_warn "Please run: source $shell_rc"
}

# ì„¤ì¹˜ í™•ì¸
verify_installation() {
    log_info "Verifying installation..."
    
    # PATH ìƒˆë¡œê³ ì¹¨
    export PATH="$HOME/.cargo/bin:$PATH"
    
    if command -v uv &> /dev/null; then
        local version=$(uv --version)
        log_info "âœ… uv installed successfully: $version"
        return 0
    else
        log_error "âŒ uv installation failed"
        return 1
    fi
}

# í™˜ê²½ ì„¤ì •
setup_environment() {
    log_info "Setting up uv environment..."
    
    # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    mkdir -p "$HOME/.cache/uv"
    
    # ì„¤ì • íŒŒì¼ ìƒì„±
    mkdir -p "$HOME/.config/uv"
    cat > "$HOME/.config/uv/config.toml" << EOF
# uv configuration

[cache]
dir = "$HOME/.cache/uv"

[pip]
# ê¸°ë³¸ ì¸ë±ìŠ¤ URL
index-url = "https://pypi.org/simple"

# ì¶”ê°€ ì¸ë±ìŠ¤ (í•„ìš” ì‹œ)
# extra-index-url = ["https://pypi.company.com/simple"]

[install]
# ì»´íŒŒì¼ ì˜µì…˜
compile-bytecode = true
EOF

    log_info "Configuration saved to ~/.config/uv/config.toml"
}

# ë©”ì¸ í•¨ìˆ˜
main() {
    echo "ğŸš€ uv Installation Script"
    echo "========================"
    
    # OS ë° ì•„í‚¤í…ì²˜ ê°ì§€
    detect_os
    detect_arch
    
    # OSë³„ ì„¤ì¹˜
    case $OS in
        ubuntu|debian)
            install_linux
            ;;
        fedora|centos|rhel)
            install_linux
            ;;
        macos)
            install_macos
            ;;
        windows)
            install_windows
            ;;
        *)
            log_error "Unsupported OS: $OS"
            exit 1
            ;;
    esac
    
    # Windowsê°€ ì•„ë‹Œ ê²½ìš° ê²€ì¦ ë° ì„¤ì •
    if [[ $OS != "windows" ]]; then
        if verify_installation; then
            setup_environment
            
            echo ""
            echo "âœ… Installation complete!"
            echo ""
            echo "Next steps:"
            echo "1. Restart your terminal or run: source ~/.bashrc (or ~/.zshrc)"
            echo "2. Verify installation: uv --version"
            echo "3. Create virtual environment: uv venv"
            echo "4. Install packages: uv pip install -r requirements.txt"
        else
            echo ""
            echo "âŒ Installation failed. Please check the errors above."
            exit 1
        fi
    fi
}

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main "$@"
```

#### SubTask 2.1.2: ê°€ìƒí™˜ê²½ ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 3ì‹œê°„

```python
#!/usr/bin/env python3
# scripts/migrate_venv.py

import os
import sys
import json
import shutil
import subprocess
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Optional

class VenvMigrator:
    """ê°€ìƒí™˜ê²½ì„ pipì—ì„œ uvë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
    
    def __init__(self, venv_path: str, backup: bool = True):
        self.venv_path = Path(venv_path)
        self.backup = backup
        self.backup_path = None
        self.report = {
            'start_time': datetime.now().isoformat(),
            'original_venv': str(self.venv_path),
            'packages': [],
            'issues': [],
            'success': False
        }
    
    def check_prerequisites(self) -> bool:
        """ì‚¬ì „ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
        print("ğŸ” Checking prerequisites...")
        
        # uv ì„¤ì¹˜ í™•ì¸
        if not shutil.which('uv'):
            self.report['issues'].append({
                'type': 'missing_uv',
                'message': 'uv is not installed'
            })
            print("âŒ uv is not installed. Please install it first.")
            return False
        
        # ê°€ìƒí™˜ê²½ í™•ì¸
        if not self.venv_path.exists():
            self.report['issues'].append({
                'type': 'missing_venv',
                'message': f'Virtual environment not found: {self.venv_path}'
            })
            print(f"âŒ Virtual environment not found: {self.venv_path}")
            return False
        
        # Python ì‹¤í–‰ íŒŒì¼ í™•ì¸
        python_exe = self._get_venv_python()
        if not python_exe.exists():
            self.report['issues'].append({
                'type': 'invalid_venv',
                'message': 'Invalid virtual environment structure'
            })
            print("âŒ Invalid virtual environment structure")
            return False
        
        print("âœ… Prerequisites check passed")
        return True
    
    def _get_venv_python(self) -> Path:
        """ê°€ìƒí™˜ê²½ Python ê²½ë¡œ ë°˜í™˜"""
        if sys.platform == 'win32':
            return self.venv_path / 'Scripts' / 'python.exe'
        return self.venv_path / 'bin' / 'python'
    
    def backup_venv(self) -> bool:
        """ê¸°ì¡´ ê°€ìƒí™˜ê²½ ë°±ì—…"""
        if not self.backup:
            return True
        
        print("ğŸ’¾ Backing up virtual environment...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.backup_path = Path(f"{self.venv_path}_backup_{timestamp}")
        
        try:
            shutil.copytree(self.venv_path, self.backup_path)
            self.report['backup_path'] = str(self.backup_path)
            print(f"âœ… Backup created: {self.backup_path}")
            return True
        except Exception as e:
            self.report['issues'].append({
                'type': 'backup_failed',
                'message': str(e)
            })
            print(f"âŒ Backup failed: {e}")
            return False
    
    def extract_packages(self) -> List[Dict[str, str]]:
        """í˜„ì¬ ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ ëª©ë¡ ì¶”ì¶œ"""
        print("ğŸ“¦ Extracting installed packages...")
        
        python_exe = self._get_venv_python()
        
        try:
            # pip list --format=json ì‹¤í–‰
            result = subprocess.run(
                [str(python_exe), '-m', 'pip', 'list', '--format=json'],
                capture_output=True,
                text=True,
                check=True
            )
            
            packages = json.loads(result.stdout)
            
            # íŒ¨í‚¤ì§€ ì •ë³´ ê°•í™”
            enhanced_packages = []
            for pkg in packages:
                # pip showë¡œ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                show_result = subprocess.run(
                    [str(python_exe), '-m', 'pip', 'show', pkg['name']],
                    capture_output=True,
                    text=True
                )
                
                # Requires ì •ë³´ íŒŒì‹±
                requires = []
                if show_result.returncode == 0:
                    for line in show_result.stdout.split('\n'):
                        if line.startswith('Requires:'):
                            requires_str = line.split(':', 1)[1].strip()
                            if requires_str:
                                requires = [r.strip() for r in requires_str.split(',')]
                
                enhanced_packages.append({
                    'name': pkg['name'],
                    'version': pkg['version'],
                    'requires': requires
                })
            
            self.report['packages'] = enhanced_packages
            print(f"âœ… Found {len(enhanced_packages)} packages")
            return enhanced_packages
            
        except Exception as e:
            self.report['issues'].append({
                'type': 'package_extraction_failed',
                'message': str(e)
            })
            print(f"âŒ Failed to extract packages: {e}")
            return []
    
    def create_requirements_file(self, packages: List[Dict[str, str]]) -> Path:
        """requirements.txt íŒŒì¼ ìƒì„±"""
        print("ğŸ“ Creating requirements file...")
        
        req_file = Path('requirements_migration.txt')
        
        with open(req_file, 'w') as f:
            for pkg in packages:
                # ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì œì™¸
                if pkg['name'] in ['pip', 'setuptools', 'wheel']:
                    continue
                f.write(f"{pkg['name']}=={pkg['version']}\n")
        
        print(f"âœ… Requirements file created: {req_file}")
        return req_file
    
    def create_new_venv(self) -> bool:
        """uvë¡œ ìƒˆ ê°€ìƒí™˜ê²½ ìƒì„±"""
        print("ğŸ”¨ Creating new virtual environment with uv...")
        
        # ì„ì‹œ ì´ë¦„ìœ¼ë¡œ ìƒì„±
        temp_venv = Path(f"{self.venv_path}_uv_temp")
        
        try:
            # ê¸°ì¡´ ì„ì‹œ í™˜ê²½ ì œê±°
            if temp_venv.exists():
                shutil.rmtree(temp_venv)
            
            # uv venv ì‹¤í–‰
            result = subprocess.run(
                ['uv', 'venv', str(temp_venv)],
                capture_output=True,
                text=True,
                check=True
            )
            
            self.report['new_venv'] = str(temp_venv)
            print(f"âœ… New virtual environment created: {temp_venv}")
            return True
            
        except subprocess.CalledProcessError as e:
            self.report['issues'].append({
                'type': 'venv_creation_failed',
                'message': e.stderr
            })
            print(f"âŒ Failed to create virtual environment: {e.stderr}")
            return False
    
    def install_packages(self, req_file: Path) -> Tuple[List[str], List[str]]:
        """uvë¡œ íŒ¨í‚¤ì§€ ì„¤ì¹˜"""
        print("ğŸ“¥ Installing packages with uv...")
        
        temp_venv = Path(self.report.get('new_venv', ''))
        if not temp_venv.exists():
            return [], []
        
        # ê°€ìƒí™˜ê²½ í™œì„±í™” ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
        if sys.platform == 'win32':
            activate = temp_venv / 'Scripts' / 'activate.bat'
            python_exe = temp_venv / 'Scripts' / 'python.exe'
        else:
            activate = temp_venv / 'bin' / 'activate'
            python_exe = temp_venv / 'bin' / 'python'
        
        success_packages = []
        failed_packages = []
        
        # ì „ì²´ ì„¤ì¹˜ ì‹œë„
        try:
            # Windowsì™€ Unix ê³„ì—´ êµ¬ë¶„
            if sys.platform == 'win32':
                cmd = f'"{activate}" && uv pip install -r {req_file}'
                shell = True
            else:
                cmd = f'source "{activate}" && uv pip install -r {req_file}'
                shell = True
            
            result = subprocess.run(
                cmd,
                shell=shell,
                capture_output=True,
                text=True,
                executable='/bin/bash' if sys.platform != 'win32' else None
            )
            
            if result.returncode == 0:
                # ëª¨ë“  íŒ¨í‚¤ì§€ ì„±ê³µ
                with open(req_file, 'r') as f:
                    success_packages = [
                        line.strip() for line in f 
                        if line.strip() and not line.startswith('#')
                    ]
                print(f"âœ… All packages installed successfully")
            else:
                # ì‹¤íŒ¨í•œ ê²½ìš° ê°œë³„ ì„¤ì¹˜ ì‹œë„
                print("âš ï¸  Bulk installation failed. Trying individual packages...")
                success_packages, failed_packages = self._install_individually(
                    req_file, activate
                )
                
        except Exception as e:
            self.report['issues'].append({
                'type': 'installation_error',
                'message': str(e)
            })
            print(f"âŒ Installation error: {e}")
        
        return success_packages, failed_packages
    
    def _install_individually(
        self, 
        req_file: Path, 
        activate: Path
    ) -> Tuple[List[str], List[str]]:
        """íŒ¨í‚¤ì§€ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì„¤ì¹˜"""
        success = []
        failed = []
        
        with open(req_file, 'r') as f:
            packages = [
                line.strip() for line in f 
                if line.strip() and not line.startswith('#')
            ]
        
        for i, package in enumerate(packages, 1):
            print(f"  [{i}/{len(packages)}] Installing {package}...")
            
            if sys.platform == 'win32':
                cmd = f'"{activate}" && uv pip install {package}'
            else:
                cmd = f'source "{activate}" && uv pip install {package}'
            
            result = subprocess.run(
                cmd,
                shell=True,
                capture_output=True,
                text=True,
                executable='/bin/bash' if sys.platform != 'win32' else None
            )
            
            if result.returncode == 0:
                success.append(package)
                print(f"    âœ… Success")
            else:
                failed.append({
                    'package': package,
                    'error': result.stderr
                })
                print(f"    âŒ Failed: {result.stderr.split('error:')[-1].strip()}")
                
                # ëŒ€ì²´ ë°©ë²• ì‹œë„
                alt_success = self._try_alternative_install(package, activate)
                if alt_success:
                    success.append(package)
                    failed = [f for f in failed if f['package'] != package]
        
        return success, failed
    
    def _try_alternative_install(self, package: str, activate: Path) -> bool:
        """ëŒ€ì²´ ì„¤ì¹˜ ë°©ë²• ì‹œë„"""
        alternatives = [
            '--pre',  # í”„ë¦¬ë¦´ë¦¬ì¦ˆ í—ˆìš©
            '--no-deps',  # ì˜ì¡´ì„± ë¬´ì‹œ
        ]
        
        for alt in alternatives:
            if sys.platform == 'win32':
                cmd = f'"{activate}" && uv pip install {alt} {package}'
            else:
                cmd = f'source "{activate}" && uv pip install {alt} {package}'
            
            result = subprocess.run(
                cmd,
                shell=True,
                capture_output=True,
                text=True,
                executable='/bin/bash' if sys.platform != 'win32' else None
            )
            
            if result.returncode == 0:
                print(f"    âœ… Success with {alt}")
                return True
        
        return False
    
    def replace_venv(self) -> bool:
        """ê¸°ì¡´ ê°€ìƒí™˜ê²½ì„ ìƒˆ í™˜ê²½ìœ¼ë¡œ êµì²´"""
        print("ğŸ”„ Replacing old virtual environment...")
        
        temp_venv = Path(self.report.get('new_venv', ''))
        if not temp_venv.exists():
            return False
        
        try:
            # ê¸°ì¡´ í™˜ê²½ ì œê±°
            if self.venv_path.exists():
                shutil.rmtree(self.venv_path)
            
            # ìƒˆ í™˜ê²½ìœ¼ë¡œ ì´ë™
            shutil.move(str(temp_venv), str(self.venv_path))
            
            print(f"âœ… Virtual environment replaced successfully")
            return True
            
        except Exception as e:
            self.report['issues'].append({
                'type': 'replacement_failed',
                'message': str(e)
            })
            print(f"âŒ Failed to replace virtual environment: {e}")
            return False
    
    def verify_migration(self) -> bool:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦"""
        print("ğŸ” Verifying migration...")
        
        python_exe = self._get_venv_python()
        
        # Python ë²„ì „ í™•ì¸
        try:
            result = subprocess.run(
                [str(python_exe), '--version'],
                capture_output=True,
                text=True,
                check=True
            )
            print(f"  Python version: {result.stdout.strip()}")
            
            # íŒ¨í‚¤ì§€ ìˆ˜ í™•ì¸
            result = subprocess.run(
                [str(python_exe), '-m', 'pip', 'list', '--format=json'],
                capture_output=True,
                text=True,
                check=True
            )
            
            packages = json.loads(result.stdout)
            print(f"  Installed packages: {len(packages)}")
            
            # uv ì‚¬ìš© í™•ì¸
            cfg_file = self.venv_path / 'pyvenv.cfg'
            if cfg_file.exists():
                with open(cfg_file, 'r') as f:
                    content = f.read()
                    if 'uv' in content.lower():
                        print("  âœ… Virtual environment created with uv")
                    else:
                        print("  âš ï¸  Virtual environment might not be created with uv")
            
            return True
            
        except Exception as e:
            self.report['issues'].append({
                'type': 'verification_failed',
                'message': str(e)
            })
            print(f"âŒ Verification failed: {e}")
            return False
    
    def generate_report(self) -> None:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ë³´ê³ ì„œ ìƒì„±"""
        self.report['end_time'] = datetime.now().isoformat()
        
        report_file = Path('venv_migration_report.json')
        with open(report_file, 'w') as f:
            json.dump(self.report, f, indent=2)
        
        print(f"\nğŸ“Š Migration report saved to: {report_file}")
        
        # ìš”ì•½ ì¶œë ¥
        print("\n=== Migration Summary ===")
        print(f"Original venv: {self.report['original_venv']}")
        print(f"Backup: {self.report.get('backup_path', 'N/A')}")
        print(f"Total packages: {len(self.report['packages'])}")
        print(f"Issues: {len(self.report['issues'])}")
        print(f"Success: {'âœ… Yes' if self.report['success'] else 'âŒ No'}")
    
    def rollback(self) -> bool:
        """ë°±ì—…ì—ì„œ ë³µì›"""
        if not self.backup_path or not self.backup_path.exists():
            print("âŒ No backup available for rollback")
            return False
        
        print("âª Rolling back to original virtual environment...")
        
        try:
            # í˜„ì¬ í™˜ê²½ ì œê±°
            if self.venv_path.exists():
                shutil.rmtree(self.venv_path)
            
            # ë°±ì—… ë³µì›
            shutil.copytree(self.backup_path, self.venv_path)
            
            print("âœ… Rollback completed successfully")
            return True
            
        except Exception as e:
            print(f"âŒ Rollback failed: {e}")
            return False
    
    def migrate(self) -> bool:
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸš€ Starting virtual environment migration to uv...\n")
        
        # 1. ì‚¬ì „ í™•ì¸
        if not self.check_prerequisites():
            return False
        
        # 2. ë°±ì—…
        if self.backup and not self.backup_venv():
            return False
        
        # 3. íŒ¨í‚¤ì§€ ì¶”ì¶œ
        packages = self.extract_packages()
        if not packages:
            return False
        
        # 4. Requirements íŒŒì¼ ìƒì„±
        req_file = self.create_requirements_file(packages)
        
        # 5. ìƒˆ ê°€ìƒí™˜ê²½ ìƒì„±
        if not self.create_new_venv():
            if self.backup_path:
                self.rollback()
            return False
        
        # 6. íŒ¨í‚¤ì§€ ì„¤ì¹˜
        success, failed = self.install_packages(req_file)
        
        if failed:
            print(f"\nâš ï¸  {len(failed)} packages failed to install:")
            for f in failed:
                if isinstance(f, dict):
                    print(f"  - {f['package']}")
                else:
                    print(f"  - {f}")
            
            # ì‚¬ìš©ì í™•ì¸
            response = input("\nContinue with migration? (y/N): ")
            if response.lower() != 'y':
                if self.backup_path:
                    self.rollback()
                return False
        
        # 7. ê°€ìƒí™˜ê²½ êµì²´
        if not self.replace_venv():
            if self.backup_path:
                self.rollback()
            return False
        
        # 8. ê²€ì¦
        self.report['success'] = self.verify_migration()
        
        # 9. ë³´ê³ ì„œ ìƒì„±
        self.generate_report()
        
        # 10. ì •ë¦¬
        req_file.unlink()
        
        return self.report['success']


def main():
    parser = argparse.ArgumentParser(
        description='Migrate pip virtual environment to uv'
    )
    parser.add_argument(
        'venv_path',
        help='Path to virtual environment'
    )
    parser.add_argument(
        '--no-backup',
        action='store_true',
        help='Skip backup creation'
    )
    parser.add_argument(
        '--rollback',
        action='store_true',
        help='Rollback to backup'
    )
    
    args = parser.parse_args()
    
    migrator = VenvMigrator(
        args.venv_path,
        backup=not args.no_backup
    )
    
    if args.rollback:
        # ë°±ì—…ì—ì„œ ë³µì›
        success = migrator.rollback()
    else:
        # ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
        success = migrator.migrate()
    
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
```

#### SubTask 2.1.3: IDE í†µí•© ì„¤ì •
**ë‹´ë‹¹ì**: ê°œë°œ ë„êµ¬ ì „ë¬¸ê°€  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 1ì‹œê°„

```python
# scripts/setup_ide_integration.py
#!/usr/bin/env python3

import json
import os
from pathlib import Path
from typing import Dict, Any

class IDEIntegrator:
    """ì£¼ìš” IDEì— uv í†µí•© ì„¤ì •"""
    
    def __init__(self):
        self.project_root = Path.cwd()
        
    def setup_vscode(self):
        """VS Code ì„¤ì •"""
        print("ğŸ”§ Setting up VS Code integration...")
        
        vscode_dir = self.project_root / '.vscode'
        vscode_dir.mkdir(exist_ok=True)
        
        # settings.json
        settings = {
            "python.defaultInterpreterPath": "${workspaceFolder}/.venv/bin/python",
            "python.terminal.activateEnvironment": True,
            "python.terminal.activateEnvInCurrentTerminal": True,
            "python.linting.enabled": True,
            "python.linting.pylintEnabled": False,
            "python.linting.flake8Enabled": True,
            "python.formatting.provider": "black",
            "python.testing.pytestEnabled": True,
            "python.testing.unittestEnabled": False,
            
            # uv ê´€ë ¨ ì„¤ì •
            "terminal.integrated.env.linux": {
                "PATH": "${env:HOME}/.cargo/bin:${env:PATH}"
            },
            "terminal.integrated.env.osx": {
                "PATH": "${env:HOME}/.cargo/bin:${env:PATH}"
            },
            "terminal.integrated.env.windows": {
                "PATH": "${env:USERPROFILE}\\.cargo\\bin;${env:PATH}"
            },
            
            # ì»¤ìŠ¤í…€ íƒœìŠ¤í¬
            "python.terminal.executeInFileDir": True,
            
            # íŒŒì¼ ì œì™¸
            "files.exclude": {
                "**/__pycache__": True,
                "**/*.pyc": True,
                ".venv": False,  # ê°€ìƒí™˜ê²½ì€ í‘œì‹œ
                ".venv_pip_backup*": True
            }
        }
        
        settings_file = vscode_dir / 'settings.json'
        if settings_file.exists():
            with open(settings_file, 'r') as f:
                existing = json.load(f)
            existing.update(settings)
            settings = existing
        
        with open(settings_file, 'w') as f:
            json.dump(settings, f, indent=2)
        
        # tasks.json
        tasks = {
            "version": "2.0.0",
            "tasks": [
                {
                    "label": "uv: Install dependencies",
                    "type": "shell",
                    "command": "uv pip install -r requirements.txt",
                    "group": {
                        "kind": "build",
                        "isDefault": True
                    },
                    "problemMatcher": []
                },
                {
                    "label": "uv: Install dev dependencies",
                    "type": "shell",
                    "command": "uv pip install -r requirements-dev.txt",
                    "group": "build",
                    "problemMatcher": []
                },
                {
                    "label": "uv: Update dependencies",
                    "type": "shell",
                    "command": "uv pip install --upgrade -r requirements.txt",
                    "group": "build",
                    "problemMatcher": []
                },
                {
                    "label": "uv: Create virtual environment",
                    "type": "shell",
                    "command": "uv venv",
                    "group": "build",
                    "problemMatcher": []
                }
            ]
        }
        
        with open(vscode_dir / 'tasks.json', 'w') as f:
            json.dump(tasks, f, indent=2)
        
        # launch.json
        launch = {
            "version": "0.2.0",
            "configurations": [
                {
                    "name": "Python: Current File",
                    "type": "python",
                    "request": "launch",
                    "program": "${file}",
                    "console": "integratedTerminal",
                    "justMyCode": True,
                    "env": {
                        "PYTHONPATH": "${workspaceFolder}"
                    }
                },
                {
                    "name": "Python: FastAPI",
                    "type": "python",
                    "request": "launch",
                    "module": "uvicorn",
                    "args": [
                        "app.main:app",
                        "--reload",
                        "--host",
                        "0.0.0.0",
                        "--port",
                        "8000"
                    ],
                    "jinja": True,
                    "justMyCode": True,
                    "env": {
                        "PYTHONPATH": "${workspaceFolder}"
                    }
                }
            ]
        }
        
        with open(vscode_dir / 'launch.json', 'w') as f:
            json.dump(launch, f, indent=2)
        
        # extensions.json
        extensions = {
            "recommendations": [
                "ms-python.python",
                "ms-python.vscode-pylance",
                "ms-python.black-formatter",
                "charliermarsh.ruff",
                "ms-vscode.makefile-tools"
            ]
        }
        
        with open(vscode_dir / 'extensions.json', 'w') as f:
            json.dump(extensions, f, indent=2)
        
        print("âœ… VS Code configuration complete")
    
    def setup_pycharm(self):
        """PyCharm ì„¤ì •"""
        print("ğŸ”§ Setting up PyCharm integration...")
        
        idea_dir = self.project_root / '.idea'
        idea_dir.mkdir(exist_ok=True)
        
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        env_xml = '''<component name="EnvironmentVariables">
  <envs>
    <env name="PATH" value="$USER_HOME$/.cargo/bin:$PATH$" />
  </envs>
</component>'''
        
        # ì™¸ë¶€ ë„êµ¬ ì„¤ì •
        external_tools = '''<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="ExternalTools">
    <tool name="uv install" description="Install dependencies with uv" showInMainMenu="true" showInEditor="false" showInProject="true" showInSearchPopup="false" disabled="false" useConsole="true" showConsoleOnStdOut="false" showConsoleOnStdErr="false" synchronizeAfterRun="true">
      <exec>
        <option name="COMMAND" value="uv" />
        <option name="PARAMETERS" value="pip install -r requirements.txt" />
        <option name="WORKING_DIRECTORY" value="$ProjectFileDir$" />
      </exec>
    </tool>
    <tool name="uv update" description="Update dependencies with uv" showInMainMenu="true" showInEditor="false" showInProject="true" showInSearchPopup="false" disabled="false" useConsole="true" showConsoleOnStdOut="false" showConsoleOnStdErr="false" synchronizeAfterRun="true">
      <exec>
        <option name="COMMAND" value="uv" />
        <option name="PARAMETERS" value="pip install --upgrade -r requirements.txt" />
        <option name="WORKING_DIRECTORY" value="$ProjectFileDir$" />
      </exec>
    </tool>
  </component>
</project>'''
        
        with open(idea_dir / 'externalTools.xml', 'w') as f:
            f.write(external_tools)
        
        # ì‹¤í–‰ êµ¬ì„±
        run_config = '''<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="RunConfigurationProducerService">
    <option name="ignoredProducers">
      <set>
        <option value="com.jetbrains.python.run.PythonRunConfigurationProducer" />
      </set>
    </option>
  </component>
  <component name="RunManager">
    <configuration name="Run with uv" type="PythonConfigurationType" factoryName="Python">
      <module name="$PROJECT_NAME$" />
      <option name="INTERPRETER_OPTIONS" value="" />
      <option name="PARENT_ENVS" value="true" />
      <envs>
        <env name="PYTHONUNBUFFERED" value="1" />
        <env name="PATH" value="$USER_HOME$/.cargo/bin:$PATH$" />
      </envs>
      <option name="SDK_HOME" value="$PROJECT_DIR$/.venv/bin/python" />
      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$" />
      <option name="IS_MODULE_SDK" value="false" />
      <option name="ADD_CONTENT_ROOTS" value="true" />
      <option name="ADD_SOURCE_ROOTS" value="true" />
      <option name="SCRIPT_NAME" value="" />
      <option name="PARAMETERS" value="" />
      <option name="SHOW_COMMAND_LINE" value="false" />
      <option name="EMULATE_TERMINAL" value="false" />
      <option name="MODULE_MODE" value="false" />
      <option name="REDIRECT_INPUT" value="false" />
      <option name="INPUT_FILE" value="" />
      <method v="2" />
    </configuration>
  </component>
</project>'''
        
        with open(idea_dir / 'runConfigurations.xml', 'w') as f:
            f.write(run_config)
        
        # ì‚¬ìš© ê°€ì´ë“œ ìƒì„±
        guide = '''# PyCharm uv Integration Guide

## Setup Complete! 

### Using uv in PyCharm:

1. **Terminal**: uv is available in the integrated terminal
2. **External Tools**: Tools â†’ External Tools â†’ uv install/update
3. **Python Interpreter**: Settings â†’ Project â†’ Python Interpreter â†’ .venv/bin/python

### Keyboard Shortcuts:
- Install dependencies: Assign shortcut in Settings â†’ Keymap â†’ External Tools
- Update dependencies: Assign shortcut in Settings â†’ Keymap â†’ External Tools

### Tips:
- Use File Watchers to auto-install when requirements.txt changes
- Configure pre-commit hooks in Version Control settings
'''
        
        with open(self.project_root / 'PYCHARM_UV_GUIDE.md', 'w') as f:
            f.write(guide)
        
        print("âœ… PyCharm configuration complete")
        print("ğŸ“– See PYCHARM_UV_GUIDE.md for usage instructions")
    
    def setup_vim(self):
        """Vim/Neovim ì„¤ì •"""
        print("ğŸ”§ Setting up Vim/Neovim integration...")
        
        # .vimrc ë˜ëŠ” init.vim ì„¤ì •
        vim_config = '''
" uv integration for Python development

" ê°€ìƒí™˜ê²½ ìë™ í™œì„±í™”
let g:python3_host_prog = getcwd() . '/.venv/bin/python'

" ALE (Asynchronous Lint Engine) ì„¤ì •
let g:ale_python_auto_virtualenv = 1
let g:ale_virtualenv_dir_names = ['.venv']

" ì»¤ìŠ¤í…€ ëª…ë ¹ì–´
command! UvInstall :!uv pip install -r requirements.txt
command! UvUpdate :!uv pip install --upgrade -r requirements.txt
command! UvFreeze :!uv pip freeze > requirements.lock
command! UvList :!uv pip list

" í‚¤ ë§¤í•‘
nnoremap <leader>ui :UvInstall<CR>
nnoremap <leader>uu :UvUpdate<CR>
nnoremap <leader>uf :UvFreeze<CR>
nnoremap <leader>ul :UvList<CR>

" ìë™ ëª…ë ¹
autocmd BufWritePost requirements*.txt :UvInstall

" ìƒíƒœ í‘œì‹œì¤„ì— ê°€ìƒí™˜ê²½ í‘œì‹œ
set statusline+=%{virtualenv#statusline()}
'''
        
        # í™ˆ ë””ë ‰í† ë¦¬ì— ì €ì¥
        vimrc_path = Path.home() / '.vimrc.uv'
        with open(vimrc_path, 'w') as f:
            f.write(vim_config)
        
        # Neovim Lua ì„¤ì •
        nvim_config = '''
-- uv integration for Neovim

-- ê°€ìƒí™˜ê²½ ì„¤ì •
vim.g.python3_host_prog = vim.fn.getcwd() .. '/.venv/bin/python'

-- ì»¤ìŠ¤í…€ ëª…ë ¹ì–´
vim.api.nvim_create_user_command('UvInstall', '!uv pip install -r requirements.txt', {})
vim.api.nvim_create_user_command('UvUpdate', '!uv pip install --upgrade -r requirements.txt', {})
vim.api.nvim_create_user_command('UvFreeze', '!uv pip freeze > requirements.lock', {})
vim.api.nvim_create_user_command('UvList', '!uv pip list', {})

-- í‚¤ ë§¤í•‘
vim.keymap.set('n', '<leader>ui', ':UvInstall<CR>')
vim.keymap.set('n', '<leader>uu', ':UvUpdate<CR>')
vim.keymap.set('n', '<leader>uf', ':UvFreeze<CR>')
vim.keymap.set('n', '<leader>ul', ':UvList<CR>')

-- ìë™ ëª…ë ¹
vim.api.nvim_create_autocmd("BufWritePost", {
    pattern = "requirements*.txt",
    command = "UvInstall"
})
'''
        
        nvim_config_dir = Path.home() / '.config' / 'nvim'
        nvim_config_dir.mkdir(parents=True, exist_ok=True)
        
        with open(nvim_config_dir / 'uv.lua', 'w') as f:
            f.write(nvim_config)
        
        print("âœ… Vim/Neovim configuration complete")
        print("ğŸ“ Add 'source ~/.vimrc.uv' to your .vimrc")
        print("ğŸ“ Add 'require(\"uv\")' to your init.lua for Neovim")
    
    def generate_editorconfig(self):
        """EditorConfig íŒŒì¼ ìƒì„±"""
        editorconfig = '''# EditorConfig is awesome: https://EditorConfig.org

root = true

[*]
charset = utf-8
end_of_line = lf
insert_final_newline = true
trim_trailing_whitespace = true

[*.py]
indent_style = space
indent_size = 4
max_line_length = 88

[*.{json,yml,yaml,toml}]
indent_style = space
indent_size = 2

[Makefile]
indent_style = tab

[*.md]
trim_trailing_whitespace = false
'''
        
        with open(self.project_root / '.editorconfig', 'w') as f:
            f.write(editorconfig)
    
    def run(self):
        """ëª¨ë“  IDE ì„¤ì • ì‹¤í–‰"""
        print("ğŸš€ Setting up IDE integrations for uv...\n")
        
        # VS Code
        self.setup_vscode()
        print()
        
        # PyCharm
        self.setup_pycharm()
        print()
        
        # Vim/Neovim
        self.setup_vim()
        print()
        
        # EditorConfig
        self.generate_editorconfig()
        print("âœ… Created .editorconfig")
        
        print("\nâœ… All IDE integrations configured!")
        print("\nğŸ“Œ Next steps:")
        print("1. Restart your IDE")
        print("2. Select Python interpreter from .venv")
        print("3. Install recommended extensions")


if __name__ == '__main__':
    integrator = IDEIntegrator()
    integrator.run()
```

### Task 2.2: pyproject.toml ë§ˆì´ê·¸ë ˆì´ì…˜

#### SubTask 2.2.1: ì˜ì¡´ì„± êµ¬ì¡° ë¶„ì„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ë¦¬ë“œ  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```python
#!/usr/bin/env python3
# scripts/analyze_dependencies.py

import re
import ast
import subprocess
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
from collections import defaultdict
import json
import graphviz

class DependencyAnalyzer:
    """í”„ë¡œì íŠ¸ ì˜ì¡´ì„± êµ¬ì¡° ë¶„ì„"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.dependencies = defaultdict(set)
        self.import_map = defaultdict(set)
        self.package_usage = defaultdict(set)
        
    def analyze_imports(self) -> Dict[str, Set[str]]:
        """ëª¨ë“  Python íŒŒì¼ì˜ import ë¶„ì„"""
        print("ğŸ” Analyzing Python imports...")
        
        py_files = list(self.project_root.rglob("*.py"))
        
        for py_file in py_files:
            if '.venv' in str(py_file) or '__pycache__' in str(py_file):
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                tree = ast.parse(content)
                imports = self._extract_imports(tree)
                
                relative_path = py_file.relative_to(self.project_root)
                self.import_map[str(relative_path)] = imports
                
                # íŒ¨í‚¤ì§€ë³„ ì‚¬ìš© íŒŒì¼ ë§¤í•‘
                for imp in imports:
                    base_package = imp.split('.')[0]
                    self.package_usage[base_package].add(str(relative_path))
                    
            except Exception as e:
                print(f"  âš ï¸  Error analyzing {py_file}: {e}")
        
        print(f"  âœ… Analyzed {len(py_files)} Python files")
        return dict(self.import_map)
    
    def _extract_imports(self, tree: ast.AST) -> Set[str]:
        """ASTì—ì„œ import ì¶”ì¶œ"""
        imports = set()
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module)
        
        return imports
    
    def analyze_requirements(self) -> Dict[str, List[Dict[str, str]]]:
        """requirements íŒŒì¼ ë¶„ì„"""
        print("ğŸ“¦ Analyzing requirements files...")
        
        req_files = list(self.project_root.glob("requirements*.txt"))
        requirements = {}
        
        for req_file in req_files:
            packages = []
            
            with open(req_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        # -r ë‹¤ë¥¸íŒŒì¼ ì²˜ë¦¬
                        if line.startswith('-r '):
                            ref_file = line[3:].strip()
                            ref_path = req_file.parent / ref_file
                            if ref_path.exists():
                                sub_packages = self._parse_requirements_file(ref_path)
                                packages.extend(sub_packages)
                        else:
                            package_info = self._parse_requirement_line(line)
                            if package_info:
                                packages.append(package_info)
            
            requirements[req_file.name] = packages
            print(f"  âœ… {req_file.name}: {len(packages)} packages")
        
        return requirements
    
    def _parse_requirement_line(self, line: str) -> Optional[Dict[str, str]]:
        """requirements ë¼ì¸ íŒŒì‹±"""
        # ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
        patterns = [
            # package==1.0.0
            r'^([a-zA-Z0-9\-_]+)==([0-9\.]+.*)$',
            # package>=1.0.0,<2.0.0
            r'^([a-zA-Z0-9\-_]+)([><=~]+.*)$',
            # package[extra]==1.0.0
            r'^([a-zA-Z0-9\-_]+)\[([^\]]+)\]([><=~]+.*)$',
            # git+https://...
            r'^git\+(.+)#egg=([a-zA-Z0-9\-_]+)$',
            # -e ./path
            r'^-e\s+(.+)$',
        ]
        
        for pattern in patterns:
            match = re.match(pattern, line)
            if match:
                if pattern.startswith('^git'):
                    return {
                        'name': match.group(2),
                        'version': 'git',
                        'source': match.group(1),
                        'original': line
                    }
                elif pattern.startswith('^-e'):
                    return {
                        'name': 'editable',
                        'version': 'local',
                        'path': match.group(1),
                        'original': line
                    }
                elif '[' in pattern:
                    return {
                        'name': match.group(1),
                        'extras': match.group(2),
                        'version': match.group(3),
                        'original': line
                    }
                else:
                    return {
                        'name': match.group(1),
                        'version': match.group(2) if len(match.groups()) > 1 else 'any',
                        'original': line
                    }
        
        # ë‹¨ìˆœ íŒ¨í‚¤ì§€ëª…
        if re.match(r'^[a-zA-Z0-9\-_]+$', line):
            return {
                'name': line,
                'version': 'any',
                'original': line
            }
        
        return None
    
    def _parse_requirements_file(self, path: Path) -> List[Dict[str, str]]:
        """requirements íŒŒì¼ íŒŒì‹±"""
        packages = []
        
        with open(path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and not line.startswith('-r '):
                    package_info = self._parse_requirement_line(line)
                    if package_info:
                        packages.append(package_info)
        
        return packages
    
    def categorize_dependencies(self) -> Dict[str, List[str]]:
        """ì˜ì¡´ì„±ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        print("ğŸ·ï¸  Categorizing dependencies...")
        
        categories = {
            'core': [],           # í•µì‹¬ í”„ë ˆì„ì›Œí¬
            'database': [],       # ë°ì´í„°ë² ì´ìŠ¤
            'web': [],           # ì›¹ ê´€ë ¨
            'ml': [],            # ë¨¸ì‹ ëŸ¬ë‹
            'testing': [],       # í…ŒìŠ¤íŠ¸
            'linting': [],       # ë¦°íŒ…/í¬ë§·íŒ…
            'development': [],   # ê°œë°œ ë„êµ¬
            'deployment': [],    # ë°°í¬
            'utilities': []      # ìœ í‹¸ë¦¬í‹°
        }
        
        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        category_patterns = {
            'core': ['fastapi', 'django', 'flask', 'starlette', 'uvicorn', 'gunicorn'],
            'database': ['sqlalchemy', 'psycopg', 'pymongo', 'redis', 'alembic', 'asyncpg'],
            'web': ['requests', 'httpx', 'aiohttp', 'beautifulsoup', 'selenium', 'httptools'],
            'ml': ['langchain', 'openai', 'anthropic', 'transformers', 'torch', 'tensorflow', 'numpy', 'pandas', 'scikit-learn'],
            'testing': ['pytest', 'unittest', 'mock', 'faker', 'factory-boy', 'hypothesis'],
            'linting': ['black', 'flake8', 'pylint', 'mypy', 'ruff', 'isort', 'bandit'],
            'development': ['ipython', 'jupyter', 'notebook', 'ipdb', 'rich', 'click'],
            'deployment': ['docker', 'kubernetes', 'ansible', 'fabric', 'supervisor'],
            'utilities': ['python-dotenv', 'pydantic', 'celery', 'schedule', 'arrow', 'pendulum']
        }
        
        # ëª¨ë“  requirements íŒŒì¼ì—ì„œ íŒ¨í‚¤ì§€ ìˆ˜ì§‘
        all_packages = set()
        requirements = self.analyze_requirements()
        
        for req_file, packages in requirements.items():
            for pkg in packages:
                all_packages.add(pkg['name'])
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
        categorized = set()
        
        for category, patterns in category_patterns.items():
            for package in all_packages:
                package_lower = package.lower()
                for pattern in patterns:
                    if pattern in package_lower:
                        categories[category].append(package)
                        categorized.add(package)
                        break
        
        # ë¯¸ë¶„ë¥˜ íŒ¨í‚¤ì§€
        uncategorized = all_packages - categorized
        categories['utilities'].extend(list(uncategorized))
        
        # ê²°ê³¼ ì¶œë ¥
        for category, packages in categories.items():
            if packages:
                print(f"  {category}: {len(packages)} packages")
        
        return categories
    
    def detect_unused_dependencies(self) -> List[str]:
        """ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì˜ì¡´ì„± ê°ì§€"""
        print("ğŸ” Detecting unused dependencies...")
        
        # import ë¶„ì„
        self.analyze_imports()
        
        # ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ì™€ import ë¹„êµ
        installed_packages = set()
        requirements = self.analyze_requirements()
        
        for packages in requirements.values():
            for pkg in packages:
                installed_packages.add(pkg['name'].lower())
        
        # ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” íŒ¨í‚¤ì§€
        used_packages = set()
        for imports in self.import_map.values():
            for imp in imports:
                base_package = imp.split('.')[0].lower()
                used_packages.add(base_package)
        
        # import ì´ë¦„ê³¼ íŒ¨í‚¤ì§€ ì´ë¦„ ë§¤í•‘ (ì¼ë¶€ íŒ¨í‚¤ì§€ëŠ” import ì´ë¦„ì´ ë‹¤ë¦„)
        package_import_map = {
            'pillow': 'PIL',
            'beautifulsoup4': 'bs4',
            'python-jose': 'jose',
            'python-multipart': 'multipart',
            'python-dotenv': 'dotenv',
            'scikit-learn': 'sklearn',
            'msgpack-python': 'msgpack',
            'mysqlclient': 'MySQLdb',
            'psycopg2-binary': 'psycopg2',
        }
        
        # ì—­ë§¤í•‘ë„ ìƒì„±
        import_package_map = {v.lower(): k for k, v in package_import_map.items()}
        
        # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” íŒ¨í‚¤ì§€ ì°¾ê¸°
        unused = []
        for package in installed_packages:
            # íŒ¨í‚¤ì§€ ì´ë¦„ìœ¼ë¡œ í™•ì¸
            if package not in used_packages:
                # import ì´ë¦„ìœ¼ë¡œë„ í™•ì¸
                import_name = package_import_map.get(package, package)
                if import_name.lower() not in used_packages:
                    # ê°œë°œ/í…ŒìŠ¤íŠ¸ ë„êµ¬ëŠ” ì œì™¸
                    dev_tools = ['pytest', 'black', 'flake8', 'mypy', 'ruff', 'pre-commit', 'ipython', 'ipdb']
                    if not any(tool in package for tool in dev_tools):
                        unused.append(package)
        
        if unused:
            print(f"  âš ï¸  Found {len(unused)} potentially unused packages:")
            for pkg in sorted(unused):
                # í•´ë‹¹ íŒ¨í‚¤ì§€ê°€ ì–´ëŠ requirements íŒŒì¼ì— ìˆëŠ”ì§€ í‘œì‹œ
                files = []
                for req_file, packages in requirements.items():
                    if any(p['name'].lower() == pkg for p in packages):
                        files.append(req_file)
                print(f"    - {pkg} (in {', '.join(files)})")
        else:
            print("  âœ… No unused dependencies detected")
        
        return unused
    
    def generate_dependency_graph(self) -> None:
        """ì˜ì¡´ì„± ê·¸ë˜í”„ ìƒì„±"""
        print("ğŸ“Š Generating dependency graph...")
        
        # graphviz ê°ì²´ ìƒì„±
        dot = graphviz.Digraph(comment='Dependency Graph')
        dot.attr(rankdir='TB', size='10,10')
        
        # ìƒ‰ìƒ ì •ì˜
        colors = {
            'core': '#FF6B6B',
            'database': '#4ECDC4',
            'web': '#45B7D1',
            'ml': '#96CEB4',
            'testing': '#FECA57',
            'linting': '#FF9FF3',
            'development': '#54A0FF',
            'deployment': '#48DBFB',
            'utilities': '#C8C8C8'
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì˜ì¡´ì„±
        categories = self.categorize_dependencies()
        
        # ë…¸ë“œ ì¶”ê°€
        for category, packages in categories.items():
            with dot.subgraph(name=f'cluster_{category}') as c:
                c.attr(label=category.title(), style='filled', color=colors.get(category, 'lightgrey'))
                for package in packages:
                    c.node(package, shape='box', style='filled', fillcolor='white')
        
        # import ê´€ê³„ ì¶”ê°€
        for file_path, imports in self.import_map.items():
            if 'test' not in file_path:  # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì œì™¸
                for imp in imports:
                    base_import = imp.split('.')[0]
                    # í”„ë¡œì íŠ¸ ë‚´ë¶€ import í‘œì‹œ
                    if base_import in ['app', 'src', 'api', 'core']:
                        dot.edge(file_path, base_import, style='dashed', color='grey')
        
        # ê·¸ë˜í”„ ì €ì¥
        output_path = self.project_root / 'dependency_graph'
        dot.render(output_path, format='png', cleanup=True)
        print(f"  âœ… Dependency graph saved to {output_path}.png")
    
    def generate_report(self) -> Dict[str, any]:
        """ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        print("\nğŸ“Š Generating dependency analysis report...")
        
        # ë¶„ì„ ìˆ˜í–‰
        imports = self.analyze_imports()
        requirements = self.analyze_requirements()
        categories = self.categorize_dependencies()
        unused = self.detect_unused_dependencies()
        
        # í†µê³„ ìƒì„±
        total_packages = sum(len(pkgs) for pkgs in requirements.values())
        total_imports = len(set().union(*self.import_map.values()))
        
        report = {
            'summary': {
                'total_packages': total_packages,
                'total_imports': total_imports,
                'total_files_analyzed': len(imports),
                'unused_dependencies': len(unused)
            },
            'requirements_files': requirements,
            'categories': categories,
            'unused_dependencies': unused,
            'package_usage': {
                pkg: list(files) for pkg, files in self.package_usage.items()
                if len(files) > 0
            },
            'recommendations': self._generate_recommendations(categories, unused)
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(self.project_root / 'dependency_analysis.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        # Markdown ë³´ê³ ì„œ ìƒì„±
        self._generate_markdown_report(report)
        
        print("âœ… Report generated: dependency_analysis.json & dependency_analysis.md")
        
        return report
    
    def _generate_recommendations(self, categories: Dict, unused: List) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” íŒ¨í‚¤ì§€
        if unused:
            recommendations.append(f"Remove {len(unused)} unused packages to reduce dependencies")
        
        # ì¤‘ë³µ ê¸°ëŠ¥ íŒ¨í‚¤ì§€
        if 'requests' in categories.get('web', []) and 'httpx' in categories.get('web', []):
            recommendations.append("Consider using only httpx instead of both requests and httpx")
        
        # ê°œë°œ ì˜ì¡´ì„± ë¶„ë¦¬
        dev_deps = len(categories.get('testing', [])) + len(categories.get('linting', [])) + len(categories.get('development', []))
        if dev_deps > 10:
            recommendations.append("Consider separating development dependencies into requirements-dev.txt")
        
        # ë²„ì „ ê³ ì •
        all_packages = []
        for packages in self.analyze_requirements().values():
            all_packages.extend(packages)
        
        unpinned = [p for p in all_packages if p['version'] == 'any']
        if unpinned:
            recommendations.append(f"Pin versions for {len(unpinned)} packages for reproducible builds")
        
        return recommendations
    
    def _generate_markdown_report(self, report: Dict) -> None:
        """Markdown í˜•ì‹ ë³´ê³ ì„œ ìƒì„±"""
        md_content = f"""# Dependency Analysis Report

Generated: {import datetime; datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Summary

- **Total Packages**: {report['summary']['total_packages']}
- **Total Imports**: {report['summary']['total_imports']}
- **Files Analyzed**: {report['summary']['total_files_analyzed']}
- **Unused Dependencies**: {report['summary']['unused_dependencies']}

## Dependencies by Category

"""
        
        for category, packages in report['categories'].items():
            if packages:
                md_content += f"### {category.title()} ({len(packages)} packages)\n"
                for pkg in sorted(packages):
                    md_content += f"- {pkg}\n"
                md_content += "\n"
        
        if report['unused_dependencies']:
            md_content += "## Unused Dependencies\n\n"
            md_content += "The following packages appear to be unused:\n\n"
            for pkg in report['unused_dependencies']:
                md_content += f"- {pkg}\n"
            md_content += "\n"
        
        if report['recommendations']:
            md_content += "## Recommendations\n\n"
            for i, rec in enumerate(report['recommendations'], 1):
                md_content += f"{i}. {rec}\n"
        
        md_content += "\n## Package Usage\n\n"
        md_content += "Top 10 most used packages:\n\n"
        
        # ì‚¬ìš© ë¹ˆë„ ì •ë ¬
        usage_sorted = sorted(
            report['package_usage'].items(),
            key=lambda x: len(x[1]),
            reverse=True
        )[:10]
        
        for pkg, files in usage_sorted:
            md_content += f"- **{pkg}**: Used in {len(files)} files\n"
        
        with open(self.project_root / 'dependency_analysis.md', 'w') as f:
            f.write(md_content)


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Analyze project dependencies')
    parser.add_argument('--path', default='.', help='Project root path')
    parser.add_argument('--graph', action='store_true', help='Generate dependency graph')
    
    args = parser.parse_args()
    
    analyzer = DependencyAnalyzer(args.path)
    report = analyzer.generate_report()
    
    if args.graph:
        analyzer.generate_dependency_graph()
    
    print("\nâœ… Analysis complete!")


if __name__ == '__main__':
    main()
```

#### SubTask 2.2.2: pyproject.toml ìƒì„± ë° ê²€ì¦
**ë‹´ë‹¹ì**: ë¹Œë“œ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 3ì‹œê°„

```python
#!/usr/bin/env python3
# scripts/generate_pyproject.py

import toml
import json
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional
import re
from packaging.version import Version
from packaging.specifiers import SpecifierSet

class PyProjectGenerator:
    """requirements.txtë¥¼ pyproject.tomlë¡œ ë³€í™˜"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.config = {
            'project': {},
            'build-system': {},
            'tool': {}
        }
    
    def analyze_project_metadata(self) -> Dict[str, Any]:
        """í”„ë¡œì íŠ¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
        print("ğŸ“‹ Collecting project metadata...")
        
        metadata = {
            'name': 'ai-agent-framework',
            'version': '0.1.0',
            'description': '',
            'authors': [],
            'license': 'MIT',
            'readme': 'README.md',
            'requires-python': '>=3.11',
            'homepage': '',
            'repository': '',
            'keywords': []
        }
        
        # setup.pyê°€ ìˆìœ¼ë©´ ì •ë³´ ì¶”ì¶œ
        setup_py = self.project_root / 'setup.py'
        if setup_py.exists():
            metadata.update(self._extract_from_setup_py(setup_py))
        
        # READMEì—ì„œ ì„¤ëª… ì¶”ì¶œ
        readme_files = ['README.md', 'README.rst', 'README.txt']
        for readme in readme_files:
            readme_path = self.project_root / readme
            if readme_path.exists():
                metadata['readme'] = readme
                with open(readme_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # ì²« ë‹¨ë½ì„ ì„¤ëª…ìœ¼ë¡œ ì‚¬ìš©
                    lines = content.split('\n')
                    for line in lines:
                        if line.strip() and not line.startswith('#'):
                            metadata['description'] = line.strip()
                            break
                break
        
        # Git ì •ë³´ì—ì„œ ì €ìì™€ ì €ì¥ì†Œ URL ì¶”ì¶œ
        try:
            # ì €ì ì •ë³´
            result = subprocess.run(
                ['git', 'config', 'user.name'],
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                name = result.stdout.strip()
                email_result = subprocess.run(
                    ['git', 'config', 'user.email'],
                    capture_output=True,
                    text=True
                )
                email = email_result.stdout.strip() if email_result.returncode == 0 else ''
                metadata['authors'] = [{'name': name, 'email': email}]
            
            # ì €ì¥ì†Œ URL
            result = subprocess.run(
                ['git', 'remote', 'get-url', 'origin'],
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                repo_url = result.stdout.strip()
                # SSHë¥¼ HTTPSë¡œ ë³€í™˜
                if repo_url.startswith('git@github.com:'):
                    repo_url = repo_url.replace('git@github.com:', 'https://github.com/')
                if repo_url.endswith('.git'):
                    repo_url = repo_url[:-4]
                metadata['repository'] = repo_url
                metadata['homepage'] = repo_url
        except:
            pass
        
        # Python ë²„ì „ í™•ì¸
        py_version_file = self.project_root / '.python-version'
        if py_version_file.exists():
            with open(py_version_file, 'r') as f:
                version = f.read().strip()
                metadata['requires-python'] = f'>={version}'
        
        return metadata
    
    def _extract_from_setup_py(self, setup_py: Path) -> Dict[str, Any]:
        """setup.pyì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {}
        
        try:
            with open(setup_py, 'r') as f:
                content = f.read()
            
            # ì •ê·œì‹ìœ¼ë¡œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            patterns = {
                'name': r"name\s*=\s*['\"]([^'\"]+)['\"]",
                'version': r"version\s*=\s*['\"]([^'\"]+)['\"]",
                'description': r"description\s*=\s*['\"]([^'\"]+)['\"]",
                'author': r"author\s*=\s*['\"]([^'\"]+)['\"]",
                'author_email': r"author_email\s*=\s*['\"]([^'\"]+)['\"]",
                'url': r"url\s*=\s*['\"]([^'\"]+)['\"]",
                'license': r"license\s*=\s*['\"]([^'\"]+)['\"]",
            }
            
            for key, pattern in patterns.items():
                match = re.search(pattern, content)
                if match:
                    if key == 'author':
                        metadata['authors'] = [{'name': match.group(1)}]
                    elif key == 'author_email' and 'authors' in metadata:
                        metadata['authors'][0]['email'] = match.group(1)
                    elif key == 'url':
                        metadata['homepage'] = match.group(1)
                    else:
                        metadata[key] = match.group(1)
            
        except Exception as e:
            print(f"  âš ï¸  Error reading setup.py: {e}")
        
        return metadata
    
    def parse_requirements(self) -> Dict[str, List[str]]:
        """requirements íŒŒì¼ íŒŒì‹± ë° ë¶„ë¥˜"""
        print("ğŸ“¦ Parsing requirements files...")
        
        dependencies = {
            'main': [],
            'dev': [],
            'test': [],
            'docs': [],
            'optional': {}
        }
        
        # requirements íŒŒì¼ ë§¤í•‘
        file_mapping = {
            'requirements.txt': 'main',
            'requirements-dev.txt': 'dev',
            'requirements-test.txt': 'test',
            'requirements-docs.txt': 'docs',
            'dev-requirements.txt': 'dev',
            'test-requirements.txt': 'test'
        }
        
        for req_file, category in file_mapping.items():
            req_path = self.project_root / req_file
            if req_path.exists():
                deps = self._parse_requirements_file(req_path)
                dependencies[category].extend(deps)
                print(f"  âœ… {req_file}: {len(deps)} dependencies")
        
        # ì¤‘ë³µ ì œê±°
        main_packages = {self._get_package_name(dep): dep for dep in dependencies['main']}
        
        # devì—ì„œ mainê³¼ ì¤‘ë³µëœ ê²ƒ ì œê±°
        dev_packages = []
        for dep in dependencies['dev']:
            pkg_name = self._get_package_name(dep)
            if pkg_name not in main_packages:
                dev_packages.append(dep)
        dependencies['dev'] = dev_packages
        
        return dependencies
    
    def _parse_requirements_file(self, file_path: Path) -> List[str]:
        """requirements íŒŒì¼ íŒŒì‹±"""
        dependencies = []
        
        with open(file_path, 'r') as f:
            for line in f:
                line = line.strip()
                
                # ì£¼ì„ê³¼ ë¹ˆ ì¤„ ì œì™¸
                if not line or line.startswith('#'):
                    continue
                
                # -r ë‹¤ë¥¸ íŒŒì¼ ì°¸ì¡°
                if line.startswith('-r '):
                    ref_file = line[3:].strip()
                    ref_path = file_path.parent / ref_file
                    if ref_path.exists():
                        dependencies.extend(self._parse_requirements_file(ref_path))
                    continue
                
                # -e í¸ì§‘ ê°€ëŠ¥ ì„¤ì¹˜
                if line.startswith('-e '):
                    # ë¡œì»¬ íŒ¨í‚¤ì§€ëŠ” pyproject.tomlì—ì„œ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
                    continue
                
                # íŠ¹ìˆ˜ í˜•ì‹ ë³€í™˜
                dependency = self._convert_requirement_format(line)
                if dependency:
                    dependencies.append(dependency)
        
        return dependencies
    
    def _convert_requirement_format(self, requirement: str) -> Optional[str]:
        """requirements í˜•ì‹ì„ pyproject.toml í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        # Git URL
        if requirement.startswith('git+'):
            match = re.match(r'git\+([^@#]+)(?:@([^#]+))?(?:#egg=(.+))?', requirement)
            if match:
                url, ref, name = match.groups()
                if not name:
                    # URLì—ì„œ í”„ë¡œì íŠ¸ ì´ë¦„ ì¶”ì¶œ
                    name = url.rstrip('/').split('/')[-1].replace('.git', '')
                
                if ref:
                    return f'{name} @ git+{url}@{ref}'
                else:
                    return f'{name} @ git+{url}'
        
        # ì¼ë°˜ íŒ¨í‚¤ì§€
        return requirement
    
    def _get_package_name(self, requirement: str) -> str:
        """requirementì—ì„œ íŒ¨í‚¤ì§€ ì´ë¦„ ì¶”ì¶œ"""
        # @ í˜•ì‹ (git ë“±)
        if ' @ ' in requirement:
            return requirement.split(' @ ')[0].strip()
        
        # ë²„ì „ ì§€ì •ì ì œê±°
        for op in ['==', '>=', '<=', '>', '<', '~=', '!=']:
            if op in requirement:
                return requirement.split(op)[0].strip()
        
        # extras ì œê±°
        if '[' in requirement:
            return requirement.split('[')[0].strip()
        
        return requirement.strip()
    
    def generate_pyproject_toml(self) -> Dict[str, Any]:
        """pyproject.toml ìƒì„±"""
        print("ğŸ”¨ Generating pyproject.toml...")
        
        # í”„ë¡œì íŠ¸ ë©”íƒ€ë°ì´í„°
        metadata = self.analyze_project_metadata()
        
        # ì˜ì¡´ì„± íŒŒì‹±
        dependencies = self.parse_requirements()
        
        # [project] ì„¹ì…˜
        self.config['project'] = {
            'name': metadata['name'],
            'version': metadata['version'],
            'description': metadata['description'],
            'readme': metadata['readme'],
            'requires-python': metadata['requires-python'],
            'license': {'text': metadata['license']},
            'authors': metadata['authors'],
            'dependencies': dependencies['main']
        }
        
        if metadata.get('homepage'):
            self.config['project']['urls'] = {
                'Homepage': metadata['homepage'],
                'Repository': metadata.get('repository', metadata['homepage']),
                'Issues': f"{metadata.get('repository', metadata['homepage'])}/issues"
            }
        
        # [project.optional-dependencies] ì„¹ì…˜
        optional_deps = {}
        if dependencies['dev']:
            optional_deps['dev'] = dependencies['dev']
        if dependencies['test']:
            optional_deps['test'] = dependencies['test']
        if dependencies['docs']:
            optional_deps['docs'] = dependencies['docs']
        
        # all extras
        all_extras = []
        for deps in optional_deps.values():
            all_extras.extend(deps)
        optional_deps['all'] = list(set(all_extras))
        
        if optional_deps:
            self.config['project']['optional-dependencies'] = optional_deps
        
        # [build-system] ì„¹ì…˜
        self.config['build-system'] = {
            'requires': ['setuptools>=68.0.0', 'wheel'],
            'build-backend': 'setuptools.build_meta'
        }
        
        # [tool] ì„¹ì…˜
        self.config['tool'] = self._generate_tool_config()
        
        return self.config
    
    def _generate_tool_config(self) -> Dict[str, Any]:
        """ë„êµ¬ë³„ ì„¤ì • ìƒì„±"""
        tool_config = {}
        
        # [tool.uv]
        tool_config['uv'] = {
            'index-url': 'https://pypi.org/simple'
        }
        
        # [tool.black] - black ì„¤ì •ì´ ìˆìœ¼ë©´
        if (self.project_root / 'pyproject.toml').exists():
            # ê¸°ì¡´ ì„¤ì • ìœ ì§€
            try:
                with open(self.project_root / 'pyproject.toml', 'r') as f:
                    existing = toml.load(f)
                    if 'tool' in existing:
                        for tool in ['black', 'isort', 'mypy', 'pytest', 'ruff']:
                            if tool in existing['tool']:
                                tool_config[tool] = existing['tool'][tool]
            except:
                pass
        
        # ê¸°ë³¸ ì„¤ì • ì¶”ê°€
        if 'black' not in tool_config:
            tool_config['black'] = {
                'line-length': 88,
                'target-version': ['py311'],
                'include': '\\.pyi?$',
                'exclude': '''
/(
    \\.git
  | \\.hg
  | \\.mypy_cache
  | \\.tox
  | \\.venv
  | _build
  | buck-out
  | build
  | dist
)/
'''
            }
        
        if 'ruff' not in tool_config:
            tool_config['ruff'] = {
                'select': ['E', 'F', 'B', 'I'],
                'ignore': ['E501'],
                'fixable': ['ALL'],
                'unfixable': [],
                'exclude': [
                    '.git',
                    '.venv',
                    '__pycache__',
                    '.mypy_cache',
                    '.pytest_cache'
                ],
                'line-length': 88,
                'dummy-variable-rgx': '^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$',
                'target-version': 'py311'
            }
        
        if 'mypy' not in tool_config:
            tool_config['mypy'] = {
                'python_version': '3.11',
                'warn_return_any': True,
                'warn_unused_configs': True,
                'disallow_untyped_defs': True,
                'ignore_missing_imports': True
            }
        
        if 'pytest' not in tool_config:
            tool_config['pytest'] = {
                'ini_options': {
                    'minversion': '7.0',
                    'addopts': '-ra -q --strict-markers',
                    'testpaths': ['tests'],
                    'python_files': ['test_*.py', '*_test.py'],
                    'python_classes': ['Test*'],
                    'python_functions': ['test_*']
                }
            }
        
        return tool_config
    
    def validate_pyproject(self) -> List[str]:
        """ìƒì„±ëœ pyproject.toml ê²€ì¦"""
        print("âœ”ï¸  Validating pyproject.toml...")
        
        issues = []
        
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        required_fields = ['name', 'version', 'dependencies']
        for field in required_fields:
            if field not in self.config['project']:
                issues.append(f"Missing required field: project.{field}")
        
        # ì˜ì¡´ì„± í˜•ì‹ ê²€ì¦
        all_deps = []
        all_deps.extend(self.config['project'].get('dependencies', []))
        
        for deps in self.config['project'].get('optional-dependencies', {}).values():
            all_deps.extend(deps)
        
        for dep in all_deps:
            if not self._validate_dependency_format(dep):
                issues.append(f"Invalid dependency format: {dep}")
        
        # Python ë²„ì „ í˜•ì‹ ê²€ì¦
        py_version = self.config['project'].get('requires-python', '')
        if py_version and not self._validate_python_version(py_version):
            issues.append(f"Invalid Python version specifier: {py_version}")
        
        return issues
    
    def _validate_dependency_format(self, dep: str) -> bool:
        """ì˜ì¡´ì„± í˜•ì‹ ê²€ì¦"""
        # Git URL
        if ' @ git+' in dep:
            return True
        
        # íŒ¨í‚¤ì§€ ì´ë¦„ê³¼ ë²„ì „
        try:
            if any(op in dep for op in ['==', '>=', '<=', '>', '<', '~=', '!=']):
                name, version = re.split(r'[<>=!~]+', dep, 1)
                # ë²„ì „ í˜•ì‹ ê²€ì¦
                SpecifierSet(f'>={version.strip()}")
            return True
        except:
            return False
    
    def _validate_python_version(self, version_spec: str) -> bool:
        """Python ë²„ì „ ì§€ì •ì ê²€ì¦"""
        try:
            SpecifierSet(version_spec)
            return True
        except:
            return False
    
    def write_pyproject_toml(self, output_path: Optional[Path] = None) -> None:
        """pyproject.toml íŒŒì¼ ì‘ì„±"""
        if output_path is None:
            output_path = self.project_root / 'pyproject.toml'
        
        # ë°±ì—…
        if output_path.exists():
            backup_path = output_path.with_suffix('.toml.backup')
            import shutil
            shutil.copy2(output_path, backup_path)
            print(f"  ğŸ“‹ Backed up existing file to {backup_path}")
        
        # íŒŒì¼ ì‘ì„±
        with open(output_path, 'w') as f:
            toml.dump(self.config, f)
        
        print(f"  âœ… Written to {output_path}")
    
    def test_with_uv(self) -> bool:
        """uvë¡œ ì„¤ì¹˜ í…ŒìŠ¤íŠ¸"""
        print("ğŸ§ª Testing with uv...")
        
        # ì„ì‹œ ê°€ìƒí™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸
        test_dir = self.project_root / '.test_venv'
        
        try:
            # ê°€ìƒí™˜ê²½ ìƒì„±
            subprocess.run(['uv', 'venv', str(test_dir)], check=True, capture_output=True)
            
            # pyproject.tomlë¡œ ì„¤ì¹˜
            if test_dir.exists():
                activate = test_dir / 'bin' / 'activate'
                cmd = f'source {activate} && uv pip install -e .'
                result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
                
                if result.returncode == 0:
                    print("  âœ… Installation test passed")
                    return True
                else:
                    print(f"  âŒ Installation test failed: {result.stderr}")
                    return False
        finally:
            # ì •ë¦¬
            if test_dir.exists():
                import shutil
                shutil.rmtree(test_dir)
        
        return False
    
    def migrate(self) -> bool:
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ í”„ë¡œì„¸ìŠ¤"""
        print("ğŸš€ Starting pyproject.toml migration...\n")
        
        # 1. pyproject.toml ìƒì„±
        self.generate_pyproject_toml()
        
        # 2. ê²€ì¦
        issues = self.validate_pyproject()
        if issues:
            print("\nâš ï¸  Validation issues found:")
            for issue in issues:
                print(f"  - {issue}")
            
            response = input("\nContinue anyway? (y/N): ")
            if response.lower() != 'y':
                return False
        
        # 3. íŒŒì¼ ì‘ì„±
        self.write_pyproject_toml()
        
        # 4. ì„¤ì¹˜ í…ŒìŠ¤íŠ¸
        if shutil.which('uv'):
            self.test_with_uv()
        
        # 5. ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ ìƒì„±
        self._generate_migration_guide()
        
        print("\nâœ… Migration complete!")
        return True
    
    def _generate_migration_guide(self) -> None:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ ìƒì„±"""
        guide = f"""# pyproject.toml Migration Guide

## Changes Made

1. Created `pyproject.toml` with:
   - Project metadata from setup.py and git
   - Dependencies from requirements*.txt files
   - Tool configurations for black, ruff, mypy, pytest

## Next Steps

### 1. Install with uv
```bash
# Create new virtual environment
uv venv

# Activate it
source .venv/bin/activate  # On Unix
# or
.venv\\Scripts\\activate  # On Windows

# Install project in editable mode
uv pip install -e .

# Install with dev dependencies
uv pip install -e ".[dev]"
```

### 2. Update CI/CD
Replace:
```yaml
pip install -r requirements.txt
```

With:
```yaml
uv pip install -e .
```

### 3. Update Documentation
Update README.md installation instructions to use:
```bash
uv pip install -e .
```

### 4. Optional: Remove old files
Once confirmed working:
```bash
rm requirements*.txt
rm setup.py  # if exists
```

## Dependency Changes

- Main dependencies: {len(self.config['project']['dependencies'])} packages
- Dev dependencies: {len(self.config['project'].get('optional-dependencies', {}).get('dev', []))} packages
- Test dependencies: {len(self.config['project'].get('optional-dependencies', {}).get('test', []))} packages

## Troubleshooting

If you encounter issues:

1. Check `pyproject.toml.backup` for the previous version
2. Ensure all git URLs are accessible
3. Verify Python version requirement
4. Run `uv pip install -e . -v` for verbose output
"""
        
        with open(self.project_root / 'MIGRATION_GUIDE.md', 'w') as f:
            f.write(guide)
        
        print("ğŸ“– Migration guide written to MIGRATION_GUIDE.md")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Generate pyproject.toml from requirements.txt')
    parser.add_argument('--path', default='.', help='Project root path')
    parser.add_argument('--output', help='Output file path')
    parser.add_argument('--no-test', action='store_true', help='Skip uv test')
    
    args = parser.parse_args()
    
    generator = PyProjectGenerator(args.path)
    
    if args.no_test:
        generator.generate_pyproject_toml()
        generator.write_pyproject_toml(Path(args.output) if args.output else None)
    else:
        generator.migrate()


if __name__ == '__main__':
    main()
```

#### SubTask 2.2.3: ë„êµ¬ ì„¤ì • í†µí•©
**ë‹´ë‹¹ì**: ì¸í”„ë¼ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 3ì‹œê°„

```python
#!/usr/bin/env python3
# scripts/consolidate_tool_configs.py

import os
import json
import toml
import yaml
from pathlib import Path
from typing import Dict, Any, List, Optional
import configparser

class ToolConfigConsolidator:
    """ì—¬ëŸ¬ ë„êµ¬ ì„¤ì •ì„ pyproject.tomlë¡œ í†µí•©"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.configs = {}
        self.pyproject_path = self.project_root / 'pyproject.toml'
        
    def find_config_files(self) -> Dict[str, List[Path]]:
        """í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ì„¤ì • íŒŒì¼ ì°¾ê¸°"""
        print("ğŸ” Finding configuration files...")
        
        config_patterns = {
            'black': ['.black.toml', 'pyproject.toml'],
            'flake8': ['.flake8', 'setup.cfg', 'tox.ini'],
            'mypy': ['mypy.ini', '.mypy.ini', 'setup.cfg'],
            'pytest': ['pytest.ini', 'tox.ini', 'setup.cfg'],
            'isort': ['.isort.cfg', 'setup.cfg'],
            'coverage': ['.coveragerc', 'setup.cfg'],
            'ruff': ['ruff.toml', '.ruff.toml', 'pyproject.toml'],
            'bandit': ['.bandit', 'setup.cfg'],
            'pylint': ['.pylintrc', 'pylintrc', 'setup.cfg']
        }
        
        found_configs = {}
        
        for tool, patterns in config_patterns.items():
            found_configs[tool] = []
            for pattern in patterns:
                config_path = self.project_root / pattern
                if config_path.exists():
                    found_configs[tool].append(config_path)
                    print(f"  âœ… Found {tool} config: {pattern}")
        
        return found_configs
    
    def extract_black_config(self, config_files: List[Path]) -> Dict[str, Any]:
        """Black ì„¤ì • ì¶”ì¶œ"""
        config = {
            'line-length': 88,
            'target-version': ['py311'],
            'include': '\\.pyi?$'
        }
        
        for config_file in config_files:
            if config_file.name == 'pyproject.toml':
                try:
                    data = toml.load(config_file)
                    if 'tool' in data and 'black' in data['tool']:
                        config.update(data['tool']['black'])
                except:
                    pass
            elif config_file.suffix == '.toml':
                try:
                    data = toml.load(config_file)
                    config.update(data)
                except:
                    pass
        
        return config
    
    def extract_flake8_config(self, config_files: List[Path]) -> Dict[str, Any]:
        """Flake8 ì„¤ì • ì¶”ì¶œ ë° Ruff í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        config = {}
        
        for config_file in config_files:
            if config_file.name == '.flake8':
                parser = configparser.ConfigParser()
                parser.read(config_file)
                
                if 'flake8' in parser:
                    section = parser['flake8']
                    
                    # Ruffë¡œ ë³€í™˜
                    ruff_config = {
                        'line-length': int(section.get('max-line-length', 88)),
                        'select': ['E', 'F', 'W'],
                        'ignore': []
                    }
                    
                    # ignore ì„¤ì • ë³€í™˜
                    if 'ignore' in section:
                        ignores = section['ignore'].split(',')
                        ruff_config['ignore'] = [i.strip() for i in ignores]
                    
                    # extend-ignore ë³‘í•©
                    if 'extend-ignore' in section:
                        extends = section['extend-ignore'].split(',')
                        ruff_config['ignore'].extend([i.strip() for i in extends])
                    
                    # exclude ì„¤ì •
                    if 'exclude' in section:
                        excludes = section['exclude'].split(',')
                        ruff_config['exclude'] = [e.strip() for e in excludes]
                    
                    config = ruff_config
                    
        return config
    
    def extract_mypy_config(self, config_files: List[Path]) -> Dict[str, Any]:
        """MyPy ì„¤ì • ì¶”ì¶œ"""
        config = {
            'python_version': '3.11',
            'warn_return_any': True,
            'warn_unused_configs': True
        }
        
        for config_file in config_files:
            if config_file.name in ['mypy.ini', '.mypy.ini']:
                parser = configparser.ConfigParser()
                parser.read(config_file)
                
                if 'mypy' in parser:
                    section = parser['mypy']
                    
                    # ë¶ˆë¦° ê°’ ë³€í™˜
                    bool_keys = [
                        'warn_return_any', 'warn_unused_configs',
                        'disallow_untyped_defs', 'ignore_missing_imports',
                        'strict_optional', 'warn_redundant_casts',
                        'warn_unused_ignores', 'warn_no_return',
                        'warn_unreachable', 'strict_equality'
                    ]
                    
                    for key in bool_keys:
                        if key in section:
                            config[key] = section.getboolean(key)
                    
                    # ë¬¸ìì—´ ê°’
                    if 'python_version' in section:
                        config['python_version'] = section['python_version']
                    
                    # ë¦¬ìŠ¤íŠ¸ ê°’
                    if 'exclude' in section:
                        config['exclude'] = section['exclude'].split('\n')
        
        return config
    
    def extract_pytest_config(self, config_files: List[Path]) -> Dict[str, Any]:
        """Pytest ì„¤ì • ì¶”ì¶œ"""
        config = {
            'minversion': '7.0',
            'testpaths': ['tests'],
            'python_files': ['test_*.py', '*_test.py']
        }
        
        for config_file in config_files:
            if config_file.name == 'pytest.ini':
                parser = configparser.ConfigParser()
                parser.read(config_file)
                
                if 'pytest' in parser:
                    section = parser['pytest']
                    
                    # ì§ì ‘ ë§¤í•‘ë˜ëŠ” ê°’ë“¤
                    direct_keys = ['minversion', 'addopts', 'norecursedirs']
                    for key in direct_keys:
                        if key in section:
                            config[key] = section[key]
                    
                    # ë¦¬ìŠ¤íŠ¸ ê°’ë“¤
                    list_keys = ['testpaths', 'python_files', 'python_classes', 'python_functions']
                    for key in list_keys:
                        if key in section:
                            values = section[key].strip().split('\n')
                            config[key] = [v.strip() for v in values if v.strip()]
        
        # ini_optionsë¡œ ë˜í•‘
        return {'ini_options': config}
    
    def extract_isort_config(self, config_files: List[Path]) -> Dict[str, Any]:
        """isort ì„¤ì • ì¶”ì¶œ"""
        config = {
            'profile': 'black',
            'line_length': 88
        }
        
        for config_file in config_files:
            if config_file.name == '.isort.cfg':
                parser = configparser.ConfigParser()
                parser.read(config_file)
                
                if 'settings' in parser:
                    section = parser['settings']
                    
                    # ì§ì ‘ ë§¤í•‘
                    if 'line_length' in section:
                        config['line_length'] = int(section['line_length'])
                    
                    if 'profile' in section:
                        config['profile'] = section['profile']
                    
                    # known_third_party ë“±
                    for key in ['known_third_party', 'known_first_party']:
                        if key in section:
                            values = section[key].strip().split(',')
                            config[key] = [v.strip() for v in values if v.strip()]
        
        return config
    
    def extract_coverage_config(self, config_files: List[Path]) -> Dict[str, Any]:
        """Coverage ì„¤ì • ì¶”ì¶œ"""
        config = {}
        
        for config_file in config_files:
            if config_file.name == '.coveragerc':
                parser = configparser.ConfigParser()
                parser.read(config_file)
                
                # run ì„¹ì…˜
                if 'run' in parser:
                    run_config = {}
                    section = parser['run']
                    
                    if 'source' in section:
                        run_config['source'] = section['source'].split(',')
                    
                    if 'omit' in section:
                        omits = section['omit'].strip().split('\n')
                        run_config['omit'] = [o.strip() for o in omits if o.strip()]
                    
                    config['run'] = run_config
                
                # report ì„¹ì…˜
                if 'report' in parser:
                    report_config = {}
                    section = parser['report']
                    
                    if 'exclude_lines' in section:
                        lines = section['exclude_lines'].strip().split('\n')
                        report_config['exclude_lines'] = [l.strip() for l in lines if l.strip()]
                    
                    config['report'] = report_config
        
        return config
    
    def create_consolidated_config(self) -> Dict[str, Any]:
        """ëª¨ë“  ë„êµ¬ ì„¤ì •ì„ í†µí•©"""
        print("\nğŸ”§ Consolidating tool configurations...")
        
        found_configs = self.find_config_files()
        tool_configs = {}
        
        # ê° ë„êµ¬ë³„ ì„¤ì • ì¶”ì¶œ
        extractors = {
            'black': self.extract_black_config,
            'flake8': self.extract_flake8_config,  # Ruffë¡œ ë³€í™˜
            'mypy': self.extract_mypy_config,
            'pytest': self.extract_pytest_config,
            'isort': self.extract_isort_config,
            'coverage': self.extract_coverage_config
        }
        
        for tool, extractor in extractors.items():
            if found_configs.get(tool):
                config = extractor(found_configs[tool])
                if config:
                    if tool == 'flake8':
                        # Flake8ì€ Ruffë¡œ ëŒ€ì²´
                        tool_configs['ruff'] = config
                        print(f"  âœ… Converted flake8 â†’ ruff config")
                    else:
                        tool_configs[tool] = config
                        print(f"  âœ… Extracted {tool} config")
        
        # ì¶”ê°€ ë„êµ¬ ê¸°ë³¸ ì„¤ì •
        if 'ruff' not in tool_configs:
            tool_configs['ruff'] = {
                'select': ['E', 'F', 'B', 'I'],
                'ignore': ['E501'],
                'line-length': 88,
                'target-version': 'py311'
            }
        
        return tool_configs
    
    def update_pyproject_toml(self, tool_configs: Dict[str, Any]) -> None:
        """pyproject.toml ì—…ë°ì´íŠ¸"""
        print("\nğŸ“ Updating pyproject.toml...")
        
        # ê¸°ì¡´ pyproject.toml ì½ê¸°
        if self.pyproject_path.exists():
            with open(self.pyproject_path, 'r') as f:
                pyproject = toml.load(f)
        else:
            pyproject = {}
        
        # tool ì„¹ì…˜ ì—…ë°ì´íŠ¸
        if 'tool' not in pyproject:
            pyproject['tool'] = {}
        
        for tool, config in tool_configs.items():
            pyproject['tool'][tool] = config
            print(f"  âœ… Added [tool.{tool}] section")
        
        # íŒŒì¼ ì“°ê¸°
        with open(self.pyproject_path, 'w') as f:
            toml.dump(pyproject, f)
        
        print(f"\nâœ… Updated {self.pyproject_path}")
    
    def create_migration_script(self) -> None:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        script = '''#!/bin/bash
# Tool configuration migration script

echo "ğŸš€ Migrating tool configurations to pyproject.toml"

# Backup existing configs
mkdir -p .config_backup
for config in .flake8 .isort.cfg .coveragerc mypy.ini pytest.ini .black.toml .pylintrc .bandit; do
    if [ -f "$config" ]; then
        cp "$config" .config_backup/
        echo "  ğŸ“‹ Backed up $config"
    fi
done

echo ""
echo "âœ… Configuration migrated to pyproject.toml"
echo ""
echo "Next steps:"
echo "1. Test the new configuration:"
echo "   - black --check ."
echo "   - ruff check ."
echo "   - mypy ."
echo "   - pytest"
echo ""
echo "2. If everything works, remove old config files:"
echo "   rm .flake8 .isort.cfg .coveragerc mypy.ini pytest.ini"
echo ""
echo "3. Update your pre-commit hooks to use pyproject.toml"
'''
        
        script_path = self.project_root / 'migrate_configs.sh'
        with open(script_path, 'w') as f:
            f.write(script)
        
        os.chmod(script_path, 0o755)
        print(f"\nğŸ“œ Created migration script: {script_path}")
    
    def generate_pre_commit_config(self) -> None:
        """pre-commit ì„¤ì • ì—…ë°ì´íŠ¸"""
        pre_commit_config = {
            'repos': [
                {
                    'repo': 'https://github.com/pre-commit/pre-commit-hooks',
                    'rev': 'v4.5.0',
                    'hooks': [
                        {'id': 'trailing-whitespace'},
                        {'id': 'end-of-file-fixer'},
                        {'id': 'check-yaml'},
                        {'id': 'check-added-large-files'}
                    ]
                },
                {
                    'repo': 'https://github.com/psf/black',
                    'rev': '23.11.0',
                    'hooks': [
                        {'id': 'black'}
                    ]
                },
                {
                    'repo': 'https://github.com/charliermarsh/ruff-pre-commit',
                    'rev': 'v0.1.6',
                    'hooks': [
                        {'id': 'ruff'}
                    ]
                },
                {
                    'repo': 'https://github.com/pre-commit/mirrors-mypy',
                    'rev': 'v1.7.1',
                    'hooks': [
                        {
                            'id': 'mypy',
                            'additional_dependencies': ['types-all']
                        }
                    ]
                }
            ]
        }
        
        with open(self.project_root / '.pre-commit-config.yaml', 'w') as f:
            yaml.dump(pre_commit_config, f, default_flow_style=False)
        
        print("ğŸ“ Created .pre-commit-config.yaml")
    
    def consolidate(self) -> None:
        """ì „ì²´ í†µí•© í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸ”„ Starting tool configuration consolidation...\n")
        
        # 1. ë„êµ¬ ì„¤ì • ì¶”ì¶œ ë° í†µí•©
        tool_configs = self.create_consolidated_config()
        
        # 2. pyproject.toml ì—…ë°ì´íŠ¸
        self.update_pyproject_toml(tool_configs)
        
        # 3. ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        self.create_migration_script()
        
        # 4. pre-commit ì„¤ì • ìƒì„±
        self.generate_pre_commit_config()
        
        # 5. ìš”ì•½ ì¶œë ¥
        self._print_summary(tool_configs)
    
    def _print_summary(self, tool_configs: Dict[str, Any]) -> None:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*50)
        print("ğŸ“Š Configuration Consolidation Summary")
        print("="*50)
        
        print(f"\nTools configured in pyproject.toml:")
        for tool in sorted(tool_configs.keys()):
            print(f"  âœ… {tool}")
        
        print("\nâš ï¸  Old configuration files to remove:")
        old_configs = [
            '.flake8', '.isort.cfg', '.coveragerc', 
            'mypy.ini', 'pytest.ini', '.black.toml'
        ]
        for config in old_configs:
            if (self.project_root / config).exists():
                print(f"  - {config}")
        
        print("\nğŸ“Œ Commands updated to use pyproject.toml:")
        print("  - black .  # Uses [tool.black]")
        print("  - ruff check .  # Uses [tool.ruff]")
        print("  - mypy .  # Uses [tool.mypy]")
        print("  - pytest  # Uses [tool.pytest]")
        print("  - isort .  # Uses [tool.isort]")
        
        print("\nâœ… Configuration consolidation complete!")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Consolidate tool configurations into pyproject.toml'
    )
    parser.add_argument('--path', default='.', help='Project root path')
    
    args = parser.parse_args()
    
    consolidator = ToolConfigConsolidator(args.path)
    consolidator.consolidate()


if __name__ == '__main__':
    main()
```

### Task 2.3: ê°œë°œì ì›Œí¬í”Œë¡œìš° ì—…ë°ì´íŠ¸

#### SubTask 2.3.1: Makefile ì—…ë°ì´íŠ¸
**ë‹´ë‹¹ì**: ë¹Œë“œ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```makefile
# Makefile - Updated for uv

# Variables
PYTHON := python3
UV := uv
VENV := .venv
PYTHON_VENV := $(VENV)/bin/python
UV_VENV := source $(VENV)/bin/activate &&

# Colors for output
RED := \033[0;31m
GREEN := \033[0;32m
YELLOW := \033[1;33m
NC := \033[0m # No Color

# Default target
.DEFAULT_GOAL := help

# Check if uv is installed
.PHONY: check-uv
check-uv:
	@command -v $(UV) >/dev/null 2>&1 || { \
		echo "$(RED)Error: uv is not installed$(NC)"; \
		echo "Install with: curl -LsSf https://astral.sh/uv/install.sh | sh"; \
		exit 1; \
	}

# Help target
.PHONY: help
help:
	@echo "$(GREEN)AI Agent Framework - Development Commands$(NC)"
	@echo ""
	@echo "$(YELLOW)Setup:$(NC)"
	@echo "  make install          - Install all dependencies"
	@echo "  make install-dev      - Install with dev dependencies"
	@echo "  make clean            - Clean up generated files"
	@echo ""
	@echo "$(YELLOW)Development:$(NC)"
	@echo "  make format          - Format code with black"
	@echo "  make lint            - Run linting (ruff)"
	@echo "  make type-check      - Run type checking (mypy)"
	@echo "  make test            - Run tests"
	@echo "  make test-cov        - Run tests with coverage"
	@echo "  make check           - Run all checks (format, lint, type, test)"
	@echo ""
	@echo "$(YELLOW)Dependencies:$(NC)"
	@echo "  make deps-update     - Update all dependencies"
	@echo "  make deps-lock       - Lock current dependencies"
	@echo "  make deps-sync       - Sync with locked dependencies"
	@echo "  make deps-check      - Check for outdated dependencies"
	@echo ""
	@echo "$(YELLOW)Docker:$(NC)"
	@echo "  make docker-build    - Build Docker image"
	@echo "  make docker-run      - Run Docker container"
	@echo "  make docker-push     - Push to registry"
	@echo ""
	@echo "$(YELLOW)Database:$(NC)"
	@echo "  make db-upgrade      - Run database migrations"
	@echo "  make db-downgrade    - Rollback database migration"
	@echo "  make db-reset        - Reset database"
	@echo ""
	@echo "$(YELLOW)Utilities:$(NC)"
	@echo "  make shell           - Open Python shell"
	@echo "  make docs            - Generate documentation"
	@echo "  make clean-cache     - Clean Python cache files"
	@echo "  make benchmark       - Run performance benchmarks"

# Virtual environment setup
$(VENV)/bin/activate: check-uv
	@echo "$(GREEN)Creating virtual environment...$(NC)"
	@$(UV) venv $(VENV)
	@echo "$(GREEN)Virtual environment created$(NC)"

# Install dependencies
.PHONY: install
install: $(VENV)/bin/activate
	@echo "$(GREEN)Installing dependencies...$(NC)"
	@$(UV_VENV) $(UV) pip sync requirements.txt
	@echo "$(GREEN)Dependencies installed$(NC)"

# Install with dev dependencies
.PHONY: install-dev
install-dev: $(VENV)/bin/activate
	@echo "$(GREEN)Installing all dependencies...$(NC)"
	@$(UV_VENV) $(UV) pip install -e ".[dev,test]"
	@$(UV_VENV) pre-commit install
	@echo "$(GREEN)All dependencies installed$(NC)"

# Format code
.PHONY: format
format: $(VENV)/bin/activate
	@echo "$(GREEN)Formatting code...$(NC)"
	@$(UV_VENV) black .
	@$(UV_VENV) isort .
	@echo "$(GREEN)Code formatted$(NC)"

# Lint code
.PHONY: lint
lint: $(VENV)/bin/activate
	@echo "$(GREEN)Running linter...$(NC)"
	@$(UV_VENV) ruff check .
	@echo "$(GREEN)Linting complete$(NC)"

# Type checking
.PHONY: type-check
type-check: $(VENV)/bin/activate
	@echo "$(GREEN)Running type checker...$(NC)"
	@$(UV_VENV) mypy app/
	@echo "$(GREEN)Type checking complete$(NC)"

# Run tests
.PHONY: test
test: $(VENV)/bin/activate
	@echo "$(GREEN)Running tests...$(NC)"
	@$(UV_VENV) pytest tests/ -v
	@echo "$(GREEN)Tests complete$(NC)"

# Run tests with coverage
.PHONY: test-cov
test-cov: $(VENV)/bin/activate
	@echo "$(GREEN)Running tests with coverage...$(NC)"
	@$(UV_VENV) pytest tests/ -v --cov=app --cov-report=html --cov-report=term
	@echo "$(GREEN)Coverage report generated in htmlcov/$(NC)"

# Run all checks
.PHONY: check
check: format lint type-check test
	@echo "$(GREEN)All checks passed!$(NC)"

# Update dependencies
.PHONY: deps-update
deps-update: $(VENV)/bin/activate
	@echo "$(GREEN)Updating dependencies...$(NC)"
	@$(UV_VENV) $(UV) pip install --upgrade -r requirements.txt
	@echo "$(GREEN)Dependencies updated$(NC)"

# Lock dependencies
.PHONY: deps-lock
deps-lock: $(VENV)/bin/activate
	@echo "$(GREEN)Locking dependencies...$(NC)"
	@$(UV_VENV) $(UV) pip freeze > requirements.lock
	@echo "$(GREEN)Dependencies locked$(NC)"

# Sync with locked dependencies
.PHONY: deps-sync
deps-sync: $(VENV)/bin/activate requirements.lock
	@echo "$(GREEN)Syncing dependencies...$(NC)"
	@$(UV_VENV) $(UV) pip sync requirements.lock
	@echo "$(GREEN)Dependencies synced$(NC)"

# Check for outdated dependencies
.PHONY: deps-check
deps-check: $(VENV)/bin/activate
	@echo "$(GREEN)Checking for outdated dependencies...$(NC)"
	@$(UV_VENV) pip list --outdated

# Docker build
.PHONY: docker-build
docker-build:
	@echo "$(GREEN)Building Docker image...$(NC)"
	@docker build -t ai-agent-framework:latest -f Dockerfile.uv .
	@echo "$(GREEN)Docker image built$(NC)"

# Docker run
.PHONY: docker-run
docker-run:
	@echo "$(GREEN)Running Docker container...$(NC)"
	@docker run -it --rm \
		-p 8000:8000 \
		-v $(PWD):/app \
		--env-file .env \
		ai-agent-framework:latest

# Docker push
.PHONY: docker-push
docker-push:
	@echo "$(GREEN)Pushing Docker image...$(NC)"
	@docker tag ai-agent-framework:latest $(DOCKER_REGISTRY)/ai-agent-framework:latest
	@docker push $(DOCKER_REGISTRY)/ai-agent-framework:latest

# Database migrations
.PHONY: db-upgrade
db-upgrade: $(VENV)/bin/activate
	@echo "$(GREEN)Running database migrations...$(NC)"
	@$(UV_VENV) alembic upgrade head

.PHONY: db-downgrade
db-downgrade: $(VENV)/bin/activate
	@echo "$(GREEN)Rolling back database migration...$(NC)"
	@$(UV_VENV) alembic downgrade -1

.PHONY: db-reset
db-reset: $(VENV)/bin/activate
	@echo "$(YELLOW)Warning: This will delete all data!$(NC)"
	@read -p "Are you sure? [y/N] " -n 1 -r; \
	echo; \
	if [[ $$REPLY =~ ^[Yy]$$ ]]; then \
		$(UV_VENV) alembic downgrade base; \
		$(UV_VENV) alembic upgrade head; \
		echo "$(GREEN)Database reset complete$(NC)"; \
	fi

# Python shell
.PHONY: shell
shell: $(VENV)/bin/activate
	@echo "$(GREEN)Starting Python shell...$(NC)"
	@$(UV_VENV) ipython

# Generate documentation
.PHONY: docs
docs: $(VENV)/bin/activate
	@echo "$(GREEN)Generating documentation...$(NC)"
	@$(UV_VENV) mkdocs build
	@echo "$(GREEN)Documentation generated in site/$(NC)"

# Clean cache files
.PHONY: clean-cache
clean-cache:
	@echo "$(GREEN)Cleaning cache files...$(NC)"
	@find . -type d -name __pycache__ -exec rm -rf {} + 2>/dev/null || true
	@find . -type f -name "*.pyc" -delete
	@find . -type f -name "*.pyo" -delete
	@find . -type f -name "*.pyd" -delete
	@find . -type f -name ".coverage" -delete
	@find . -type d -name "*.egg-info" -exec rm -rf {} + 2>/dev/null || true
	@find . -type d -name ".pytest_cache" -exec rm -rf {} + 2>/dev/null || true
	@find . -type d -name ".mypy_cache" -exec rm -rf {} + 2>/dev/null || true
	@find . -type d -name ".ruff_cache" -exec rm -rf {} + 2>/dev/null || true
	@echo "$(GREEN)Cache cleaned$(NC)"

# Clean everything
.PHONY: clean
clean: clean-cache
	@echo "$(GREEN)Cleaning all generated files...$(NC)"
	@rm -rf $(VENV)
	@rm -rf htmlcov/
	@rm -rf dist/
	@rm -rf build/
	@rm -rf site/
	@rm -f requirements.lock
	@echo "$(GREEN)Clean complete$(NC)"

# Performance benchmark
.PHONY: benchmark
benchmark: $(VENV)/bin/activate
	@echo "$(GREEN)Running performance benchmarks...$(NC)"
	@$(UV_VENV) python scripts/benchmark_uv.py
	@echo "$(GREEN)Benchmark complete$(NC)"

# Development server
.PHONY: run
run: $(VENV)/bin/activate
	@echo "$(GREEN)Starting development server...$(NC)"
	@$(UV_VENV) uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Production server
.PHONY: run-prod
run-prod: $(VENV)/bin/activate
	@echo "$(GREEN)Starting production server...$(NC)"
	@$(UV_VENV) gunicorn app.main:app \
		--workers 4 \
		--worker-class uvicorn.workers.UvicornWorker \
		--bind 0.0.0.0:8000

# Watch for changes and run tests
.PHONY: watch
watch: $(VENV)/bin/activate
	@echo "$(GREEN)Watching for changes...$(NC)"
	@$(UV_VENV) ptw tests/ -- -v

# Migration from pip to uv
.PHONY: migrate-from-pip
migrate-from-pip:
	@echo "$(GREEN)Migrating from pip to uv...$(NC)"
	@python scripts/migrate_venv.py $(VENV)
	@echo "$(GREEN)Migration complete$(NC)"

# Verify uv installation
.PHONY: verify-uv
verify-uv: $(VENV)/bin/activate
	@echo "$(GREEN)Verifying uv installation...$(NC)"
	@$(UV) --version
	@$(UV_VENV) $(UV) pip list | head -10
	@echo "$(GREEN)uv is working correctly$(NC)"

# Generate requirements files from pyproject.toml
.PHONY: requirements
requirements: $(VENV)/bin/activate
	@echo "$(GREEN)Generating requirements files...$(NC)"
	@$(UV_VENV) pip-compile pyproject.toml -o requirements.txt
	@$(UV_VENV) pip-compile --extra dev pyproject.toml -o requirements-dev.txt
	@echo "$(GREEN)Requirements files generated$(NC)"

# Quick install for CI
.PHONY: ci-install
ci-install: check-uv
	@$(UV) venv $(VENV)
	@$(UV_VENV) $(UV) pip install -e ".[test]"

# Security check
.PHONY: security
security: $(VENV)/bin/activate
	@echo "$(GREEN)Running security checks...$(NC)"
	@$(UV_VENV) bandit -r app/
	@$(UV_VENV) safety check
	@echo "$(GREEN)Security checks complete$(NC)"
```

#### SubTask 2.3.2: ê°œë°œ ìŠ¤í¬ë¦½íŠ¸ ì—…ë°ì´íŠ¸
**ë‹´ë‹¹ì**: ìŠ¤í¬ë¦½íŠ¸ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 1ì‹œê°„

```bash
#!/bin/bash
# scripts/dev-setup.sh - Development environment setup with uv

set -e

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# Functions
print_step() {
    echo -e "\n${BLUE}==>${NC} $1"
}

print_success() {
    echo -e "${GREEN}âœ“${NC} $1"
}

print_error() {
    echo -e "${RED}âœ—${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}!${NC} $1"
}

# Banner
echo -e "${GREEN}"
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘   AI Agent Framework Dev Setup (uv)   â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo -e "${NC}"

# Check OS
print_step "Detecting operating system"
OS="Unknown"
if [[ "$OSTYPE" == "linux-gnu"* ]]; then
    OS="Linux"
elif [[ "$OSTYPE" == "darwin"* ]]; then
    OS="macOS"
elif [[ "$OSTYPE" == "msys" ]] || [[ "$OSTYPE" == "cygwin" ]]; then
    OS="Windows"
fi
print_success "Detected: $OS"

# Check Python version
print_step "Checking Python version"
PYTHON_VERSION=$(python3 --version 2>&1 | grep -oE '[0-9]+\.[0-9]+')
REQUIRED_VERSION="3.11"

if [ "$(printf '%s\n' "$REQUIRED_VERSION" "$PYTHON_VERSION" | sort -V | head -n1)" = "$REQUIRED_VERSION" ]; then
    print_success "Python $PYTHON_VERSION (>= $REQUIRED_VERSION required)"
else
    print_error "Python $PYTHON_VERSION is too old. Please install Python $REQUIRED_VERSION or newer."
    exit 1
fi

# Install uv if not present
print_step "Checking for uv installation"
if ! command -v uv &> /dev/null; then
    print_warning "uv not found. Installing..."
    
    if [[ "$OS" == "Windows" ]]; then
        print_error "Please run this in PowerShell as Administrator:"
        echo "powershell -c \"irm https://astral.sh/uv/install.ps1 | iex\""
        exit 1
    else
        curl -LsSf https://astral.sh/uv/install.sh | sh
        source $HOME/.cargo/env
    fi
    
    print_success "uv installed successfully"
else
    UV_VERSION=$(uv --version | cut -d' ' -f2)
    print_success "uv $UV_VERSION is already installed"
fi

# Create project structure
print_step "Setting up project structure"
mkdir -p {app,tests,scripts,docs,logs,data}
touch app/__init__.py tests/__init__.py
print_success "Project structure created"

# Create/update .gitignore
print_step "Creating .gitignore"
cat > .gitignore << 'EOF'
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# Virtual environments
.venv/
venv/
ENV/
env/
.venv_pip_backup*/

# uv
.uv/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store

# Testing
.coverage
.pytest_cache/
htmlcov/
.mypy_cache/
.ruff_cache/

# Logs
logs/
*.log

# Environment variables
.env
.env.local
.env.*.local

# Build
dist/
build/
*.egg-info/

# Data
data/
*.db
*.sqlite
EOF
print_success ".gitignore created"

# Create .env.example
print_step "Creating .env.example"
cat > .env.example << 'EOF'
# Application
APP_NAME=ai-agent-framework
APP_ENV=development
DEBUG=true

# Database
DATABASE_URL=postgresql://user:password@localhost/dbname

# Redis
REDIS_URL=redis://localhost:6379/0

# API Keys
OPENAI_API_KEY=your-openai-api-key
ANTHROPIC_API_KEY=your-anthropic-api-key

# AWS (optional)
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION=us-east-1

# Logging
LOG_LEVEL=INFO
EOF
print_success ".env.example created"

# Create virtual environment
print_step "Creating virtual environment with uv"
if [ -d ".venv" ]; then
    print_warning "Virtual environment already exists. Recreating..."
    rm -rf .venv
fi

uv venv .venv
print_success "Virtual environment created"

# Activate virtual environment
print_step "Activating virtual environment"
if [[ "$OS" == "Windows" ]]; then
    source .venv/Scripts/activate
else
    source .venv/bin/activate
fi
print_success "Virtual environment activated"

# Install dependencies
print_step "Installing dependencies with uv"
if [ -f "pyproject.toml" ]; then
    uv pip install -e ".[dev]"
elif [ -f "requirements.txt" ]; then
    uv pip sync requirements.txt
    if [ -f "requirements-dev.txt" ]; then
        uv pip install -r requirements-dev.txt
    fi
else
    print_warning "No requirements file found. Installing basic packages..."
    uv pip install fastapi uvicorn pydantic sqlalchemy alembic pytest black ruff mypy
fi
print_success "Dependencies installed"

# Setup pre-commit hooks
print_step "Setting up pre-commit hooks"
if [ -f ".pre-commit-config.yaml" ]; then
    pre-commit install
    print_success "Pre-commit hooks installed"
else
    print_warning "No .pre-commit-config.yaml found. Creating one..."
    cat > .pre-commit-config.yaml << 'EOF'
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: https://github.com/psf/black
    rev: 23.11.0
    hooks:
      - id: black

  - repo: https://github.com/charliermarsh/ruff-pre-commit
    rev: v0.1.6
    hooks:
      - id: ruff

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.7.1
    hooks:
      - id: mypy
        additional_dependencies: [types-all]
EOF
    pre-commit install
    print_success "Pre-commit config created and hooks installed"
fi

# Setup IDE
print_step "Setting up IDE integration"
python scripts/setup_ide_integration.py 2>/dev/null || print_warning "IDE setup script not found"

# Initialize git if needed
if [ ! -d ".git" ]; then
    print_step "Initializing git repository"
    git init
    git add .
    git commit -m "Initial commit with uv setup"
    print_success "Git repository initialized"
fi

# Create initial test
print_step "Creating sample test"
cat > tests/test_sample.py << 'EOF'
"""Sample test to verify setup."""
import pytest


def test_setup():
    """Test that the setup is working."""
    assert True


def test_imports():
    """Test that main imports work."""
    import fastapi
    import pydantic
    import sqlalchemy
    
    assert fastapi.__version__
    assert pydantic.__version__
    assert sqlalchemy.__version__


if __name__ == "__main__":
    pytest.main([__file__])
EOF
print_success "Sample test created"

# Run tests
print_step "Running tests to verify setup"
pytest tests/test_sample.py -v || print_warning "Tests failed - please check your setup"

# Performance comparison
print_step "Comparing uv vs pip performance (optional)"
if command -v pip &> /dev/null; then
    echo "Installing requests with pip (for comparison)..."
    TIME_PIP=$( { time pip install --dry-run requests >/dev/null 2>&1; } 2>&1 | grep real | awk '{print $2}' )
    
    echo "Installing requests with uv..."
    TIME_UV=$( { time uv pip install --dry-run requests >/dev/null 2>&1; } 2>&1 | grep real | awk '{print $2}' )
    
    echo -e "\n${GREEN}Performance Comparison:${NC}"
    echo "  pip:  $TIME_PIP"
    echo "  uv:   $TIME_UV"
fi

# Summary
echo -e "\n${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo -e "${GREEN}âœ“ Development environment setup complete!${NC}"
echo -e "${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}\n"

echo "Next steps:"
echo "1. Copy .env.example to .env and fill in your values"
echo "2. Activate the virtual environment:"
if [[ "$OS" == "Windows" ]]; then
    echo "   .venv\\Scripts\\activate"
else
    echo "   source .venv/bin/activate"
fi
echo "3. Start developing!"
echo "   make run       # Start development server"
echo "   make test      # Run tests"
echo "   make check     # Run all checks"
echo ""
echo "For more commands, run: make help"

# Create quick start script
cat > start-dev.sh << 'EOF'
#!/bin/bash
source .venv/bin/activate
echo "Virtual environment activated. Run 'make help' for available commands."
exec bash
EOF
chmod +x start-dev.sh

echo -e "\nTip: Run ${GREEN}./start-dev.sh${NC} to quickly activate the environment next time!"
```

#### SubTask 2.3.3: íŒ€ êµìœ¡ ìë£Œ ì‘ì„±
**ë‹´ë‹¹ì**: êµìœ¡ ë‹´ë‹¹ì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 1ì‹œê°„

```markdown
# uv êµìœ¡ ìë£Œ - ê°œë°œíŒ€ ì „í™˜ ê°€ì´ë“œ

## ğŸ“š ëª©ì°¨
1. [uv ì†Œê°œ](#uv-ì†Œê°œ)
2. [ì£¼ìš” ë³€ê²½ì‚¬í•­](#ì£¼ìš”-ë³€ê²½ì‚¬í•­)
3. [ì¼ìƒ ì‘ì—… ê°€ì´ë“œ](#ì¼ìƒ-ì‘ì—…-ê°€ì´ë“œ)
4. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)
5. [ì‹¤ìŠµ ì˜ˆì œ](#ì‹¤ìŠµ-ì˜ˆì œ)

---

## uv ì†Œê°œ

### uvë€?
- **Rustë¡œ ì‘ì„±ëœ ì´ˆê³ ì† Python íŒ¨í‚¤ì§€ ë§¤ë‹ˆì €**
- pip ëŒ€ë¹„ 10-100ë°° ë¹ ë¥¸ ì„±ëŠ¥
- ë” ë‚˜ì€ ì˜ì¡´ì„± í•´ê²°
- íš¨ìœ¨ì ì¸ ìºì‹±

### ì™œ ì „í™˜í•˜ëŠ”ê°€?
1. **ê°œë°œ ìƒì‚°ì„± í–¥ìƒ**
   - íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹œê°„ 90% ë‹¨ì¶•
   - CI/CD íŒŒì´í”„ë¼ì¸ ì†ë„ ê°œì„ 

2. **ë” ë‚˜ì€ ì˜ì¡´ì„± ê´€ë¦¬**
   - ì—„ê²©í•œ ì˜ì¡´ì„± í•´ê²°
   - ì¬í˜„ ê°€ëŠ¥í•œ ë¹Œë“œ

3. **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±**
   - ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½
   - ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ

---

## ì£¼ìš” ë³€ê²½ì‚¬í•­

### ëª…ë ¹ì–´ ë¹„êµí‘œ

| ì‘ì—… | pip (ê¸°ì¡´) | uv (ìƒˆë¡œìš´) |
|------|-----------|------------|
| íŒ¨í‚¤ì§€ ì„¤ì¹˜ | `pip install requests` | `uv pip install requests` |
| requirements ì„¤ì¹˜ | `pip install -r requirements.txt` | `uv pip sync requirements.txt` |
| íŒ¨í‚¤ì§€ ì—…ê·¸ë ˆì´ë“œ | `pip install --upgrade requests` | `uv pip install --upgrade requests` |
| íŒ¨í‚¤ì§€ ì œê±° | `pip uninstall requests` | `uv pip uninstall requests` |
| ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ í™•ì¸ | `pip list` | `uv pip list` |
| íŒ¨í‚¤ì§€ ì •ë³´ | `pip show requests` | `uv pip show requests` |

### ê°€ìƒí™˜ê²½ ìƒì„±

**ê¸°ì¡´ (pip)**
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

**ìƒˆë¡œìš´ (uv)**
```bash
uv venv
source .venv/bin/activate
uv pip sync requirements.txt
```

---

## ì¼ìƒ ì‘ì—… ê°€ì´ë“œ

### 1. í”„ë¡œì íŠ¸ ì‹œì‘í•˜ê¸°

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/company/project.git
cd project

# uvë¡œ í™˜ê²½ ì„¤ì •
uv venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
uv pip install -e ".[dev]"
```

### 2. ìƒˆ íŒ¨í‚¤ì§€ ì¶”ê°€í•˜ê¸°

```bash
# íŒ¨í‚¤ì§€ ì„¤ì¹˜
uv pip install pandas

# pyproject.tomlì— ì¶”ê°€ (ìˆ˜ë™)
# dependencies = [
#     ...
#     "pandas>=2.0.0",
# ]

# ë˜ëŠ” requirements.txt ì—…ë°ì´íŠ¸
uv pip freeze > requirements.txt
```

### 3. ì˜ì¡´ì„± ì—…ë°ì´íŠ¸

```bash
# ëª¨ë“  íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸
uv pip install --upgrade -r requirements.txt

# íŠ¹ì • íŒ¨í‚¤ì§€ë§Œ ì—…ë°ì´íŠ¸
uv pip install --upgrade fastapi
```

### 4. í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# Makefile ì‚¬ìš© (ê¶Œì¥)
make test

# ì§ì ‘ ì‹¤í–‰
uv pip install -e ".[test]"
pytest tests/
```

### 5. ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬

```bash
# ì „ì²´ ê²€ì‚¬
make check

# ê°œë³„ ì‹¤í–‰
make format  # Blackìœ¼ë¡œ í¬ë§·íŒ…
make lint    # Ruffë¡œ ë¦°íŒ…
make type-check  # MyPyë¡œ íƒ€ì… ì²´í¬
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ

#### 1. "uv: command not found"
```bash
# í•´ê²°: uv ì¬ì„¤ì¹˜
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.bashrc  # ë˜ëŠ” ìƒˆ í„°ë¯¸ë„ ì—´ê¸°
```

#### 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤íŒ¨
```bash
# ì˜µì…˜ 1: í”„ë¦¬ë¦´ë¦¬ì¦ˆ ë²„ì „ í—ˆìš©
uv pip install --pre problematic-package

# ì˜µì…˜ 2: ìºì‹œ ì •ë¦¬ í›„ ì¬ì‹œë„
rm -rf ~/.cache/uv
uv pip install problematic-package

# ì˜µì…˜ 3: ìƒì„¸ ë¡œê·¸ í™•ì¸
uv pip install problematic-package -vvv
```

#### 3. ê¸°ì—… í”„ë¡ì‹œ í™˜ê²½
```bash
# í”„ë¡ì‹œ ì„¤ì •
export HTTP_PROXY=http://proxy.company.com:8080
export HTTPS_PROXY=http://proxy.company.com:8080
export NO_PROXY=localhost,127.0.0.1,.company.com

# ì‚¬ë‚´ PyPI ì‚¬ìš©
uv pip install --index-url https://pypi.company.com/simple package
```

#### 4. IDEê°€ ê°€ìƒí™˜ê²½ì„ ì¸ì‹í•˜ì§€ ëª»í•¨
```bash
# VS Code
1. Cmd+Shift+P â†’ "Python: Select Interpreter"
2. .venv/bin/python ì„ íƒ

# PyCharm
1. Settings â†’ Project â†’ Python Interpreter
2. Add â†’ Existing Environment â†’ .venv/bin/python
```

---

## ì‹¤ìŠµ ì˜ˆì œ

### ì‹¤ìŠµ 1: ê¸°ë³¸ í”„ë¡œì íŠ¸ ì„¤ì •
```bash
# 1. ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±
mkdir my-uv-project
cd my-uv-project

# 2. ê°€ìƒí™˜ê²½ ìƒì„±
uv venv

# 3. í™œì„±í™”
source .venv/bin/activate

# 4. íŒ¨í‚¤ì§€ ì„¤ì¹˜
uv pip install fastapi uvicorn

# 5. ê°„ë‹¨í•œ ì•± ì‘ì„±
cat > main.py << 'EOF'
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def read_root():
    return {"Hello": "World", "tool": "uv"}
EOF

# 6. ì‹¤í–‰
uvicorn main:app --reload
```

### ì‹¤ìŠµ 2: ì„±ëŠ¥ ë¹„êµ
```bash
# pip ì„±ëŠ¥ ì¸¡ì •
time pip install --dry-run numpy pandas scikit-learn

# uv ì„±ëŠ¥ ì¸¡ì •
time uv pip install --dry-run numpy pandas scikit-learn
```

### ì‹¤ìŠµ 3: í”„ë¡œì íŠ¸ ë§ˆì´ê·¸ë ˆì´ì…˜
```bash
# 1. ê¸°ì¡´ í”„ë¡œì íŠ¸ì—ì„œ
cd existing-project

# 2. ë°±ì—…
cp -r .venv .venv_backup

# 3. uvë¡œ ì¬ìƒì„±
rm -rf .venv
uv venv
source .venv/bin/activate
uv pip sync requirements.txt

# 4. í…ŒìŠ¤íŠ¸
pytest tests/
```

---

## íŒ€ ê·œì¹™

### 1. ë¸Œëœì¹˜ë³„ ì‘ì—…
```bash
# feature ë¸Œëœì¹˜ ì²´í¬ì•„ì›ƒ í›„
git checkout feature/new-feature
uv pip sync requirements.txt  # í•­ìƒ ì˜ì¡´ì„± ë™ê¸°í™”
```

### 2. ìƒˆ íŒ¨í‚¤ì§€ ì¶”ê°€ ì‹œ
1. íŒ€ ë¦¬ë“œì™€ ë…¼ì˜
2. `pyproject.toml` ë˜ëŠ” `requirements.txt` ì—…ë°ì´íŠ¸
3. PRì— ë³€ê²½ ì‚¬í•­ ëª…ì‹œ

### 3. ë¬¸ì œ ë°œìƒ ì‹œ
1. ì—ëŸ¬ ë©”ì‹œì§€ ìº¡ì²˜
2. `uv --version` ì¶œë ¥ í¬í•¨
3. #dev-help ì±„ë„ì— ê³µìœ 

---

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

### ë¬¸ì„œ
- [uv ê³µì‹ ë¬¸ì„œ](https://github.com/astral-sh/uv)
- [ë‚´ë¶€ ìœ„í‚¤ - uv ê°€ì´ë“œ](https://wiki.company.com/uv)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ](./docs/uv-troubleshooting.md)

### ì§€ì›
- Slack: #uv-support
- ì£¼ê°„ ì˜¤í”¼ìŠ¤ì•„ì›Œ: ëª©ìš”ì¼ ì˜¤í›„ 3ì‹œ
- 1:1 ì§€ì›: dev-support@company.com

---

## ì²´í¬ë¦¬ìŠ¤íŠ¸

ì‹ ê·œ íŒ€ì›ì€ ë‹¤ìŒì„ ì™„ë£Œí•´ì£¼ì„¸ìš”:

- [ ] uv ì„¤ì¹˜ ì™„ë£Œ
- [ ] ì‹¤ìŠµ ì˜ˆì œ 3ê°œ ëª¨ë‘ ì™„ë£Œ
- [ ] IDE ì„¤ì • ì™„ë£Œ
- [ ] ì²« PRì„ uv í™˜ê²½ì—ì„œ ì œì¶œ
- [ ] #uv-users ì±„ë„ ê°€ì…

ì™„ë£Œ í›„ íŒ€ ë¦¬ë“œì—ê²Œ í™•ì¸ë°›ìœ¼ì„¸ìš”.
```

---

## ğŸ“‹ Phase 3: CI/CD íŒŒì´í”„ë¼ì¸ ì „í™˜ (Day 8-12)

### Task 3.1: GitHub Actions ì›Œí¬í”Œë¡œìš° ì—…ë°ì´íŠ¸

#### SubTask 3.1.1: CI íŒŒì´í”„ë¼ì¸ ìˆ˜ì •
**ë‹´ë‹¹ì**: CI/CD ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 3ì‹œê°„

```yaml
# .github/workflows/ci.yml
name: CI Pipeline

on:
  push:
    branches: [main, develop, 'feature/**']
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  UV_CACHE_DIR: ~/.cache/uv
  UV_SYSTEM_PYTHON: 1

jobs:
  # ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬
  quality-checks:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better analysis
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
    
    - name: Cache uv packages
      uses: actions/cache@v3
      with:
        path: |
          ${{ env.UV_CACHE_DIR }}
          .venv
        key: ${{ runner.os }}-uv-${{ hashFiles('**/pyproject.toml', '**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-uv-
    
    - name: Create virtual environment
      run: uv venv
    
    - name: Install dependencies
      run: |
        source .venv/bin/activate
        uv pip install -e ".[dev]"
    
    - name: Run formatters check
      run: |
        source .venv/bin/activate
        black --check .
        isort --check-only .
    
    - name: Run linters
      run: |
        source .venv/bin/activate
        ruff check .
        
    - name: Run type checking
      run: |
        source .venv/bin/activate
        mypy app/
    
    - name: Security scan with Bandit
      run: |
        source .venv/bin/activate
        bandit -r app/ -ll
    
    - name: Check for dependency vulnerabilities
      run: |
        source .venv/bin/activate
        pip install safety
        safety check

  # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
  test:
    name: Test - Python ${{ matrix.python-version }} on ${{ matrix.os }}
    needs: quality-checks
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ['3.11', '3.12']
        exclude:
          - os: macos-latest
            python-version: '3.12'  # Skip macOS + 3.12 for speed
    
    runs-on: ${{ matrix.os }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install uv (Unix)
      if: runner.os != 'Windows'
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
    
    - name: Install uv (Windows)
      if: runner.os == 'Windows'
      run: |
        irm https://astral.sh/uv/install.ps1 | iex
        echo "$env:USERPROFILE\.cargo\bin" | Out-File -FilePath $env:GITHUB_PATH -Encoding utf8 -Append
      shell: pwsh
    
    - name: Cache uv packages
      uses: actions/cache@v3
      with:
        path: |
          ${{ env.UV_CACHE_DIR }}
          .venv
        key: ${{ runner.os }}-${{ matrix.python-version }}-uv-${{ hashFiles('**/pyproject.toml', '**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-${{ matrix.python-version }}-uv-
    
    - name: Create virtual environment
      run: uv venv
    
    - name: Install dependencies (Unix)
      if: runner.os != 'Windows'
      run: |
        source .venv/bin/activate
        uv pip install -e ".[test]"
    
    - name: Install dependencies (Windows)
      if: runner.os == 'Windows'
      run: |
        .venv\Scripts\activate
        uv pip install -e ".[test]"
      shell: pwsh
    
    - name: Run tests with coverage (Unix)
      if: runner.os != 'Windows'
      run: |
        source .venv/bin/activate
        pytest tests/ -v --cov=app --cov-report=xml --cov-report=html --cov-report=term
    
    - name: Run tests with coverage (Windows)
      if: runner.os == 'Windows'
      run: |
        .venv\Scripts\activate
        pytest tests/ -v --cov=app --cov-report=xml --cov-report=html --cov-report=term
      shell: pwsh
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-${{ matrix.os }}-${{ matrix.python-version }}
    
    - name: Upload coverage HTML report
      uses: actions/upload-artifact@v3
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
      with:
        name: coverage-report
        path: htmlcov/

  # í†µí•© í…ŒìŠ¤íŠ¸
  integration-test:
    name: Integration Tests
    needs: test
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
    
    - name: Cache uv packages
      uses: actions/cache@v3
      with:
        path: |
          ${{ env.UV_CACHE_DIR }}
          .venv
        key: ${{ runner.os }}-uv-integration-${{ hashFiles('**/pyproject.toml', '**/requirements*.txt') }}
    
    - name: Install dependencies
      run: |
        uv venv
        source .venv/bin/activate
        uv pip install -e ".[test]"
    
    - name: Run integration tests
      env:
        DATABASE_URL: postgresql://test:test@localhost:5432/test_db
        REDIS_URL: redis://localhost:6379/0
      run: |
        source .venv/bin/activate
        pytest tests/integration/ -v --tb=short

  # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
  benchmark:
    name: Performance Benchmark
    needs: quality-checks
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
    
    - name: Benchmark pip vs uv
      run: |
        # Create test requirements
        cat > benchmark-requirements.txt << EOF
        numpy==1.24.3
        pandas==2.0.3
        scikit-learn==1.3.0
        requests==2.31.0
        fastapi==0.104.1
        pydantic==2.5.0
        sqlalchemy==2.0.23
        EOF
        
        # Benchmark pip
        python -m venv .venv_pip
        source .venv_pip/bin/activate
        echo "Benchmarking pip..."
        PIP_TIME=$(time -p pip install -r benchmark-requirements.txt 2>&1 | grep real | awk '{print $2}')
        deactivate
        rm -rf .venv_pip
        
        # Benchmark uv
        echo "Benchmarking uv..."
        uv venv .venv_uv
        source .venv_uv/bin/activate
        UV_TIME=$(time -p uv pip install -r benchmark-requirements.txt 2>&1 | grep real | awk '{print $2}')
        deactivate
        rm -rf .venv_uv
        
        # Calculate speedup
        SPEEDUP=$(echo "scale=2; $PIP_TIME / $UV_TIME" | bc)
        
        # Create summary
        echo "## Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Tool | Time (seconds) |" >> $GITHUB_STEP_SUMMARY
        echo "|------|----------------|" >> $GITHUB_STEP_SUMMARY
        echo "| pip  | $PIP_TIME |" >> $GITHUB_STEP_SUMMARY
        echo "| uv   | $UV_TIME |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Speedup: ${SPEEDUP}x** ğŸš€" >> $GITHUB_STEP_SUMMARY
    
    - name: Store benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: |
          benchmark-*.txt

  # Docker ë¹Œë“œ í…ŒìŠ¤íŠ¸
  docker-build:
    name: Docker Build Test
    needs: quality-checks
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile.uv
        push: false
        tags: ai-agent-framework:test
        cache-from: type=gha
        cache-to: type=gha,mode=max
        build-args: |
          PYTHON_VERSION=${{ env.PYTHON_VERSION }}
    
    - name: Test Docker image
      run: |
        docker run --rm ai-agent-framework:test python -c "import app; print('Docker image OK')"

  # ìµœì¢… ìƒíƒœ ì²´í¬
  ci-success:
    name: CI Success
    needs: [quality-checks, test, integration-test, docker-build]
    runs-on: ubuntu-latest
    if: success()
    
    steps:
    - name: Success notification
      run: |
        echo "âœ… All CI checks passed successfully!"
        echo "## CI Pipeline Success âœ…" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "All checks have passed:" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… Code quality checks" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… Unit tests (all platforms)" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… Integration tests" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… Docker build" >> $GITHUB_STEP_SUMMARY
```

#### SubTask 3.1.2: CD íŒŒì´í”„ë¼ì¸ ìˆ˜ì •
**ë‹´ë‹¹ì**: ë°°í¬ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```yaml
# .github/workflows/cd.yml
name: CD Pipeline

on:
  push:
    branches: [main]
    tags:
      - 'v*'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}
  PYTHON_VERSION: '3.11'

jobs:
  # ë¹Œë“œ ë° í‘¸ì‹œ
  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=semver,pattern={{major}}
          type=sha,prefix={{branch}}-
    
    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile.uv
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        build-args: |
          PYTHON_VERSION=${{ env.PYTHON_VERSION }}
          BUILD_DATE=${{ github.event.head_commit.timestamp }}
          VCS_REF=${{ github.sha }}
    
    - name: Generate SBOM
      uses: anchore/sbom-action@v0
      with:
        image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build.outputs.digest }}
        format: spdx-json
        output-file: sbom.spdx.json
    
    - name: Upload SBOM
      uses: actions/upload-artifact@v3
      with:
        name: sbom
        path: sbom.spdx.json

  # ìŠ¤í…Œì´ì§• ë°°í¬
  deploy-staging:
    name: Deploy to Staging
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.event.inputs.environment == 'staging'
    environment:
      name: staging
      url: https://staging.ai-agent-framework.com
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Install kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-1
    
    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --name staging-cluster --region us-east-1
    
    - name: Deploy to Kubernetes
      run: |
        # Update image in deployment
        kubectl set image deployment/agent-api \
          agent-api=${{ needs.build-and-push.outputs.image-tag }} \
          -n staging
        
        # Wait for rollout
        kubectl rollout status deployment/agent-api -n staging --timeout=5m
        
        # Verify deployment
        kubectl get pods -n staging -l app=agent-api
    
    - name: Run smoke tests
      run: |
        # Wait for service to be ready
        sleep 30
        
        # Run smoke tests
        curl -f https://staging.ai-agent-framework.com/health || exit 1
        curl -f https://staging.ai-agent-framework.com/api/v1/status || exit 1
    
    - name: Notify deployment
      uses: 8398a7/action-slack@v3
      if: always()
      with:
        status: ${{ job.status }}
        text: |
          Staging Deployment ${{ job.status }}
          Image: ${{ needs.build-and-push.outputs.image-tag }}
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}

  # í”„ë¡œë•ì…˜ ë°°í¬
  deploy-production:
    name: Deploy to Production
    needs: [build-and-push, deploy-staging]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/v') || github.event.inputs.environment == 'production'
    environment:
      name: production
      url: https://ai-agent-framework.com
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_PROD }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_PROD }}
        aws-region: us-east-1
    
    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --name production-cluster --region us-east-1
    
    - name: Create backup
      run: |
        # Backup current deployment
        kubectl get deployment agent-api -n production -o yaml > backup-deployment.yaml
        kubectl get service agent-api -n production -o yaml > backup-service.yaml
        
        # Upload backups
        aws s3 cp backup-deployment.yaml s3://prod-backups/deployments/$(date +%Y%m%d-%H%M%S)-deployment.yaml
        aws s3 cp backup-service.yaml s3://prod-backups/deployments/$(date +%Y%m%d-%H%M%S)-service.yaml
    
    - name: Deploy to Production (Canary)
      run: |
        # Deploy canary version (10% traffic)
        kubectl apply -f - <<EOF
        apiVersion: v1
        kind: Service
        metadata:
          name: agent-api-canary
          namespace: production
        spec:
          selector:
            app: agent-api
            version: canary
          ports:
            - port: 80
              targetPort: 8000
        ---
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: agent-api-canary
          namespace: production
        spec:
          replicas: 2
          selector:
            matchLabels:
              app: agent-api
              version: canary
          template:
            metadata:
              labels:
                app: agent-api
                version: canary
            spec:
              containers:
              - name: agent-api
                image: ${{ needs.build-and-push.outputs.image-tag }}
                ports:
                - containerPort: 8000
                env:
                - name: ENV
                  value: "production"
                livenessProbe:
                  httpGet:
                    path: /health
                    port: 8000
                  initialDelaySeconds: 30
                  periodSeconds: 10
                readinessProbe:
                  httpGet:
                    path: /ready
                    port: 8000
                  initialDelaySeconds: 5
                  periodSeconds: 5
        EOF
        
        # Wait for canary rollout
        kubectl rollout status deployment/agent-api-canary -n production --timeout=5m
    
    - name: Monitor canary metrics
      run: |
        # Monitor for 5 minutes
        echo "Monitoring canary deployment for 5 minutes..."
        sleep 300
        
        # Check error rates
        # This would typically query your monitoring system
        # For now, we'll do a simple health check
        CANARY_HEALTH=$(curl -s -o /dev/null -w "%{http_code}" https://ai-agent-framework.com/health)
        
        if [ "$CANARY_HEALTH" != "200" ]; then
          echo "Canary health check failed!"
          exit 1
        fi
    
    - name: Promote to full production
      run: |
        # Update main deployment
        kubectl set image deployment/agent-api \
          agent-api=${{ needs.build-and-push.outputs.image-tag }} \
          -n production
        
        # Wait for rollout
        kubectl rollout status deployment/agent-api -n production --timeout=10m
        
        # Remove canary
        kubectl delete deployment agent-api-canary -n production
        kubectl delete service agent-api-canary -n production
    
    - name: Verify production deployment
      run: |
        # Run production tests
        curl -f https://ai-agent-framework.com/health || exit 1
        curl -f https://ai-agent-framework.com/api/v1/status || exit 1
        
        # Run more comprehensive tests
        python scripts/production_tests.py
    
    - name: Create release
      uses: actions/create-release@v1
      if: startsWith(github.ref, 'refs/tags/v')
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        tag_name: ${{ github.ref }}
        release_name: Release ${{ github.ref }}
        body: |
          ## Changes in this release
          - Docker image: ${{ needs.build-and-push.outputs.image-tag }}
          - Digest: ${{ needs.build-and-push.outputs.image-digest }}
          
          ## Deployment notes
          - Deployed to production with canary rollout
          - All health checks passed
        draft: false
        prerelease: false
    
    - name: Notify deployment
      uses: 8398a7/action-slack@v3
      if: always()
      with:
        status: ${{ job.status }}
        text: |
          Production Deployment ${{ job.status }}
          Version: ${{ github.ref }}
          Image: ${{ needs.build-and-push.outputs.image-tag }}
        webhook_url: ${{ secrets.SLACK_WEBHOOK_PROD }}

  # ë¡¤ë°± ì‘ì—… (ìˆ˜ë™ íŠ¸ë¦¬ê±°)
  rollback:
    name: Rollback Deployment
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch'
    environment:
      name: ${{ github.event.inputs.environment }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-1
    
    - name: Rollback deployment
      run: |
        NAMESPACE=${{ github.event.inputs.environment }}
        
        # Get previous revision
        PREVIOUS_REVISION=$(kubectl rollout history deployment/agent-api -n $NAMESPACE | tail -2 | head -1 | awk '{print $1}')
        
        # Rollback
        kubectl rollout undo deployment/agent-api -n $NAMESPACE --to-revision=$PREVIOUS_REVISION
        
        # Wait for rollback
        kubectl rollout status deployment/agent-api -n $NAMESPACE --timeout=5m
        
        echo "Rolled back to revision $PREVIOUS_REVISION"
```

#### SubTask 3.1.3: ìºì‹± ì „ëµ ìµœì í™”
**ë‹´ë‹¹ì**: ì„±ëŠ¥ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 1ì‹œê°„

```yaml
# .github/workflows/cache-strategy.yml
name: Cache Strategy Test

on:
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * 0'  # Weekly on Sunday at 2 AM

env:
  UV_CACHE_DIR: ~/.cache/uv

jobs:
  test-cache-strategies:
    name: Test Cache Strategy - ${{ matrix.strategy }}
    runs-on: ubuntu-latest
    strategy:
      matrix:
        strategy:
          - name: 'full-cache'
            cache-key: 'uv-full-${{ hashFiles(''**/pyproject.toml'', ''**/requirements*.txt'') }}'
            cache-paths: |
              ~/.cache/uv
              .venv
          
          - name: 'uv-only'
            cache-key: 'uv-only-${{ hashFiles(''**/pyproject.toml'', ''**/requirements*.txt'') }}'
            cache-paths: |
              ~/.cache/uv
          
          - name: 'split-cache'
            cache-key: 'uv-split'
            cache-paths: ''  # Will use multiple caches
          
          - name: 'no-cache'
            cache-key: ''
            cache-paths: ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
    
    # Strategy: Full cache
    - name: Cache (full-cache)
      if: matrix.strategy.name == 'full-cache'
      uses: actions/cache@v3
      with:
        path: ${{ matrix.strategy.cache-paths }}
        key: ${{ matrix.strategy.cache-key }}
        restore-keys: |
          uv-full-
    
    # Strategy: uv only
    - name: Cache (uv-only)
      if: matrix.strategy.name == 'uv-only'
      uses: actions/cache@v3
      with:
        path: ${{ matrix.strategy.cache-paths }}
        key: ${{ matrix.strategy.cache-key }}
        restore-keys: |
          uv-only-
    
    # Strategy: Split cache
    - name: Cache uv packages (split-cache)
      if: matrix.strategy.name == 'split-cache'
      uses: actions/cache@v3
      with:
        path: ~/.cache/uv
        key: uv-packages-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          uv-packages-
    
    - name: Cache venv (split-cache)
      if: matrix.strategy.name == 'split-cache'
      uses: actions/cache@v3
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          venv-${{ runner.os }}-
    
    # Measure performance
    - name: Measure installation time
      run: |
        # Clear any existing venv
        rm -rf .venv
        
        # Record start time
        START_TIME=$(date +%s.%N)
        
        # Create venv and install
        uv venv
        source .venv/bin/activate
        uv pip install -e ".[dev,test]"
        
        # Record end time
        END_TIME=$(date +%s.%N)
        
        # Calculate duration
        DURATION=$(echo "$END_TIME - $START_TIME" | bc)
        
        # Get cache sizes
        UV_CACHE_SIZE=$(du -sh ~/.cache/uv 2>/dev/null | cut -f1 || echo "0")
        VENV_SIZE=$(du -sh .venv 2>/dev/null | cut -f1 || echo "0")
        
        # Output results
        echo "## Cache Strategy: ${{ matrix.strategy.name }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
        echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
        echo "| Installation Time | ${DURATION}s |" >> $GITHUB_STEP_SUMMARY
        echo "| UV Cache Size | $UV_CACHE_SIZE |" >> $GITHUB_STEP_SUMMARY
        echo "| Venv Size | $VENV_SIZE |" >> $GITHUB_STEP_SUMMARY
        echo "| Total Packages | $(uv pip list | wc -l) |" >> $GITHUB_STEP_SUMMARY
        
        # Save results for comparison
        echo "${{ matrix.strategy.name }},$DURATION,$UV_CACHE_SIZE,$VENV_SIZE" >> results.csv
    
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: cache-strategy-results-${{ matrix.strategy.name }}
        path: results.csv

  # ê²°ê³¼ ë¶„ì„
  analyze-results:
    name: Analyze Cache Strategy Results
    needs: test-cache-strategies
    runs-on: ubuntu-latest
    
    steps:
    - name: Download all results
      uses: actions/download-artifact@v3
      with:
        path: results
    
    - name: Analyze and create report
      run: |
        # Combine all results
        echo "Strategy,Duration,UV_Cache,Venv_Size" > combined_results.csv
        cat results/*/results.csv >> combined_results.csv
        
        # Create analysis script
        cat > analyze.py << 'EOF'
        import csv
        import statistics
        
        results = []
        with open('combined_results.csv', 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                results.append({
                    'strategy': row['Strategy'],
                    'duration': float(row['Duration']),
                    'uv_cache': row['UV_Cache'],
                    'venv_size': row['Venv_Size']
                })
        
        # Sort by duration
        results.sort(key=lambda x: x['duration'])
        
        print("# Cache Strategy Analysis")
        print()
        print("## Performance Ranking")
        print()
        print("| Rank | Strategy | Duration | UV Cache | Venv Size |")
        print("|------|----------|----------|----------|-----------|")
        
        for i, r in enumerate(results, 1):
            print(f"| {i} | {r['strategy']} | {r['duration']:.2f}s | {r['uv_cache']} | {r['venv_size']} |")
        
        print()
        print("## Recommendations")
        print()
        
        best = results[0]
        if best['strategy'] == 'full-cache':
            print("âœ… **Full cache strategy is recommended** - Caching both uv packages and venv provides the best performance.")
        elif best['strategy'] == 'uv-only':
            print("âœ… **UV-only cache strategy is recommended** - Caching only uv packages provides good performance with less storage.")
        elif best['strategy'] == 'split-cache':
            print("âœ… **Split cache strategy is recommended** - Separate caches for uv and venv provide flexibility.")
        
        # Calculate speedup
        no_cache = next((r for r in results if r['strategy'] == 'no-cache'), None)
        if no_cache and best['strategy'] != 'no-cache':
            speedup = no_cache['duration'] / best['duration']
            print(f"\n**Speedup: {speedup:.1f}x** compared to no cache")
        EOF
        
        python analyze.py >> $GITHUB_STEP_SUMMARY
        
        # Save full report
        python analyze.py > cache_strategy_report.md
    
    - name: Upload analysis report
      uses: actions/upload-artifact@v3
      with:
        name: cache-strategy-analysis
        path: |
          combined_results.csv
          cache_strategy_report.md
    
    - name: Update cache strategy documentation
      run: |
        echo "Based on the analysis, update .github/workflows/README.md with the recommended cache strategy."
```

### Task 3.2: Docker ì´ë¯¸ì§€ ìµœì í™”

#### SubTask 3.2.1: Multi-stage Dockerfile ì‘ì„±
**ë‹´ë‹¹ì**: ì»¨í…Œì´ë„ˆ ì „ë¬¸ê°€  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 3ì‹œê°„

```dockerfile
# Dockerfile.uv - Optimized multi-stage build with uv
# Build with: docker build -f Dockerfile.uv -t ai-agent-framework:latest .

# Arguments
ARG PYTHON_VERSION=3.11
ARG UV_VERSION=0.1.0

# Stage 1: UV installer
FROM alpine:3.19 AS uv-installer

RUN apk add --no-cache curl
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
RUN mv /root/.cargo/bin/uv /usr/local/bin/uv

# Stage 2: Python dependencies builder
FROM python:${PYTHON_VERSION}-slim AS builder

# Install system dependencies for building
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Copy uv from installer stage
COPY --from=uv-installer /usr/local/bin/uv /usr/local/bin/uv

# Set working directory
WORKDIR /app

# Copy dependency files
COPY pyproject.toml .
COPY requirements*.txt ./

# Create virtual environment and install dependencies
ENV VIRTUAL_ENV=/opt/venv
RUN uv venv $VIRTUAL_ENV
ENV PATH="$VIRTUAL_ENV/bin:$PATH"

# Install dependencies
RUN uv pip install --no-cache -r requirements.txt

# Stage 3: Development image (optional)
FROM builder AS development

# Install dev dependencies
COPY requirements-dev.txt .
RUN uv pip install --no-cache -r requirements-dev.txt

# Copy application code
COPY . .

# Install package in editable mode
RUN uv pip install -e .

# Development command
CMD ["uvicorn", "app.main:app", "--reload", "--host", "0.0.0.0", "--port", "8000"]

# Stage 4: Production runtime
FROM python:${PYTHON_VERSION}-slim AS runtime

# Install only runtime dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    libpq5 \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user
RUN groupadd -r appuser && useradd -r -g appuser appuser

# Copy virtual environment from builder
COPY --from=builder /opt/venv /opt/venv

# Set environment variables
ENV VIRTUAL_ENV=/opt/venv \
    PATH="/opt/venv/bin:$PATH" \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONHASHSEED=random \
    PIP_NO_CACHE_DIR=off \
    PIP_DISABLE_PIP_VERSION_CHECK=on \
    PIP_DEFAULT_TIMEOUT=100

# Set working directory
WORKDIR /app

# Copy application code
COPY --chown=appuser:appuser . .

# Create necessary directories
RUN mkdir -p /app/logs /app/data && \
    chown -R appuser:appuser /app

# Switch to non-root user
USER appuser

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Default command
CMD ["gunicorn", "app.main:app", \
     "--workers", "4", \
     "--worker-class", "uvicorn.workers.UvicornWorker", \
     "--bind", "0.0.0.0:8000", \
     "--access-logfile", "-", \
     "--error-logfile", "-"]

# Stage 5: Test runner
FROM development AS test

# Run tests
RUN pytest tests/ -v --cov=app --cov-report=term-missing

# Stage 6: Production with monitoring
FROM runtime AS production-monitored

# Install monitoring agents (example with Datadog)
USER root
RUN apt-get update && apt-get install -y --no-install-recommends \
    datadog-agent \
    && rm -rf /var/lib/apt/lists/*

# Copy monitoring configs
COPY monitoring/datadog.yaml /etc/datadog-agent/datadog.yaml

USER appuser

# Start script that runs both the app and monitoring
COPY scripts/start-with-monitoring.sh /app/
CMD ["/app/start-with-monitoring.sh"]
```

```yaml
# docker-compose.yml - Development environment with uv
version: '3.8'

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile.uv
      target: development
    volumes:
      - .:/app
      - uv-cache:/root/.cache/uv
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:password@db:5432/app_db
      - REDIS_URL=redis://redis:6379/0
      - ENVIRONMENT=development
    depends_on:
      - db
      - redis
    command: uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

  db:
    image: postgres:15-alpine
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=app_db
    volumes:
      - postgres-data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data
    ports:
      - "6379:6379"

  # Development tools
  pgadmin:
    image: dpage/pgadmin4:latest
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@example.com
      - PGADMIN_DEFAULT_PASSWORD=admin
    ports:
      - "5050:80"
    depends_on:
      - db

volumes:
  postgres-data:
  redis-data:
  uv-cache:
```

#### SubTask 3.2.2: ì´ë¯¸ì§€ í¬ê¸° ìµœì í™”
**ë‹´ë‹¹ì**: ì¸í”„ë¼ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```bash
#!/bin/bash
# scripts/optimize-docker-image.sh

set -e

echo "ğŸ³ Docker Image Optimization Script"
echo "=================================="

# Variables
IMAGE_NAME="ai-agent-framework"
ORIGINAL_TAG="${IMAGE_NAME}:original"
OPTIMIZED_TAG="${IMAGE_NAME}:optimized"

# Build original image for comparison
echo "ğŸ“¦ Building original image with pip..."
cat > Dockerfile.pip << 'EOF'
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
EOF

docker build -f Dockerfile.pip -t $ORIGINAL_TAG .

# Build optimized image with uv
echo "ğŸš€ Building optimized image with uv..."
docker build -f Dockerfile.uv -t $OPTIMIZED_TAG .

# Analyze image sizes
echo -e "\nğŸ“Š Image Size Comparison:"
echo "========================"

ORIGINAL_SIZE=$(docker image inspect $ORIGINAL_TAG --format='{{.Size}}' | numfmt --to=iec)
OPTIMIZED_SIZE=$(docker image inspect $OPTIMIZED_TAG --format='{{.Size}}' | numfmt --to=iec)

echo "Original (pip): $ORIGINAL_SIZE"
echo "Optimized (uv): $OPTIMIZED_SIZE"

# Use dive for detailed analysis
echo -e "\nğŸ” Detailed layer analysis with dive..."
if command -v dive &> /dev/null; then
    echo "Original image layers:"
    dive $ORIGINAL_TAG --ci --highestWastedBytes=0.1 --lowestEfficiency=0.9 || true
    
    echo -e "\nOptimized image layers:"
    dive $OPTIMIZED_TAG --ci --highestWastedBytes=0.1 --lowestEfficiency=0.9 || true
else
    echo "â„¹ï¸  Install dive for detailed analysis: https://github.com/wagoodman/dive"
fi

# Security scanning
echo -e "\nğŸ”’ Security scanning..."
if command -v trivy &> /dev/null; then
    echo "Scanning optimized image:"
    trivy image --severity HIGH,CRITICAL $OPTIMIZED_TAG
else
    echo "â„¹ï¸  Install trivy for security scanning: https://github.com/aquasecurity/trivy"
fi

# Create optimization report
cat > docker-optimization-report.md << EOF
# Docker Image Optimization Report

Generated: $(date)

## Size Comparison

| Image | Size |
|-------|------|
| Original (pip) | $ORIGINAL_SIZE |
| Optimized (uv) | $OPTIMIZED_SIZE |

## Optimization Techniques Used

1. **Multi-stage builds** - Separate build and runtime stages
2. **uv for faster installs** - Reduced build time and layer size
3. **Minimal base image** - python:3.11-slim instead of full image
4. **Non-root user** - Security best practice
5. **Layer caching** - Optimized COPY order for better caching

## Build Time Comparison

\`\`\`bash
# Measure build times
time docker build -f Dockerfile.pip -t test:pip . --no-cache
time docker build -f Dockerfile.uv -t test:uv . --no-cache
\`\`\`

## Recommendations

1. Use \`.dockerignore\` to exclude unnecessary files
2. Combine RUN commands where possible
3. Order COPY commands from least to most frequently changed
4. Use specific version tags instead of \`latest\`
5. Regularly update base images for security patches

## Next Steps

1. Set up automated image scanning in CI/CD
2. Implement image signing for supply chain security
3. Use distroless images for even smaller size
4. Consider using BuildKit cache mounts for pip/uv cache
EOF

echo -e "\nâœ… Optimization report saved to docker-optimization-report.md"

# Create .dockerignore if it doesn't exist
if [ ! -f .dockerignore ]; then
    echo "ğŸ“ Creating .dockerignore..."
    cat > .dockerignore << 'EOF'
# Git
.git
.gitignore
.github

# Python
__pycache__
*.pyc
*.pyo
*.pyd
.Python
*.egg-info
.pytest_cache
.mypy_cache
.ruff_cache
.coverage
htmlcov
.tox

# Virtual environments
venv
.venv
env
.env.*

# IDE
.vscode
.idea
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# Project specific
logs
*.log
tests
docs
scripts
*.md
!README.md
docker-compose*.yml
Dockerfile*
!requirements*.txt

# Backups
*.backup
*.bak
*~
EOF
    echo "âœ… .dockerignore created"
fi

# Generate BuildKit optimization example
cat > Dockerfile.buildkit << 'EOF'
# syntax=docker/dockerfile:1.4
# Dockerfile with BuildKit optimizations

ARG PYTHON_VERSION=3.11
FROM python:${PYTHON_VERSION}-slim AS builder

# Mount cache for uv
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=requirements.txt,target=requirements.txt \
    uv pip install --prefix=/install -r requirements.txt

FROM python:${PYTHON_VERSION}-slim AS runtime

COPY --from=builder /install /usr/local

WORKDIR /app
COPY . .

# Run as non-root
USER nobody

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
EOF

echo -e "\nğŸ’¡ BuildKit example saved to Dockerfile.buildkit"
echo "   Build with: DOCKER_BUILDKIT=1 docker build -f Dockerfile.buildkit ."

# Cleanup
rm -f Dockerfile.pip
```

#### SubTask 3.2.3: ë ˆì§€ìŠ¤íŠ¸ë¦¬ í†µí•©
**ë‹´ë‹¹ì**: DevOps ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```bash
#!/bin/bash
# scripts/registry-integration.sh

set -e

# Configuration
REGISTRY_URL="${DOCKER_REGISTRY:-ghcr.io}"
ORGANIZATION="${DOCKER_ORG:-your-org}"
REPOSITORY="${DOCKER_REPO:-ai-agent-framework}"
FULL_IMAGE_NAME="${REGISTRY_URL}/${ORGANIZATION}/${REPOSITORY}"

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

echo "ğŸš€ Docker Registry Integration Setup"
echo "===================================="

# Function to check command exists
check_command() {
    if ! command -v $1 &> /dev/null; then
        echo -e "${RED}âŒ $1 is not installed${NC}"
        return 1
    fi
    echo -e "${GREEN}âœ… $1 is installed${NC}"
    return 0
}

# Check prerequisites
echo -e "\nğŸ“‹ Checking prerequisites..."
check_command docker
check_command jq

# Registry login
echo -e "\nğŸ” Registry Authentication"
case $REGISTRY_URL in
    "ghcr.io")
        echo "GitHub Container Registry detected"
        echo "Please set GITHUB_TOKEN environment variable"
        if [ -n "$GITHUB_TOKEN" ]; then
            echo $GITHUB_TOKEN | docker login ghcr.io -u $GITHUB_USER --password-stdin
        fi
        ;;
    "docker.io")
        echo "Docker Hub detected"
        docker login
        ;;
    *)
        echo "Custom registry: $REGISTRY_URL"
        docker login $REGISTRY_URL
        ;;
esac

# Create build and push script
cat > .github/scripts/docker-build-push.sh << 'EOF'
#!/bin/bash
set -e

# Arguments
TAG=${1:-latest}
PLATFORMS=${2:-linux/amd64,linux/arm64}

# Build metadata
BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ')
VCS_REF=$(git rev-parse --short HEAD)
VERSION=$(git describe --tags --always)

# Enable BuildKit
export DOCKER_BUILDKIT=1

echo "ğŸ—ï¸  Building multi-platform image..."
docker buildx create --use --name multibuilder || true

docker buildx build \
    --platform $PLATFORMS \
    --build-arg BUILD_DATE=$BUILD_DATE \
    --build-arg VCS_REF=$VCS_REF \
    --build-arg VERSION=$VERSION \
    --tag $FULL_IMAGE_NAME:$TAG \
    --tag $FULL_IMAGE_NAME:$VERSION \
    --push \
    -f Dockerfile.uv \
    .

echo "âœ… Image pushed to registry"
EOF

chmod +x .github/scripts/docker-build-push.sh

# Create GitHub Actions workflow for automated builds
cat > .github/workflows/docker-publish.yml << 'EOF'
name: Docker Publish

on:
  push:
    branches: [main]
    tags:
      - 'v*'
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Set up QEMU
      uses: docker/setup-qemu-action@v3

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./Dockerfile.uv
        platforms: linux/amd64,linux/arm64
        push: ${{ github.event_name != 'pull_request' }}
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        build-args: |
          BUILD_DATE=${{ github.event.head_commit.timestamp }}
          VCS_REF=${{ github.sha }}
          VERSION=${{ steps.meta.outputs.version }}

    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.meta.outputs.version }}
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v3
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'
EOF

# Create Makefile targets for Docker operations
cat >> Makefile << 'EOF'

# Docker targets
.PHONY: docker-build
docker-build:
	@echo "$(GREEN)Building Docker image...$(NC)"
	@docker build -f Dockerfile.uv -t $(REPOSITORY):latest .

.PHONY: docker-run
docker-run:
	@echo "$(GREEN)Running Docker container...$(NC)"
	@docker run -it --rm \
		-p 8000:8000 \
		-v $(PWD):/app \
		--env-file .env \
		$(REPOSITORY):latest

.PHONY: docker-push
docker-push:
	@echo "$(GREEN)Pushing to registry...$(NC)"
	@.github/scripts/docker-build-push.sh latest

.PHONY: docker-scan
docker-scan:
	@echo "$(GREEN)Scanning for vulnerabilities...$(NC)"
	@trivy image $(REPOSITORY):latest

.PHONY: docker-clean
docker-clean:
	@echo "$(GREEN)Cleaning up Docker resources...$(NC)"
	@docker system prune -af --volumes
EOF

# Create container registry documentation
cat > docs/container-registry.md << EOF
# Container Registry Setup

## Overview

This project uses container images built with uv for faster builds and smaller image sizes.

## Registry Configuration

- **Registry**: $REGISTRY_URL
- **Organization**: $ORGANIZATION
- **Repository**: $REPOSITORY
- **Full Image**: $FULL_IMAGE_NAME

## Authentication

### GitHub Container Registry (ghcr.io)
\`\`\`bash
export GITHUB_TOKEN=your_token
echo \$GITHUB_TOKEN | docker login ghcr.io -u USERNAME --password-stdin
\`\`\`

### Docker Hub
\`\`\`bash
docker login
\`\`\`

### Custom Registry
\`\`\`bash
docker login $REGISTRY_URL
\`\`\`

## Building Images

### Local Build
\`\`\`bash
make docker-build
\`\`\`

### Multi-platform Build
\`\`\`bash
docker buildx build --platform linux/amd64,linux/arm64 -t $FULL_IMAGE_NAME:latest .
\`\`\`

## Pushing Images

### Manual Push
\`\`\`bash
make docker-push
\`\`\`

### Automated Push
Images are automatically built and pushed on:
- Push to main branch
- New tags (v*)

## Using Images

### Pull Latest
\`\`\`bash
docker pull $FULL_IMAGE_NAME:latest
\`\`\`

### Run Container
\`\`\`bash
docker run -p 8000:8000 $FULL_IMAGE_NAME:latest
\`\`\`

### Docker Compose
\`\`\`yaml
services:
  app:
    image: $FULL_IMAGE_NAME:latest
    ports:
      - "8000:8000"
\`\`\`

## Image Tags

- \`latest\` - Latest build from main branch
- \`v1.2.3\` - Specific version release
- \`sha-xxxxx\` - Specific commit
- \`pr-123\` - Pull request build

## Security

### Vulnerability Scanning
\`\`\`bash
make docker-scan
\`\`\`

### Image Signing
\`\`\`bash
cosign sign $FULL_IMAGE_NAME:latest
\`\`\`

## Troubleshooting

### Push Denied
- Check authentication: \`docker login\`
- Verify permissions in registry
- Check organization membership

### Build Failures
- Clear Docker cache: \`docker builder prune\`
- Check Dockerfile syntax
- Verify base image availability

### Large Image Size
- Use multi-stage builds
- Minimize layers
- Use .dockerignore
- Check for unnecessary dependencies
EOF

echo -e "\nâœ… Registry integration complete!"
echo -e "\nğŸ“‹ Next steps:"
echo "1. Update DOCKER_REGISTRY, DOCKER_ORG, DOCKER_REPO in your environment"
echo "2. Set up registry authentication in CI/CD"
echo "3. Test the build and push workflow"
echo "4. Enable vulnerability scanning"
```

### Task 3.3: ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ì—…ë°ì´íŠ¸

#### SubTask 3.3.1: ë°°í¬ ìë™í™” ìŠ¤í¬ë¦½íŠ¸
**ë‹´ë‹¹ì**: ë°°í¬ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 3ì‹œê°„

```bash
#!/bin/bash
# scripts/deploy.sh - Automated deployment script with uv

set -euo pipefail

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
DEPLOYMENT_CONFIG="${DEPLOYMENT_CONFIG:-deployment.yaml}"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# Functions
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

show_help() {
    cat << EOF
Usage: $0 [COMMAND] [OPTIONS]

Commands:
    deploy      Deploy application to specified environment
    rollback    Rollback to previous version
    status      Check deployment status
    logs        View deployment logs
    shell       Open shell in deployment environment

Options:
    -e, --environment   Environment (staging/production)
    -v, --version      Version to deploy
    -f, --force        Force deployment without confirmations
    -n, --dry-run      Show what would be deployed
    -h, --help         Show this help message

Examples:
    $0 deploy -e staging -v v1.2.3
    $0 rollback -e production
    $0 status -e staging
EOF
}

# Parse arguments
COMMAND=""
ENVIRONMENT=""
VERSION=""
FORCE=false
DRY_RUN=false

while [[ $# -gt 0 ]]; do
    case $1 in
        deploy|rollback|status|logs|shell)
            COMMAND=$1
            shift
            ;;
        -e|--environment)
            ENVIRONMENT="$2"
            shift 2
            ;;
        -v|--version)
            VERSION="$2"
            shift 2
            ;;
        -f|--force)
            FORCE=true
            shift
            ;;
        -n|--dry-run)
            DRY_RUN=true
            shift
            ;;
        -h|--help)
            show_help
            exit 0
            ;;
        *)
            log_error "Unknown option: $1"
            show_help
            exit 1
            ;;
    esac
done

# Validate inputs
if [ -z "$COMMAND" ]; then
    log_error "No command specified"
    show_help
    exit 1
fi

if [ -z "$ENVIRONMENT" ]; then
    log_error "Environment not specified"
    exit 1
fi

# Load deployment configuration
load_config() {
    local config_file="$PROJECT_ROOT/deployments/$ENVIRONMENT/$DEPLOYMENT_CONFIG"
    
    if [ ! -f "$config_file" ]; then
        log_error "Configuration file not found: $config_file"
        exit 1
    fi
    
    log_info "Loading configuration from $config_file"
    
    # Parse YAML configuration
    eval $(python3 -c "
import yaml
with open('$config_file', 'r') as f:
    config = yaml.safe_load(f)
    for key, value in config.items():
        if isinstance(value, dict):
            for k, v in value.items():
                print(f'{key.upper()}_{k.upper()}=\"{v}\"')
        else:
            print(f'{key.upper()}=\"{value}\"')
    ")
}

# Pre-deployment checks
pre_deploy_checks() {
    log_info "Running pre-deployment checks..."
    
    # Check uv installation
    if ! command -v uv &> /dev/null; then
        log_error "uv is not installed"
        exit 1
    fi
    
    # Check Docker
    if ! docker info &> /dev/null; then
        log_error "Docker is not running"
        exit 1
    fi
    
    # Check kubectl (for Kubernetes deployments)
    if [ "$DEPLOYMENT_TYPE" = "kubernetes" ]; then
        if ! command -v kubectl &> /dev/null; then
            log_error "kubectl is not installed"
            exit 1
        fi
        
        # Check cluster connectivity
        if ! kubectl cluster-info &> /dev/null; then
            log_error "Cannot connect to Kubernetes cluster"
            exit 1
        fi
    fi
    
    # Check AWS CLI (for AWS deployments)
    if [ "$DEPLOYMENT_TYPE" = "aws" ]; then
        if ! command -v aws &> /dev/null; then
            log_error "AWS CLI is not installed"
            exit 1
        fi
        
        # Check AWS credentials
        if ! aws sts get-caller-identity &> /dev/null; then
            log_error "AWS credentials not configured"
            exit 1
        fi
    fi
    
    log_success "Pre-deployment checks passed"
}

# Build application
build_application() {
    log_info "Building application with uv..."
    
    cd "$PROJECT_ROOT"
    
    # Create virtual environment
    if [ ! -d ".venv" ]; then
        uv venv
    fi
    
    # Activate and install dependencies
    source .venv/bin/activate
    uv pip sync requirements.txt
    
    # Run tests
    if [ "$RUN_TESTS" = "true" ] && [ "$DRY_RUN" = false ]; then
        log_info "Running tests..."
        pytest tests/ -v --tb=short || {
            log_error "Tests failed"
            exit 1
        }
    fi
    
    # Build Docker image
    local image_tag="${DOCKER_REGISTRY}/${DOCKER_IMAGE}:${VERSION:-latest}"
    
    log_info "Building Docker image: $image_tag"
    
    if [ "$DRY_RUN" = false ]; then
        docker build -f Dockerfile.uv -t "$image_tag" . || {
            log_error "Docker build failed"
            exit 1
        }
        
        # Push to registry
        if [ "$PUSH_IMAGE" = "true" ]; then
            log_info "Pushing image to registry..."
            docker push "$image_tag"
        fi
    fi
    
    log_success "Build completed"
}

# Deploy to Kubernetes
deploy_kubernetes() {
    local namespace="${KUBERNETES_NAMESPACE:-default}"
    local deployment="${KUBERNETES_DEPLOYMENT:-app}"
    local image_tag="${DOCKER_REGISTRY}/${DOCKER_IMAGE}:${VERSION:-latest}"
    
    log_info "Deploying to Kubernetes cluster..."
    log_info "Namespace: $namespace"
    log_info "Deployment: $deployment"
    log_info "Image: $image_tag"
    
    if [ "$DRY_RUN" = true ]; then
        log_warning "DRY RUN - No actual deployment will occur"
        kubectl set image deployment/$deployment \
            $deployment=$image_tag \
            -n $namespace \
            --dry-run=client -o yaml
        return
    fi
    
    # Create namespace if not exists
    kubectl create namespace $namespace --dry-run=client -o yaml | kubectl apply -f -
    
    # Apply configurations
    if [ -d "$PROJECT_ROOT/deployments/$ENVIRONMENT/k8s" ]; then
        log_info "Applying Kubernetes configurations..."
        kubectl apply -f "$PROJECT_ROOT/deployments/$ENVIRONMENT/k8s/" -n $namespace
    fi
    
    # Update deployment image
    kubectl set image deployment/$deployment \
        $deployment=$image_tag \
        -n $namespace \
        --record
    
    # Wait for rollout
    log_info "Waiting for rollout to complete..."
    kubectl rollout status deployment/$deployment -n $namespace --timeout=10m
    
    # Verify deployment
    local ready_replicas=$(kubectl get deployment $deployment -n $namespace -o jsonpath='{.status.readyReplicas}')
    local desired_replicas=$(kubectl get deployment $deployment -n $namespace -o jsonpath='{.spec.replicas}')
    
    if [ "$ready_replicas" = "$desired_replicas" ]; then
        log_success "Deployment successful! $ready_replicas/$desired_replicas replicas ready"
    else
        log_error "Deployment failed! Only $ready_replicas/$desired_replicas replicas ready"
        exit 1
    fi
}

# Deploy to AWS ECS
deploy_aws_ecs() {
    local cluster="${AWS_ECS_CLUSTER}"
    local service="${AWS_ECS_SERVICE}"
    local task_definition="${AWS_ECS_TASK_DEFINITION}"
    local image_tag="${DOCKER_REGISTRY}/${DOCKER_IMAGE}:${VERSION:-latest}"
    
    log_info "Deploying to AWS ECS..."
    log_info "Cluster: $cluster"
    log_info "Service: $service"
    log_info "Image: $image_tag"
    
    if [ "$DRY_RUN" = true ]; then
        log_warning "DRY RUN - No actual deployment will occur"
        return
    fi
    
    # Update task definition with new image
    local task_def_json=$(aws ecs describe-task-definition --task-definition $task_definition)
    local new_task_def=$(echo $task_def_json | jq --arg IMAGE "$image_tag" '.taskDefinition | .containerDefinitions[0].image = $IMAGE | del(.taskDefinitionArn) | del(.revision) | del(.status) | del(.requiresAttributes) | del(.compatibilities) | del(.registeredAt) | del(.registeredBy)')
    
    # Register new task definition
    local new_task_arn=$(echo $new_task_def | aws ecs register-task-definition --cli-input-json file:///dev/stdin --query 'taskDefinition.taskDefinitionArn' --output text)
    
    # Update service
    aws ecs update-service \
        --cluster $cluster \
        --service $service \
        --task-definition $new_task_arn \
        --force-new-deployment
    
    # Wait for service to stabilize
    log_info "Waiting for service to stabilize..."
    aws ecs wait services-stable --cluster $cluster --services $service
    
    log_success "ECS deployment successful!"
}

# Deploy function
deploy() {
    load_config
    
    # Confirmation
    if [ "$FORCE" = false ] && [ "$DRY_RUN" = false ]; then
        echo -e "\n${YELLOW}Deployment Summary:${NC}"
        echo "  Environment: $ENVIRONMENT"
        echo "  Version: ${VERSION:-latest}"
        echo "  Type: $DEPLOYMENT_TYPE"
        echo ""
        read -p "Continue with deployment? (y/N) " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            log_info "Deployment cancelled"
            exit 0
        fi
    fi
    
    # Run pre-deployment checks
    pre_deploy_checks
    
    # Build application
    build_application
    
    # Deploy based on type
    case $DEPLOYMENT_TYPE in
        kubernetes)
            deploy_kubernetes
            ;;
        aws)
            deploy_aws_ecs
            ;;
        docker-compose)
            deploy_docker_compose
            ;;
        *)
            log_error "Unknown deployment type: $DEPLOYMENT_TYPE"
            exit 1
            ;;
    esac
    
    # Post-deployment tasks
    if [ "$DRY_RUN" = false ]; then
        post_deploy_tasks
    fi
    
    log_success "Deployment completed successfully!"
}

# Rollback function
rollback() {
    load_config
    
    log_info "Starting rollback for $ENVIRONMENT environment..."
    
    case $DEPLOYMENT_TYPE in
        kubernetes)
            local namespace="${KUBERNETES_NAMESPACE:-default}"
            local deployment="${KUBERNETES_DEPLOYMENT:-app}"
            
            # Get rollout history
            log_info "Rollout history:"
            kubectl rollout history deployment/$deployment -n $namespace
            
            # Rollback to previous version
            kubectl rollout undo deployment/$deployment -n $namespace
            
            # Wait for rollout
            kubectl rollout status deployment/$deployment -n $namespace --timeout=10m
            ;;
        aws)
            # Implement AWS rollback
            log_error "AWS rollback not implemented yet"
            exit 1
            ;;
        *)
            log_error "Rollback not supported for deployment type: $DEPLOYMENT_TYPE"
            exit 1
            ;;
    esac
    
    log_success "Rollback completed successfully!"
}

# Status function
status() {
    load_config
    
    log_info "Checking deployment status for $ENVIRONMENT environment..."
    
    case $DEPLOYMENT_TYPE in
        kubernetes)
            local namespace="${KUBERNETES_NAMESPACE:-default}"
            local deployment="${KUBERNETES_DEPLOYMENT:-app}"
            
            echo -e "\n${GREEN}Deployment Status:${NC}"
            kubectl get deployment $deployment -n $namespace
            
            echo -e "\n${GREEN}Pods:${NC}"
            kubectl get pods -n $namespace -l app=$deployment
            
            echo -e "\n${GREEN}Recent Events:${NC}"
            kubectl get events -n $namespace --sort-by='.lastTimestamp' | tail -10
            ;;
        aws)
            local cluster="${AWS_ECS_CLUSTER}"
            local service="${AWS_ECS_SERVICE}"
            
            echo -e "\n${GREEN}Service Status:${NC}"
            aws ecs describe-services --cluster $cluster --services $service \
                --query 'services[0].{Status:status,Running:runningCount,Desired:desiredCount,Pending:pendingCount}'
            
            echo -e "\n${GREEN}Recent Tasks:${NC}"
            aws ecs list-tasks --cluster $cluster --service-name $service \
                --query 'taskArns' --output table
            ;;
        *)
            log_error "Status not supported for deployment type: $DEPLOYMENT_TYPE"
            exit 1
            ;;
    esac
}

# Post-deployment tasks
post_deploy_tasks() {
    log_info "Running post-deployment tasks..."
    
    # Run migrations if needed
    if [ "$RUN_MIGRATIONS" = "true" ]; then
        log_info "Running database migrations..."
        # Implement migration logic
    fi
    
    # Health checks
    if [ -n "$HEALTH_CHECK_URL" ]; then
        log_info "Running health checks..."
        
        local max_attempts=30
        local attempt=0
        
        while [ $attempt -lt $max_attempts ]; do
            if curl -sf "$HEALTH_CHECK_URL" > /dev/null; then
                log_success "Health check passed"
                break
            fi
            
            attempt=$((attempt + 1))
            log_info "Health check attempt $attempt/$max_attempts..."
            sleep 10
        done
        
        if [ $attempt -eq $max_attempts ]; then
            log_error "Health check failed after $max_attempts attempts"
            exit 1
        fi
    fi
    
    # Send notifications
    if [ "$SEND_NOTIFICATIONS" = "true" ]; then
        send_deployment_notification
    fi
}

# Send notifications
send_deployment_notification() {
    local message="Deployment completed: $ENVIRONMENT environment, version ${VERSION:-latest}"
    
    # Slack notification
    if [ -n "$SLACK_WEBHOOK" ]; then
        curl -X POST -H 'Content-type: application/json' \
            --data "{\"text\":\"$message\"}" \
            "$SLACK_WEBHOOK"
    fi
    
    # Email notification
    if [ -n "$EMAIL_RECIPIENT" ]; then
        echo "$message" | mail -s "Deployment Notification" "$EMAIL_RECIPIENT"
    fi
}

# Main execution
case $COMMAND in
    deploy)
        deploy
        ;;
    rollback)
        rollback
        ;;
    status)
        status
        ;;
    logs)
        # Implement logs viewing
        log_error "Logs command not implemented yet"
        exit 1
        ;;
    shell)
        # Implement shell access
        log_error "Shell command not implemented yet"
        exit 1
        ;;
    *)
        log_error "Unknown command: $COMMAND"
        show_help
        exit 1
        ;;
esac
```

```yaml
# deployments/staging/deployment.yaml
# Staging environment configuration

deployment_type: kubernetes

# Docker configuration
docker:
  registry: ghcr.io
  image: your-org/ai-agent-framework
  push_image: true

# Kubernetes configuration
kubernetes:
  namespace: staging
  deployment: agent-api
  replicas: 2

# Build configuration
build:
  run_tests: true
  
# Health check
health_check:
  url: https://staging.example.com/health
  
# Notifications
notifications:
  send_notifications: true
  slack_webhook: ${SLACK_WEBHOOK}
  
# Feature flags
features:
  run_migrations: true
  enable_monitoring: true
```

#### SubTask 3.3.2: í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬
**ë‹´ë‹¹ì**: ì„¤ì • ê´€ë¦¬ì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```python
#!/usr/bin/env python3
# scripts/config_manager.py

import os
import yaml
import json
from pathlib import Path
from typing import Dict, Any, Optional
from dataclasses import dataclass
import click
from cryptography.fernet import Fernet

@dataclass
class Environment:
    name: str
    config_path: Path
    secrets_path: Optional[Path] = None
    variables: Dict[str, Any] = None

class ConfigManager:
    """Manage environment-specific configurations"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.config_dir = self.project_root / "deployments"
        self.environments = self._load_environments()
        self._key = self._get_or_create_key()
        
    def _get_or_create_key(self) -> bytes:
        """Get or create encryption key for secrets"""
        key_file = self.project_root / ".secrets.key"
        
        if key_file.exists():
            with open(key_file, 'rb') as f:
                return f.read()
        else:
            key = Fernet.generate_key()
            with open(key_file, 'wb') as f:
                f.write(key)
            os.chmod(key_file, 0o600)
            print(f"Created new encryption key: {key_file}")
            return key
    
    def _load_environments(self) -> Dict[str, Environment]:
        """Load all environment configurations"""
        environments = {}
        
        if not self.config_dir.exists():
            self.config_dir.mkdir(parents=True)
            
        for env_dir in self.config_dir.iterdir():
            if env_dir.is_dir():
                env_name = env_dir.name
                config_path = env_dir / "config.yaml"
                secrets_path = env_dir / "secrets.enc"
                
                if config_path.exists():
                    with open(config_path, 'r') as f:
                        variables = yaml.safe_load(f)
                    
                    environments[env_name] = Environment(
                        name=env_name,
                        config_path=config_path,
                        secrets_path=secrets_path if secrets_path.exists() else None,
                        variables=variables
                    )
        
        return environments
    
    def create_environment(self, name: str, base: Optional[str] = None):
        """Create new environment configuration"""
        env_dir = self.config_dir / name
        env_dir.mkdir(parents=True, exist_ok=True)
        
        # Base configuration
        config = {
            'environment': name,
            'deployment': {
                'type': 'kubernetes',
                'namespace': name,
                'replicas': 1 if name == 'development' else 2
            },
            'app': {
                'name': 'ai-agent-framework',
                'version': '${VERSION}',
                'debug': name in ['development', 'staging']
            },
            'database': {
                'host': f'{name}-db.example.com',
                'port': 5432,
                'name': f'app_{name}'
            },
            'redis': {
                'host': f'{name}-redis.example.com',
                'port': 6379
            },
            'logging': {
                'level': 'DEBUG' if name == 'development' else 'INFO'
            }
        }
        
        # Copy from base environment if specified
        if base and base in self.environments:
            base_config = self.environments[base].variables
            config = self._merge_configs(base_config, config)
        
        # Write configuration
        config_path = env_dir / "config.yaml"
        with open(config_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        
        # Create empty secrets file
        secrets_path = env_dir / "secrets.yaml"
        with open(secrets_path, 'w') as f:
            yaml.dump({
                'api_keys': {},
                'credentials': {},
                'certificates': {}
            }, f)
        
        # Create deployment files
        self._create_deployment_files(env_dir, name)
        
        print(f"Created environment: {name}")
        print(f"Configuration: {config_path}")
        print(f"Secrets: {secrets_path}")
    
    def _create_deployment_files(self, env_dir: Path, env_name: str):
        """Create deployment-specific files"""
        # Kubernetes manifests
        k8s_dir = env_dir / "k8s"
        k8s_dir.mkdir(exist_ok=True)
        
        # Namespace
        namespace_yaml = {
            'apiVersion': 'v1',
            'kind': 'Namespace',
            'metadata': {
                'name': env_name
            }
        }
        
        with open(k8s_dir / "namespace.yaml", 'w') as f:
            yaml.dump(namespace_yaml, f)
        
        # ConfigMap
        configmap_yaml = {
            'apiVersion': 'v1',
            'kind': 'ConfigMap',
            'metadata': {
                'name': 'app-config',
                'namespace': env_name
            },
            'data': {
                'APP_ENV': env_name,
                'LOG_LEVEL': 'INFO'
            }
        }
        
        with open(k8s_dir / "configmap.yaml", 'w') as f:
            yaml.dump(configmap_yaml, f)
        
        # Deployment
        deployment_yaml = {
            'apiVersion': 'apps/v1',
            'kind': 'Deployment',
            'metadata': {
                'name': 'agent-api',
                'namespace': env_name
            },
            'spec': {
                'replicas': 2 if env_name == 'production' else 1,
                'selector': {
                    'matchLabels': {
                        'app': 'agent-api'
                    }
                },
                'template': {
                    'metadata': {
                        'labels': {
                            'app': 'agent-api'
                        }
                    },
                    'spec': {
                        'containers': [{
                            'name': 'agent-api',
                            'image': 'ghcr.io/your-org/ai-agent-framework:latest',
                            'ports': [{
                                'containerPort': 8000
                            }],
                            'envFrom': [{
                                'configMapRef': {
                                    'name': 'app-config'
                                }
                            }, {
                                'secretRef': {
                                    'name': 'app-secrets'
                                }
                            }],
                            'resources': {
                                'requests': {
                                    'memory': '256Mi',
                                    'cpu': '100m'
                                },
                                'limits': {
                                    'memory': '512Mi' if env_name != 'production' else '1Gi',
                                    'cpu': '500m' if env_name != 'production' else '1000m'
                                }
                            }
                        }]
                    }
                }
            }
        }
        
        with open(k8s_dir / "deployment.yaml", 'w') as f:
            yaml.dump(deployment_yaml, f)
    
    def _merge_configs(self, base: Dict, overlay: Dict) -> Dict:
        """Deep merge configurations"""
        result = base.copy()
        
        for key, value in overlay.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._merge_configs(result[key], value)
            else:
                result[key] = value
        
        return result
    
    def encrypt_secrets(self, env_name: str):
        """Encrypt secrets file"""
        if env_name not in self.environments:
            raise ValueError(f"Environment not found: {env_name}")
        
        env_dir = self.config_dir / env_name
        secrets_file = env_dir / "secrets.yaml"
        
        if not secrets_file.exists():
            raise FileNotFoundError(f"Secrets file not found: {secrets_file}")
        
        # Read secrets
        with open(secrets_file, 'r') as f:
            secrets = yaml.safe_load(f)
        
        # Encrypt
        fernet = Fernet(self._key)
        encrypted_data = fernet.encrypt(yaml.dump(secrets).encode())
        
        # Write encrypted file
        encrypted_file = env_dir / "secrets.enc"
        with open(encrypted_file, 'wb') as f:
            f.write(encrypted_data)
        
        # Remove plain text file
        secrets_file.unlink()
        
        print(f"Encrypted secrets for {env_name}")
        print(f"Encrypted file: {encrypted_file}")
    
    def decrypt_secrets(self, env_name: str) -> Dict[str, Any]:
        """Decrypt secrets file"""
        if env_name not in self.environments:
            raise ValueError(f"Environment not found: {env_name}")
        
        env = self.environments[env_name]
        
        if not env.secrets_path or not env.secrets_path.exists():
            return {}
        
        # Read encrypted file
        with open(env.secrets_path, 'rb') as f:
            encrypted_data = f.read()
        
        # Decrypt
        fernet = Fernet(self._key)
        decrypted_data = fernet.decrypt(encrypted_data)
        
        return yaml.safe_load(decrypted_data)
    
    def get_config(self, env_name: str, include_secrets: bool = False) -> Dict[str, Any]:
        """Get complete configuration for environment"""
        if env_name not in self.environments:
            raise ValueError(f"Environment not found: {env_name}")
        
        env = self.environments[env_name]
        config = env.variables.copy()
        
        if include_secrets:
            secrets = self.decrypt_secrets(env_name)
            config['secrets'] = secrets
        
        return config
    
    def validate_config(self, env_name: str) -> List[str]:
        """Validate environment configuration"""
        issues = []
        
        if env_name not in self.environments:
            issues.append(f"Environment not found: {env_name}")
            return issues
        
        config = self.get_config(env_name)
        
        # Required fields
        required_fields = [
            'environment',
            'deployment.type',
            'app.name',
            'database.host',
            'redis.host'
        ]
        
        for field in required_fields:
            value = config
            for part in field.split('.'):
                if isinstance(value, dict) and part in value:
                    value = value[part]
                else:
                    issues.append(f"Missing required field: {field}")
                    break
        
        # Validate deployment type
        if config.get('deployment', {}).get('type') not in ['kubernetes', 'docker', 'aws']:
            issues.append("Invalid deployment type")
        
        # Validate replicas
        replicas = config.get('deployment', {}).get('replicas', 1)
        if not isinstance(replicas, int) or replicas < 1:
            issues.append("Invalid replica count")
        
        return issues
    
    def export_dotenv(self, env_name: str, output_file: str = ".env"):
        """Export configuration as .env file"""
        config = self.get_config(env_name, include_secrets=True)
        
        with open(output_file, 'w') as f:
            f.write(f"# Environment: {env_name}\n")
            f.write(f"# Generated by config_manager.py\n\n")
            
            self._write_dotenv_vars(f, config)
        
        print(f"Exported configuration to {output_file}")
    
    def _write_dotenv_vars(self, f, config: Dict, prefix: str = ""):
        """Recursively write configuration as environment variables"""
        for key, value in config.items():
            if isinstance(value, dict):
                new_prefix = f"{prefix}{key.upper()}_" if prefix else f"{key.upper()}_"
                self._write_dotenv_vars(f, value, new_prefix)
            else:
                var_name = f"{prefix}{key.upper()}"
                f.write(f"{var_name}={value}\n")

@click.group()
def cli():
    """Environment configuration manager"""
    pass

@cli.command()
@click.argument('name')
@click.option('--base', help='Base environment to copy from')
def create(name: str, base: Optional[str]):
    """Create new environment"""
    manager = ConfigManager()
    manager.create_environment(name, base)

@cli.command()
@click.argument('name')
def encrypt(name: str):
    """Encrypt secrets for environment"""
    manager = ConfigManager()
    manager.encrypt_secrets(name)

@cli.command()
@click.argument('name')
@click.option('--secrets', is_flag=True, help='Include decrypted secrets')
def show(name: str, secrets: bool):
    """Show environment configuration"""
    manager = ConfigManager()
    config = manager.get_config(name, include_secrets=secrets)
    print(yaml.dump(config, default_flow_style=False))

@cli.command()
@click.argument('name')
def validate(name: str):
    """Validate environment configuration"""
    manager = ConfigManager()
    issues = manager.validate_config(name)
    
    if issues:
        print("Configuration issues found:")
        for issue in issues:
            print(f"  - {issue}")
    else:
        print("Configuration is valid")

@cli.command()
@click.argument('name')
@click.option('--output', '-o', default='.env', help='Output file')
def export(name: str, output: str):
    """Export configuration as .env file"""
    manager = ConfigManager()
    manager.export_dotenv(name, output)

@cli.command()
def list():
    """List all environments"""
    manager = ConfigManager()
    
    print("Available environments:")
    for env_name, env in manager.environments.items():
        has_secrets = "Yes" if env.secrets_path else "No"
        print(f"  - {env_name} (secrets: {has_secrets})")

if __name__ == '__main__':
    cli()
```

#### SubTask 3.3.3: ëª¨ë‹ˆí„°ë§ í†µí•©
**ë‹´ë‹¹ì**: ëª¨ë‹ˆí„°ë§ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```python
#!/usr/bin/env python3
# scripts/setup_monitoring.py

import os
import yaml
import json
from pathlib import Path
from typing import Dict, Any, List
import requests
import click

class MonitoringSetup:
    """Setup monitoring for deployed applications"""
    
    def __init__(self):
        self.project_root = Path.cwd()
        self.monitoring_dir = self.project_root / "monitoring"
        self.monitoring_dir.mkdir(exist_ok=True)
        
    def setup_prometheus(self, environment: str):
        """Setup Prometheus configuration"""
        print("ğŸ“Š Setting up Prometheus monitoring...")
        
        # Prometheus configuration
        prometheus_config = {
            'global': {
                'scrape_interval': '15s',
                'evaluation_interval': '15s',
                'external_labels': {
                    'environment': environment,
                    'service': 'ai-agent-framework'
                }
            },
            'alerting': {
                'alertmanagers': [{
                    'static_configs': [{
                        'targets': ['alertmanager:9093']
                    }]
                }]
            },
            'rule_files': [
                '/etc/prometheus/rules/*.yaml'
            ],
            'scrape_configs': [
                {
                    'job_name': 'agent-api',
                    'static_configs': [{
                        'targets': ['agent-api:8000']
                    }],
                    'metrics_path': '/metrics'
                },
                {
                    'job_name': 'node-exporter',
                    'static_configs': [{
                        'targets': ['node-exporter:9100']
                    }]
                },
                {
                    'job_name': 'cadvisor',
                    'static_configs': [{
                        'targets': ['cadvisor:8080']
                    }]
                }
            ]
        }
        
        # Write Prometheus config
        prometheus_file = self.monitoring_dir / "prometheus.yaml"
        with open(prometheus_file, 'w') as f:
            yaml.dump(prometheus_config, f)
        
        # Create alert rules
        self._create_alert_rules()
        
        print(f"âœ… Prometheus configuration saved to {prometheus_file}")
        
    def _create_alert_rules(self):
        """Create Prometheus alert rules"""
        rules_dir = self.monitoring_dir / "rules"
        rules_dir.mkdir(exist_ok=True)
        
        # Application alerts
        app_alerts = {
            'groups': [{
                'name': 'agent_api',
                'interval': '30s',
                'rules': [
                    {
                        'alert': 'HighErrorRate',
                        'expr': 'rate(http_requests_total{status=~"5.."}[5m]) > 0.05',
                        'for': '5m',
                        'labels': {
                            'severity': 'critical'
                        },
                        'annotations': {
                            'summary': 'High error rate detected',
                            'description': 'Error rate is above 5% for 5 minutes'
                        }
                    },
                    {
                        'alert': 'HighResponseTime',
                        'expr': 'histogram_quantile(0.95, http_request_duration_seconds_bucket) > 1',
                        'for': '5m',
                        'labels': {
                            'severity': 'warning'
                        },
                        'annotations': {
                            'summary': 'High response time detected',
                            'description': '95th percentile response time is above 1s'
                        }
                    },
                    {
                        'alert': 'PodDown',
                        'expr': 'up{job="agent-api"} == 0',
                        'for': '1m',
                        'labels': {
                            'severity': 'critical'
                        },
                        'annotations': {
                            'summary': 'Pod is down',
                            'description': 'Agent API pod has been down for 1 minute'
                        }
                    }
                ]
            }]
        }
        
        # Resource alerts
        resource_alerts = {
            'groups': [{
                'name': 'resources',
                'interval': '30s',
                'rules': [
                    {
                        'alert': 'HighCPUUsage',
                        'expr': 'rate(container_cpu_usage_seconds_total[5m]) * 100 > 80',
                        'for': '10m',
                        'labels': {
                            'severity': 'warning'
                        },
                        'annotations': {
                            'summary': 'High CPU usage',
                            'description': 'CPU usage is above 80% for 10 minutes'
                        }
                    },
                    {
                        'alert': 'HighMemoryUsage',
                        'expr': 'container_memory_usage_bytes / container_spec_memory_limit_bytes * 100 > 85',
                        'for': '5m',
                        'labels': {
                            'severity': 'warning'
                        },
                        'annotations': {
                            'summary': 'High memory usage',
                            'description': 'Memory usage is above 85% of limit'
                        }
                    },
                    {
                        'alert': 'DiskSpaceLow',
                        'expr': 'node_filesystem_avail_bytes / node_filesystem_size_bytes * 100 < 15',
                        'for': '5m',
                        'labels': {
                            'severity': 'critical'
                        },
                        'annotations': {
                            'summary': 'Low disk space',
                            'description': 'Disk space is below 15%'
                        }
                    }
                ]
            }]
        }
        
        # Write alert rules
        with open(rules_dir / "app_alerts.yaml", 'w') as f:
            yaml.dump(app_alerts, f)
        
        with open(rules_dir / "resource_alerts.yaml", 'w') as f:
            yaml.dump(resource_alerts, f)
        
    def setup_grafana(self, environment: str):
        """Setup Grafana dashboards"""
        print("ğŸ“ˆ Setting up Grafana dashboards...")
        
        dashboards_dir = self.monitoring_dir / "grafana" / "dashboards"
        dashboards_dir.mkdir(parents=True, exist_ok=True)
        
        # Application dashboard
        app_dashboard = {
            "dashboard": {
                "title": f"AI Agent Framework - {environment}",
                "panels": [
                    {
                        "title": "Request Rate",
                        "targets": [{
                            "expr": "rate(http_requests_total[5m])"
                        }],
                        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
                    },
                    {
                        "title": "Error Rate",
                        "targets": [{
                            "expr": "rate(http_requests_total{status=~'5..'}[5m])"
                        }],
                        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
                    },
                    {
                        "title": "Response Time (p95)",
                        "targets": [{
                            "expr": "histogram_quantile(0.95, http_request_duration_seconds_bucket)"
                        }],
                        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
                    },
                    {
                        "title": "Active Agents",
                        "targets": [{
                            "expr": "agent_active_count"
                        }],
                        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
                    }
                ]
            }
        }
        
        # Resource dashboard
        resource_dashboard = {
            "dashboard": {
                "title": f"Resource Usage - {environment}",
                "panels": [
                    {
                        "title": "CPU Usage",
                        "targets": [{
                            "expr": "rate(container_cpu_usage_seconds_total[5m]) * 100"
                        }],
                        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
                    },
                    {
                        "title": "Memory Usage",
                        "targets": [{
                            "expr": "container_memory_usage_bytes / 1024 / 1024 / 1024"
                        }],
                        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
                    },
                    {
                        "title": "Network I/O",
                        "targets": [{
                            "expr": "rate(container_network_receive_bytes_total[5m])"
                        }],
                        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
                    },
                    {
                        "title": "Disk Usage",
                        "targets": [{
                            "expr": "node_filesystem_avail_bytes / node_filesystem_size_bytes * 100"
                        }],
                        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
                    }
                ]
            }
        }
        
        # Write dashboards
        with open(dashboards_dir / "app_dashboard.json", 'w') as f:
            json.dump(app_dashboard, f, indent=2)
        
        with open(dashboards_dir / "resource_dashboard.json", 'w') as f:
            json.dump(resource_dashboard, f, indent=2)
        
        # Grafana provisioning
        self._create_grafana_provisioning()
        
        print(f"âœ… Grafana dashboards saved to {dashboards_dir}")
        
    def _create_grafana_provisioning(self):
        """Create Grafana provisioning configuration"""
        provisioning_dir = self.monitoring_dir / "grafana" / "provisioning"
        
        # Datasources
        datasources_dir = provisioning_dir / "datasources"
        datasources_dir.mkdir(parents=True, exist_ok=True)
        
        datasources_config = {
            'apiVersion': 1,
            'datasources': [{
                'name': 'Prometheus',
                'type': 'prometheus',
                'access': 'proxy',
                'url': 'http://prometheus:9090',
                'isDefault': True
            }]
        }
        
        with open(datasources_dir / "prometheus.yaml", 'w') as f:
            yaml.dump(datasources_config, f)
        
        # Dashboard provisioning
        dashboards_dir = provisioning_dir / "dashboards"
        dashboards_dir.mkdir(parents=True, exist_ok=True)
        
        dashboards_config = {
            'apiVersion': 1,
            'providers': [{
                'name': 'default',
                'orgId': 1,
                'folder': '',
                'type': 'file',
                'disableDeletion': False,
                'updateIntervalSeconds': 10,
                'options': {
                    'path': '/var/lib/grafana/dashboards'
                }
            }]
        }
        
        with open(dashboards_dir / "dashboards.yaml", 'w') as f:
            yaml.dump(dashboards_config, f)
        
    def setup_logging(self, environment: str):
        """Setup centralized logging"""
        print("ğŸ“ Setting up centralized logging...")
        
        # Fluent Bit configuration
        fluentbit_config = """
[SERVICE]
    Flush         5
    Daemon        off
    Log_Level     info

[INPUT]
    Name              forward
    Listen            0.0.0.0
    Port              24224

[INPUT]
    Name              tail
    Path              /var/log/containers/*.log
    Parser            docker
    Tag               kube.*
    Refresh_Interval  5

[FILTER]
    Name              kubernetes
    Match             kube.*
    Kube_URL          https://kubernetes.default.svc:443
    Kube_CA_File      /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    Kube_Token_File   /var/run/secrets/kubernetes.io/serviceaccount/token

[OUTPUT]
    Name              es
    Match             *
    Host              elasticsearch
    Port              9200
    Index             agent-logs
    Type              _doc

[OUTPUT]
    Name              stdout
    Match             *
    Format            json_lines
"""
        
        fluentbit_file = self.monitoring_dir / "fluent-bit.conf"
        with open(fluentbit_file, 'w') as f:
            f.write(fluentbit_config)
        
        # Elasticsearch index template
        index_template = {
            "index_patterns": ["agent-logs-*"],
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "index.lifecycle.name": "agent-logs-policy",
                "index.lifecycle.rollover_alias": "agent-logs"
            },
            "mappings": {
                "properties": {
                    "@timestamp": {"type": "date"},
                    "level": {"type": "keyword"},
                    "service": {"type": "keyword"},
                    "environment": {"type": "keyword"},
                    "message": {"type": "text"},
                    "trace_id": {"type": "keyword"},
                    "span_id": {"type": "keyword"}
                }
            }
        }
        
        template_file = self.monitoring_dir / "elasticsearch_template.json"
        with open(template_file, 'w') as f:
            json.dump(index_template, f, indent=2)
        
        print(f"âœ… Logging configuration saved")
        
    def create_docker_compose(self):
        """Create Docker Compose file for monitoring stack"""
        print("ğŸ³ Creating monitoring stack Docker Compose...")
        
        compose_config = {
            'version': '3.8',
            'services': {
                'prometheus': {
                    'image': 'prom/prometheus:latest',
                    'volumes': [
                        './monitoring/prometheus.yaml:/etc/prometheus/prometheus.yml',
                        './monitoring/rules:/etc/prometheus/rules',
                        'prometheus-data:/prometheus'
                    ],
                    'ports': ['9090:9090'],
                    'command': [
                        '--config.file=/etc/prometheus/prometheus.yml',
                        '--storage.tsdb.path=/prometheus',
                        '--web.console.libraries=/usr/share/prometheus/console_libraries',
                        '--web.console.templates=/usr/share/prometheus/consoles'
                    ]
                },
                'grafana': {
                    'image': 'grafana/grafana:latest',
                    'volumes': [
                        './monitoring/grafana/provisioning:/etc/grafana/provisioning',
                        './monitoring/grafana/dashboards:/var/lib/grafana/dashboards',
                        'grafana-data:/var/lib/grafana'
                    ],
                    'ports': ['3000:3000'],
                    'environment': {
                        'GF_SECURITY_ADMIN_PASSWORD': 'admin',
                        'GF_USERS_ALLOW_SIGN_UP': 'false'
                    }
                },
                'alertmanager': {
                    'image': 'prom/alertmanager:latest',
                    'volumes': [
                        './monitoring/alertmanager.yaml:/etc/alertmanager/alertmanager.yml',
                        'alertmanager-data:/alertmanager'
                    ],
                    'ports': ['9093:9093']
                },
                'node-exporter': {
                    'image': 'prom/node-exporter:latest',
                    'volumes': [
                        '/proc:/host/proc:ro',
                        '/sys:/host/sys:ro',
                        '/:/rootfs:ro'
                    ],
                    'command': [
                        '--path.procfs=/host/proc',
                        '--path.sysfs=/host/sys',
                        '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
                    ],
                    'ports': ['9100:9100']
                },
                'cadvisor': {
                    'image': 'gcr.io/cadvisor/cadvisor:latest',
                    'volumes': [
                        '/:/rootfs:ro',
                        '/var/run:/var/run:ro',
                        '/sys:/sys:ro',
                        '/var/lib/docker/:/var/lib/docker:ro',
                        '/dev/disk/:/dev/disk:ro'
                    ],
                    'ports': ['8080:8080'],
                    'privileged': True
                }
            },
            'volumes': {
                'prometheus-data': {},
                'grafana-data': {},
                'alertmanager-data': {}
            }
        }
        
        compose_file = self.monitoring_dir / "docker-compose.monitoring.yml"
        with open(compose_file, 'w') as f:
            yaml.dump(compose_config, f)
        
        print(f"âœ… Docker Compose file saved to {compose_file}")
        
    def setup_application_metrics(self):
        """Setup application metrics collection"""
        print("ğŸ“Š Setting up application metrics...")
        
        # Create metrics module
        metrics_dir = self.project_root / "app" / "monitoring"
        metrics_dir.mkdir(exist_ok=True)
        
        # Metrics module
        metrics_code = '''"""
Application metrics collection using Prometheus client
"""
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from functools import wraps
import time

# Request metrics
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

http_request_duration_seconds = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration in seconds',
    ['method', 'endpoint']
)

# Agent metrics
agent_active_count = Gauge(
    'agent_active_count',
    'Number of active agents'
)

agent_task_duration_seconds = Histogram(
    'agent_task_duration_seconds',
    'Agent task duration in seconds',
    ['agent_type', 'task_type']
)

agent_errors_total = Counter(
    'agent_errors_total',
    'Total agent errors',
    ['agent_type', 'error_type']
)

# Database metrics
db_query_duration_seconds = Histogram(
    'db_query_duration_seconds',
    'Database query duration in seconds',
    ['query_type']
)

db_connections_active = Gauge(
    'db_connections_active',
    'Number of active database connections'
)

# Cache metrics
cache_hits_total = Counter(
    'cache_hits_total',
    'Total cache hits',
    ['cache_type']
)

cache_misses_total = Counter(
    'cache_misses_total',
    'Total cache misses',
    ['cache_type']
)


def track_request_metrics(func):
    """Decorator to track HTTP request metrics"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        
        try:
            response = await func(*args, **kwargs)
            status = response.status_code
        except Exception as e:
            status = 500
            raise
        finally:
            duration = time.time() - start_time
            
            # Get request info from args
            request = args[0] if args else None
            method = request.method if request else 'UNKNOWN'
            endpoint = request.url.path if request else 'UNKNOWN'
            
            # Record metrics
            http_requests_total.labels(
                method=method,
                endpoint=endpoint,
                status=status
            ).inc()
            
            http_request_duration_seconds.labels(
                method=method,
                endpoint=endpoint
            ).observe(duration)
        
        return response
    
    return wrapper


def track_agent_metrics(agent_type: str):
    """Decorator to track agent metrics"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            
            try:
                result = await func(*args, **kwargs)
            except Exception as e:
                agent_errors_total.labels(
                    agent_type=agent_type,
                    error_type=type(e).__name__
                ).inc()
                raise
            finally:
                duration = time.time() - start_time
                
                # Get task type from function name or args
                task_type = func.__name__
                
                agent_task_duration_seconds.labels(
                    agent_type=agent_type,
                    task_type=task_type
                ).observe(duration)
            
            return result
        
        return wrapper
    
    return decorator


async def metrics_endpoint():
    """Endpoint to expose metrics for Prometheus"""
    return generate_latest()
'''
        
        with open(metrics_dir / "__init__.py", 'w') as f:
            f.write(metrics_code)
        
        # FastAPI integration
        fastapi_integration = '''"""
FastAPI integration for metrics
"""
from fastapi import FastAPI, Request, Response
from fastapi.responses import PlainTextResponse
from .monitoring import (
    metrics_endpoint,
    track_request_metrics,
    http_requests_total,
    http_request_duration_seconds
)
import time

def setup_metrics(app: FastAPI):
    """Setup metrics collection for FastAPI app"""
    
    @app.middleware("http")
    async def metrics_middleware(request: Request, call_next):
        start_time = time.time()
        
        try:
            response = await call_next(request)
            status = response.status_code
        except Exception as e:
            status = 500
            raise
        finally:
            duration = time.time() - start_time
            
            # Record metrics
            http_requests_total.labels(
                method=request.method,
                endpoint=request.url.path,
                status=status
            ).inc()
            
            http_request_duration_seconds.labels(
                method=request.method,
                endpoint=request.url.path
            ).observe(duration)
        
        return response
    
    @app.get("/metrics", response_class=PlainTextResponse)
    async def get_metrics():
        """Prometheus metrics endpoint"""
        return await metrics_endpoint()
'''
        
        with open(metrics_dir / "fastapi.py", 'w') as f:
            f.write(fastapi_integration)
        
        print("âœ… Application metrics setup complete")
        
    def create_kubernetes_manifests(self, environment: str):
        """Create Kubernetes manifests for monitoring"""
        print("â˜¸ï¸  Creating Kubernetes monitoring manifests...")
        
        k8s_dir = self.monitoring_dir / "k8s"
        k8s_dir.mkdir(exist_ok=True)
        
        # ServiceMonitor for Prometheus Operator
        service_monitor = {
            'apiVersion': 'monitoring.coreos.com/v1',
            'kind': 'ServiceMonitor',
            'metadata': {
                'name': 'agent-api',
                'namespace': environment
            },
            'spec': {
                'selector': {
                    'matchLabels': {
                        'app': 'agent-api'
                    }
                },
                'endpoints': [{
                    'port': 'metrics',
                    'interval': '30s',
                    'path': '/metrics'
                }]
            }
        }
        
        with open(k8s_dir / "service-monitor.yaml", 'w') as f:
            yaml.dump(service_monitor, f)
        
        # PrometheusRule for alerts
        prometheus_rule = {
            'apiVersion': 'monitoring.coreos.com/v1',
            'kind': 'PrometheusRule',
            'metadata': {
                'name': 'agent-api-alerts',
                'namespace': environment
            },
            'spec': {
                'groups': [{
                    'name': 'agent-api',
                    'rules': [
                        {
                            'alert': 'AgentAPIDown',
                            'expr': 'up{job="agent-api"} == 0',
                            'for': '5m',
                            'labels': {
                                'severity': 'critical',
                                'service': 'agent-api'
                            },
                            'annotations': {
                                'summary': 'Agent API is down',
                                'description': 'Agent API has been down for more than 5 minutes'
                            }
                        }
                    ]
                }]
            }
        }
        
        with open(k8s_dir / "prometheus-rule.yaml", 'w') as f:
            yaml.dump(prometheus_rule, f)
        
        print(f"âœ… Kubernetes manifests saved to {k8s_dir}")

@click.group()
def cli():
    """Monitoring setup CLI"""
    pass

@cli.command()
@click.argument('environment')
def setup(environment: str):
    """Setup complete monitoring stack"""
    setup = MonitoringSetup()
    
    # Setup all components
    setup.setup_prometheus(environment)
    setup.setup_grafana(environment)
    setup.setup_logging(environment)
    setup.setup_application_metrics()
    setup.create_docker_compose()
    setup.create_kubernetes_manifests(environment)
    
    print("\nâœ… Monitoring setup complete!")
    print("\nNext steps:")
    print("1. Start monitoring stack: docker-compose -f monitoring/docker-compose.monitoring.yml up -d")
    print("2. Access Grafana: http://localhost:3000 (admin/admin)")
    print("3. Access Prometheus: http://localhost:9090")
    print("4. Import dashboards from monitoring/grafana/dashboards/")

@cli.command()
@click.argument('component')
@click.argument('environment')
def setup_component(component: str, environment: str):
    """Setup specific monitoring component"""
    setup = MonitoringSetup()
    
    if component == 'prometheus':
        setup.setup_prometheus(environment)
    elif component == 'grafana':
        setup.setup_grafana(environment)
    elif component == 'logging':
        setup.setup_logging(environment)
    elif component == 'metrics':
        setup.setup_application_metrics()
    else:
        print(f"Unknown component: {component}")
        print("Available: prometheus, grafana, logging, metrics")

if __name__ == '__main__':
    cli()
```

---

## ğŸ“‹ Phase 4: í”„ë¡œë•ì…˜ ì „í™˜ ë° ëª¨ë‹ˆí„°ë§ (Day 13-21)

### Task 4.1: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

#### SubTask 4.1.1: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ëª¨ë‹ˆí„°ë§ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 4ì‹œê°„

```python
#!/usr/bin/env python3
# scripts/metrics_collector.py

import asyncio
import time
import psutil
import aiohttp
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import json
from prometheus_client import CollectorRegistry, Gauge, Counter, Histogram, push_to_gateway
import logging

@dataclass
class SystemMetrics:
    timestamp: datetime
    cpu_percent: float
    memory_percent: float
    memory_mb: float
    disk_io_read_mb: float
    disk_io_write_mb: float
    network_io_sent_mb: float
    network_io_received_mb: float
    open_files: int
    threads: int

@dataclass
class UvMetrics:
    timestamp: datetime
    cache_size_mb: float
    cache_hits: int
    cache_misses: int
    packages_installed: int
    install_time_seconds: float
    resolution_time_seconds: float
    download_time_seconds: float
    build_time_seconds: float

class MetricsCollector:
    """Collect system and uv-specific metrics"""
    
    def __init__(self, 
                 push_gateway: Optional[str] = None,
                 collection_interval: int = 60):
        self.push_gateway = push_gateway
        self.collection_interval = collection_interval
        self.registry = CollectorRegistry()
        self.logger = logging.getLogger(__name__)
        
        # System metrics
        self.cpu_usage = Gauge('system_cpu_usage_percent', 
                              'CPU usage percentage', 
                              registry=self.registry)
        self.memory_usage = Gauge('system_memory_usage_percent', 
                                 'Memory usage percentage', 
                                 registry=self.registry)
        self.memory_usage_mb = Gauge('system_memory_usage_mb', 
                                    'Memory usage in MB', 
                                    registry=self.registry)
        self.disk_io_read = Gauge('system_disk_io_read_mb', 
                                 'Disk I/O read in MB', 
                                 registry=self.registry)
        self.disk_io_write = Gauge('system_disk_io_write_mb', 
                                  'Disk I/O write in MB', 
                                  registry=self.registry)
        self.network_io_sent = Gauge('system_network_io_sent_mb', 
                                    'Network I/O sent in MB', 
                                    registry=self.registry)
        self.network_io_received = Gauge('system_network_io_received_mb', 
                                        'Network I/O received in MB', 
                                        registry=self.registry)
        
        # uv specific metrics
        self.uv_cache_size = Gauge('uv_cache_size_mb', 
                                  'uv cache size in MB', 
                                  registry=self.registry)
        self.uv_cache_hits = Counter('uv_cache_hits_total', 
                                    'Total uv cache hits', 
                                    registry=self.registry)
        self.uv_cache_misses = Counter('uv_cache_misses_total', 
                                      'Total uv cache misses', 
                                      registry=self.registry)
        self.uv_install_duration = Histogram('uv_install_duration_seconds', 
                                           'uv install duration in seconds',
                                           buckets=(0.1, 0.5, 1, 2, 5, 10, 30, 60),
                                           registry=self.registry)
        self.uv_packages_installed = Counter('uv_packages_installed_total', 
                                           'Total packages installed with uv', 
                                           registry=self.registry)
        
        # Initialize baseline metrics
        self.baseline_disk_io = psutil.disk_io_counters()
        self.baseline_network_io = psutil.net_io_counters()
        
    async def collect_system_metrics(self) -> SystemMetrics:
        """Collect current system metrics"""
        # CPU and Memory
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        # Disk I/O
        current_disk_io = psutil.disk_io_counters()
        disk_read_mb = (current_disk_io.read_bytes - self.baseline_disk_io.read_bytes) / (1024 * 1024)
        disk_write_mb = (current_disk_io.write_bytes - self.baseline_disk_io.write_bytes) / (1024 * 1024)
        
        # Network I/O
        current_network_io = psutil.net_io_counters()
        network_sent_mb = (current_network_io.bytes_sent - self.baseline_network_io.bytes_sent) / (1024 * 1024)
        network_recv_mb = (current_network_io.bytes_recv - self.baseline_network_io.bytes_recv) / (1024 * 1024)
        
        # Process info
        process = psutil.Process()
        
        metrics = SystemMetrics(
            timestamp=datetime.now(),
            cpu_percent=cpu_percent,
            memory_percent=memory.percent,
            memory_mb=memory.used / (1024 * 1024),
            disk_io_read_mb=disk_read_mb,
            disk_io_write_mb=disk_write_mb,
            network_io_sent_mb=network_sent_mb,
            network_io_received_mb=network_recv_mb,
            open_files=len(process.open_files()),
            threads=process.num_threads()
        )
        
        # Update Prometheus metrics
        self.cpu_usage.set(metrics.cpu_percent)
        self.memory_usage.set(metrics.memory_percent)
        self.memory_usage_mb.set(metrics.memory_mb)
        self.disk_io_read.set(metrics.disk_io_read_mb)
        self.disk_io_write.set(metrics.disk_io_write_mb)
        self.network_io_sent.set(metrics.network_io_sent_mb)
        self.network_io_received.set(metrics.network_io_received_mb)
        
        # Update baselines
        self.baseline_disk_io = current_disk_io
        self.baseline_network_io = current_network_io
        
        return metrics
    
    async def collect_uv_metrics(self) -> Optional[UvMetrics]:
        """Collect uv-specific metrics"""
        try:
            import subprocess
            import os
            from pathlib import Path
            
            # Get uv cache size
            cache_dir = Path.home() / '.cache' / 'uv'
            cache_size_mb = 0
            
            if cache_dir.exists():
                for path in cache_dir.rglob('*'):
                    if path.is_file():
                        cache_size_mb += path.stat().st_size / (1024 * 1024)
            
            # Parse uv stats if available
            # This would need to be implemented based on uv's actual metrics output
            metrics = UvMetrics(
                timestamp=datetime.now(),
                cache_size_mb=cache_size_mb,
                cache_hits=0,  # Would need actual implementation
                cache_misses=0,
                packages_installed=0,
                install_time_seconds=0,
                resolution_time_seconds=0,
                download_time_seconds=0,
                build_time_seconds=0
            )
            
            # Update Prometheus metrics
            self.uv_cache_size.set(metrics.cache_size_mb)
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"Error collecting uv metrics: {e}")
            return None
    
    async def monitor_uv_command(self, cmd: List[str]) -> Dict[str, Any]:
        """Monitor a uv command execution"""
        start_time = time.time()
        
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await process.communicate()
        
        duration = time.time() - start_time
        
        # Record metrics
        self.uv_install_duration.observe(duration)
        
        if process.returncode == 0:
            # Try to parse package count from output
            output = stdout.decode()
            package_count = self._parse_package_count(output)
            if package_count:
                self.uv_packages_installed.inc(package_count)
        
        return {
            'command': ' '.join(cmd),
            'duration': duration,
            'success': process.returncode == 0,
            'stdout': stdout.decode(),
            'stderr': stderr.decode()
        }
    
    def _parse_package_count(self, output: str) -> int:
        """Parse package count from uv output"""
        # This would need to be implemented based on actual uv output format
        import re
        
        # Example pattern - adjust based on actual output
        match = re.search(r'Successfully installed (\d+) packages', output)
        if match:
            return int(match.group(1))
        
        return 0
    
    async def push_metrics(self):
        """Push metrics to Prometheus push gateway"""
        if self.push_gateway:
            try:
                push_to_gateway(
                    self.push_gateway,
                    job='uv_metrics_collector',
                    registry=self.registry
                )
                self.logger.info("Metrics pushed to gateway")
            except Exception as e:
                self.logger.error(f"Failed to push metrics: {e}")
    
    async def collect_and_store(self, storage_backend: Optional[Any] = None):
        """Collect metrics and optionally store them"""
        system_metrics = await self.collect_system_metrics()
        uv_metrics = await self.collect_uv_metrics()
        
        # Push to Prometheus
        await self.push_metrics()
        
        # Store to backend if provided
        if storage_backend:
            await storage_backend.store({
                'system': asdict(system_metrics),
                'uv': asdict(uv_metrics) if uv_metrics else None
            })
        
        return {
            'system': system_metrics,
            'uv': uv_metrics
        }
    
    async def run_continuous_monitoring(self):
        """Run continuous metrics collection"""
        self.logger.info(f"Starting continuous monitoring (interval: {self.collection_interval}s)")
        
        while True:
            try:
                metrics = await self.collect_and_store()
                self.logger.debug(f"Collected metrics: {metrics}")
                
            except Exception as e:
                self.logger.error(f"Error in metrics collection: {e}")
            
            await asyncio.sleep(self.collection_interval)


class MetricsComparator:
    """Compare pip vs uv metrics"""
    
    def __init__(self):
        self.pip_metrics: List[Dict] = []
        self.uv_metrics: List[Dict] = []
        
    async def benchmark_installation(self, requirements_file: str) -> Dict[str, Any]:
        """Benchmark pip vs uv installation"""
        results = {}
        
        # Benchmark pip
        pip_start = time.time()
        pip_process = await asyncio.create_subprocess_exec(
            'pip', 'install', '-r', requirements_file, '--dry-run',
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        await pip_process.communicate()
        pip_duration = time.time() - pip_start
        
        # Benchmark uv
        uv_start = time.time()
        uv_process = await asyncio.create_subprocess_exec(
            'uv', 'pip', 'install', '-r', requirements_file, '--dry-run',
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        await uv_process.communicate()
        uv_duration = time.time() - uv_start
        
        results = {
            'pip': {
                'duration': pip_duration,
                'success': pip_process.returncode == 0
            },
            'uv': {
                'duration': uv_duration,
                'success': uv_process.returncode == 0
            },
            'speedup': pip_duration / uv_duration if uv_duration > 0 else 0
        }
        
        return results
    
    def generate_comparison_report(self) -> str:
        """Generate comparison report between pip and uv"""
        report = """# pip vs uv Performance Comparison

## Summary Statistics

"""
        
        if not self.pip_metrics or not self.uv_metrics:
            return report + "No metrics collected yet.\n"
        
        # Calculate averages
        pip_avg_time = sum(m['duration'] for m in self.pip_metrics) / len(self.pip_metrics)
        uv_avg_time = sum(m['duration'] for m in self.uv_metrics) / len(self.uv_metrics)
        
        report += f"""
| Metric | pip | uv | Improvement |
|--------|-----|-----|-------------|
| Average Install Time | {pip_avg_time:.2f}s | {uv_avg_time:.2f}s | {pip_avg_time/uv_avg_time:.1f}x |
| Success Rate | {sum(m['success'] for m in self.pip_metrics)/len(self.pip_metrics)*100:.1f}% | {sum(m['success'] for m in self.uv_metrics)/len(self.uv_metrics)*100:.1f}% | - |

## Detailed Metrics

### Installation Times Distribution

```
pip:  Min: {min(m['duration'] for m in self.pip_metrics):.2f}s  Max: {max(m['duration'] for m in self.pip_metrics):.2f}s
uv:   Min: {min(m['duration'] for m in self.uv_metrics):.2f}s  Max: {max(m['duration'] for m in self.uv_metrics):.2f}s
```
"""
        
        return report


class MetricsDashboard:
    """Simple metrics dashboard server"""
    
    def __init__(self, collector: MetricsCollector, port: int = 8080):
        self.collector = collector
        self.port = port
        self.app = self._create_app()
        
    def _create_app(self):
        from aiohttp import web
        
        async def index(request):
            html = """
<!DOCTYPE html>
<html>
<head>
    <title>uv Metrics Dashboard</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .metric { 
            display: inline-block; 
            margin: 10px; 
            padding: 15px; 
            border: 1px solid #ddd; 
            border-radius: 5px;
        }
        .chart-container { 
            width: 45%; 
            display: inline-block; 
            margin: 10px;
        }
    </style>
</head>
<body>
    <h1>uv Performance Metrics</h1>
    
    <div id="metrics">
        <div class="metric">
            <h3>CPU Usage</h3>
            <p id="cpu">-</p>
        </div>
        <div class="metric">
            <h3>Memory Usage</h3>
            <p id="memory">-</p>
        </div>
        <div class="metric">
            <h3>Cache Size</h3>
            <p id="cache">-</p>
        </div>
    </div>
    
    <div class="chart-container">
        <canvas id="performanceChart"></canvas>
    </div>
    
    <div class="chart-container">
        <canvas id="cacheChart"></canvas>
    </div>
    
    <script>
        // Initialize charts
        const perfCtx = document.getElementById('performanceChart').getContext('2d');
        const perfChart = new Chart(perfCtx, {
            type: 'line',
            data: {
                labels: [],
                datasets: [{
                    label: 'CPU %',
                    data: [],
                    borderColor: 'rgb(255, 99, 132)',
                    tension: 0.1
                }, {
                    label: 'Memory %',
                    data: [],
                    borderColor: 'rgb(54, 162, 235)',
                    tension: 0.1
                }]
            },
            options: {
                responsive: true,
                scales: {
                    y: {
                        beginAtZero: true,
                        max: 100
                    }
                }
            }
        });
        
        // Update metrics
        async function updateMetrics() {
            try {
                const response = await fetch('/api/metrics');
                const data = await response.json();
                
                // Update text metrics
                document.getElementById('cpu').textContent = data.system.cpu_percent.toFixed(1) + '%';
                document.getElementById('memory').textContent = data.system.memory_percent.toFixed(1) + '%';
                document.getElementById('cache').textContent = (data.uv?.cache_size_mb || 0).toFixed(1) + ' MB';
                
                // Update charts
                const now = new Date().toLocaleTimeString();
                perfChart.data.labels.push(now);
                perfChart.data.datasets[0].data.push(data.system.cpu_percent);
                perfChart.data.datasets[1].data.push(data.system.memory_percent);
                
                // Keep only last 20 points
                if (perfChart.data.labels.length > 20) {
                    perfChart.data.labels.shift();
                    perfChart.data.datasets.forEach(dataset => dataset.data.shift());
                }
                
                perfChart.update();
                
            } catch (error) {
                console.error('Error fetching metrics:', error);
            }
        }
        
        // Update every 5 seconds
        setInterval(updateMetrics, 5000);
        updateMetrics();
    </script>
</body>
</html>
"""
            return web.Response(text=html, content_type='text/html')
        
        async def api_metrics(request):
            metrics = await self.collector.collect_and_store()
            return web.json_response({
                'system': asdict(metrics['system']),
                'uv': asdict(metrics['uv']) if metrics['uv'] else None
            })
        
        app = web.Application()
        app.router.add_get('/', index)
        app.router.add_get('/api/metrics', api_metrics)
        
        return app
    
    async def start(self):
        """Start the dashboard server"""
        runner = web.AppRunner(self.app)
        await runner.setup()
        site = web.TCPSite(runner, 'localhost', self.port)
        await site.start()
        print(f"Dashboard running at http://localhost:{self.port}")


async def main():
    """Main function to run metrics collection"""
    import argparse
    
    parser = argparse.ArgumentParser(description='uv metrics collector')
    parser.add_argument('--push-gateway', help='Prometheus push gateway URL')
    parser.add_argument('--interval', type=int, default=60, help='Collection interval in seconds')
    parser.add_argument('--dashboard', action='store_true', help='Run metrics dashboard')
    parser.add_argument('--benchmark', help='Run benchmark with requirements file')
    
    args = parser.parse_args()
    
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Create collector
    collector = MetricsCollector(
        push_gateway=args.push_gateway,
        collection_interval=args.interval
    )
    
    if args.benchmark:
        # Run benchmark
        comparator = MetricsComparator()
        result = await comparator.benchmark_installation(args.benchmark)
        print(f"\nBenchmark Results:")
        print(f"pip: {result['pip']['duration']:.2f}s")
        print(f"uv: {result['uv']['duration']:.2f}s")
        print(f"Speedup: {result['speedup']:.1f}x")
        
    elif args.dashboard:
        # Run dashboard
        dashboard = MetricsDashboard(collector)
        await dashboard.start()
        
        # Run collector in background
        await collector.run_continuous_monitoring()
        
    else:
        # Run continuous monitoring
        await collector.run_continuous_monitoring()


if __name__ == '__main__':
    asyncio.run(main())
```

#### SubTask 4.1.2: ëŒ€ì‹œë³´ë“œ êµ¬í˜„
**ë‹´ë‹¹ì**: í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 4ì‹œê°„

```typescript
// dashboard/src/components/UvMetricsDashboard.tsx
import React, { useState, useEffect } from 'react';
import {
  LineChart, Line, BarChart, Bar, PieChart, Pie,
  XAxis, YAxis, CartesianGrid, Tooltip, Legend,
  ResponsiveContainer, Cell
} from 'recharts';
import { Card, CardHeader, CardContent } from '@/components/ui/card';
import { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';
import { Badge } from '@/components/ui/badge';
import { Alert, AlertDescription } from '@/components/ui/alert';
import { 
  Activity, HardDrive, Download, Clock, 
  TrendingUp, AlertCircle, CheckCircle 
} from 'lucide-react';

interface MetricData {
  timestamp: string;
  cpu: number;
  memory: number;
  cacheSize: number;
  installTime: number;
  cacheHitRate: number;
}

interface ComparisonData {
  tool: string;
  avgInstallTime: number;
  cacheEfficiency: number;
  resourceUsage: number;
}

const UvMetricsDashboard: React.FC = () => {
  const [metrics, setMetrics] = useState<MetricData[]>([]);
  const [comparison, setComparison] = useState<ComparisonData[]>([]);
  const [selectedTimeRange, setSelectedTimeRange] = useState('1h');
  const [isLoading, setIsLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);

  // Fetch metrics data
  useEffect(() => {
    const fetchMetrics = async () => {
      try {
        const response = await fetch(`/api/metrics?range=${selectedTimeRange}`);
        const data = await response.json();
        setMetrics(data.metrics);
        setComparison(data.comparison);
        setIsLoading(false);
      } catch (err) {
        setError('Failed to fetch metrics');
        setIsLoading(false);
      }
    };

    fetchMetrics();
    const interval = setInterval(fetchMetrics, 5000); // Update every 5 seconds

    return () => clearInterval(interval);
  }, [selectedTimeRange]);

  // Calculate statistics
  const stats = metrics.length > 0 ? {
    avgInstallTime: metrics.reduce((sum, m) => sum + m.installTime, 0) / metrics.length,
    avgCacheHitRate: metrics.reduce((sum, m) => sum + m.cacheHitRate, 0) / metrics.length,
    totalCacheSize: metrics[metrics.length - 1]?.cacheSize || 0,
    peakCPU: Math.max(...metrics.map(m => m.cpu)),
    peakMemory: Math.max(...metrics.map(m => m.memory))
  } : null;

  const COLORS = ['#0088FE', '#00C49F', '#FFBB28', '#FF8042'];

  return (
    <div className="p-6 space-y-6">
      <div className="flex justify-between items-center">
        <h1 className="text-3xl font-bold">uv Performance Dashboard</h1>
        <div className="flex gap-2">
          {['1h', '6h', '24h', '7d'].map(range => (
            <button
              key={range}
              onClick={() => setSelectedTimeRange(range)}
              className={`px-4 py-2 rounded ${
                selectedTimeRange === range 
                  ? 'bg-blue-500 text-white' 
                  : 'bg-gray-200'
              }`}
            >
              {range}
            </button>
          ))}
        </div>
      </div>

      {error && (
        <Alert variant="destructive">
          <AlertCircle className="h-4 w-4" />
          <AlertDescription>{error}</AlertDescription>
        </Alert>
      )}

      {/* Summary Cards */}
      <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-5 gap-4">
        <Card>
          <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
            <h3 className="text-sm font-medium">Avg Install Time</h3>
            <Clock className="h-4 w-4 text-muted-foreground" />
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">
              {stats?.avgInstallTime.toFixed(2)}s
            </div>
            <p className="text-xs text-muted-foreground">
              <TrendingUp className="inline h-3 w-3 text-green-500" />
              {' '}85% faster than pip
            </p>
          </CardContent>
        </Card>

        <Card>
          <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
            <h3 className="text-sm font-medium">Cache Hit Rate</h3>
            <HardDrive className="h-4 w-4 text-muted-foreground" />
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">
              {stats?.avgCacheHitRate.toFixed(1)}%
            </div>
            <Badge variant="success">Excellent</Badge>
          </CardContent>
        </Card>

        <Card>
          <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
            <h3 className="text-sm font-medium">Cache Size</h3>
            <Download className="h-4 w-4 text-muted-foreground" />
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">
              {(stats?.totalCacheSize || 0 / 1024).toFixed(1)} GB
            </div>
            <p className="text-xs text-muted-foreground">
              {metrics.length > 1 && 
                `+${((metrics[metrics.length - 1].cacheSize - metrics[0].cacheSize) / 1024).toFixed(1)} GB today`
              }
            </p>
          </CardContent>
        </Card>

        <Card>
          <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
            <h3 className="text-sm font-medium">Peak CPU</h3>
            <Activity className="h-4 w-4 text-muted-foreground" />
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">
              {stats?.peakCPU.toFixed(1)}%
            </div>
            <p className="text-xs text-muted-foreground">Last hour</p>
          </CardContent>
        </Card>

        <Card>
          <CardHeader className="flex flex-row items-center justify-between space-y-0 pb-2">
            <h3 className="text-sm font-medium">Peak Memory</h3>
            <Activity className="h-4 w-4 text-muted-foreground" />
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">
              {stats?.peakMemory.toFixed(1)}%
            </div>
            <p className="text-xs text-muted-foreground">Last hour</p>
          </CardContent>
        </Card>
      </div>

      <Tabs defaultValue="performance" className="space-y-4">
        <TabsList>
          <TabsTrigger value="performance">Performance</TabsTrigger>
          <TabsTrigger value="comparison">pip vs uv</TabsTrigger>
          <TabsTrigger value="resources">Resources</TabsTrigger>
          <TabsTrigger value="cache">Cache Analytics</TabsTrigger>
        </TabsList>

        <TabsContent value="performance" className="space-y-4">
          <Card>
            <CardHeader>
              <h3 className="text-lg font-semibold">Installation Performance</h3>
            </CardHeader>
            <CardContent>
              <ResponsiveContainer width="100%" height={300}>
                <LineChart data={metrics}>
                  <CartesianGrid strokeDasharray="3 3" />
                  <XAxis dataKey="timestamp" />
                  <YAxis />
                  <Tooltip />
                  <Legend />
                  <Line 
                    type="monotone" 
                    dataKey="installTime" 
                    stroke="#8884d8" 
                    name="Install Time (s)"
                  />
                  <Line 
                    type="monotone" 
                    dataKey="cacheHitRate" 
                    stroke="#82ca9d" 
                    name="Cache Hit Rate (%)"
                  />
                </LineChart>
              </ResponsiveContainer>
            </CardContent>
          </Card>
        </TabsContent>

        <TabsContent value="comparison" className="space-y-4">
          <Card>
            <CardHeader>
              <h3 className="text-lg font-semibold">pip vs uv Comparison</h3>
            </CardHeader>
            <CardContent>
              <ResponsiveContainer width="100%" height={300}>
                <BarChart data={comparison}>
                  <CartesianGrid strokeDasharray="3 3" />
                  <XAxis dataKey="tool" />
                  <YAxis />
                  <Tooltip />
                  <Legend />
                  <Bar dataKey="avgInstallTime" fill="#8884d8" name="Avg Install Time (s)" />
                  <Bar dataKey="cacheEfficiency" fill="#82ca9d" name="Cache Efficiency (%)" />
                  <Bar dataKey="resourceUsage" fill="#ffc658" name="Resource Usage (%)" />
                </BarChart>
              </ResponsiveContainer>
            </CardContent>
          </Card>

          <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
            <Card>
              <CardHeader>
                <h3 className="text-lg font-semibold">Speed Improvement</h3>
              </CardHeader>
              <CardContent>
                <div className="text-center">
                  <div className="text-5xl font-bold text-green-500">12.5x</div>
                  <p className="text-sm text-muted-foreground mt-2">
                    Average speedup compared to pip
                  </p>
                </div>
              </CardContent>
            </Card>

            <Card>
              <CardHeader>
                <h3 className="text-lg font-semibold">Resource Efficiency</h3>
              </CardHeader>
              <CardContent>
                <ResponsiveContainer width="100%" height={200}>
                  <PieChart>
                    <Pie
                      data={[
                        { name: 'uv CPU', value: 15 },
                        { name: 'pip CPU', value: 45 },
                        { name: 'uv Memory', value: 20 },
                        { name: 'pip Memory', value: 35 }
                      ]}
                      cx="50%"
                      cy="50%"
                      outerRadius={80}
                      fill="#8884d8"
                      dataKey="value"
                    >
                      {comparison.map((entry, index) => (
                        <Cell key={`cell-${index}`} fill={COLORS[index % COLORS.length]} />
                      ))}
                    </Pie>
                    <Tooltip />
                  </PieChart>
                </ResponsiveContainer>
              </CardContent>
            </Card>
          </div>
        </TabsContent>

        <TabsContent value="resources" className="space-y-4">
          <Card>
            <CardHeader>
              <h3 className="text-lg font-semibold">System Resource Usage</h3>
            </CardHeader>
            <CardContent>
              <ResponsiveContainer width="100%" height={300}>
                <LineChart data={metrics}>
                  <CartesianGrid strokeDasharray="3 3" />
                  <XAxis dataKey="timestamp" />
                  <YAxis />
                  <Tooltip />
                  <Legend />
                  <Line 
                    type="monotone" 
                    dataKey="cpu" 
                    stroke="#ff7300" 
                    name="CPU Usage (%)"
                  />
                  <Line 
                    type="monotone" 
                    dataKey="memory" 
                    stroke="#387908" 
                    name="Memory Usage (%)"
                  />
                </LineChart>
              </ResponsiveContainer>
            </CardContent>
          </Card>
        </TabsContent>

        <TabsContent value="cache" className="space-y-4">
          <Card>
            <CardHeader>
              <h3 className="text-lg font-semibold">Cache Analytics</h3>
            </CardHeader>
            <CardContent>
              <ResponsiveContainer width="100%" height={300}>
                <LineChart data={metrics}>
                  <CartesianGrid strokeDasharray="3 3" />
                  <XAxis dataKey="timestamp" />
                  <YAxis yAxisId="left" />
                  <YAxis yAxisId="right" orientation="right" />
                  <Tooltip />
                  <Legend />
                  <Line 
                    yAxisId="left"
                    type="monotone" 
                    dataKey="cacheSize" 
                    stroke="#8884d8" 
                    name="Cache Size (MB)"
                  />
                  <Line 
                    yAxisId="right"
                    type="monotone" 
                    dataKey="cacheHitRate" 
                    stroke="#82ca9d" 
                    name="Hit Rate (%)"
                  />
                </LineChart>
              </ResponsiveContainer>
            </CardContent>
          </Card>

          <div className="grid grid-cols-1 md:grid-cols-3 gap-4">
            <Card>
              <CardContent className="pt-6">
                <div className="flex items-center justify-between">
                  <span className="text-sm font-medium">Cache Efficiency</span>
                  <CheckCircle className="h-4 w-4 text-green-500" />
                </div>
                <div className="mt-2">
                  <div className="h-2 bg-gray-200 rounded-full">
                    <div 
                      className="h-2 bg-green-500 rounded-full"
                      style={{ width: `${stats?.avgCacheHitRate || 0}%` }}
                    />
                  </div>
                </div>
              </CardContent>
            </Card>

            <Card>
              <CardContent className="pt-6">
                <div className="text-center">
                  <p className="text-sm text-muted-foreground">Packages Cached</p>
                  <p className="text-2xl font-bold">1,247</p>
                </div>
              </CardContent>
            </Card>

            <Card>
              <CardContent className="pt-6">
                <div className="text-center">
                  <p className="text-sm text-muted-foreground">Cache Savings</p>
                  <p className="text-2xl font-bold">4.2 GB</p>
                </div>
              </CardContent>
            </Card>
          </div>
        </TabsContent>
      </Tabs>
    </div>
  );
};

export default UvMetricsDashboard;
```

```python
# api/metrics_api.py
from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional, List
from datetime import datetime, timedelta
import asyncio
from pydantic import BaseModel

app = FastAPI(title="uv Metrics API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class MetricPoint(BaseModel):
    timestamp: str
    cpu: float
    memory: float
    cacheSize: float
    installTime: float
    cacheHitRate: float

class ComparisonData(BaseModel):
    tool: str
    avgInstallTime: float
    cacheEfficiency: float
    resourceUsage: float

class MetricsResponse(BaseModel):
    metrics: List[MetricPoint]
    comparison: List[ComparisonData]

@app.get("/api/metrics", response_model=MetricsResponse)
async def get_metrics(range: str = Query("1h", regex="^(1h|6h|24h|7d)$")):
    """Get metrics for the specified time range"""
    
    # Calculate time range
    now = datetime.now()
    range_map = {
        "1h": timedelta(hours=1),
        "6h": timedelta(hours=6),
        "24h": timedelta(days=1),
        "7d": timedelta(days=7)
    }
    
    start_time = now - range_map[range]
    
    # Fetch metrics from database/collector
    # This is mock data - replace with actual data source
    metrics = []
    for i in range(20):
        timestamp = start_time + timedelta(minutes=i * 3)
        metrics.append(MetricPoint(
            timestamp=timestamp.isoformat(),
            cpu=15 + (i % 5) * 2,
            memory=30 + (i % 3) * 5,
            cacheSize=1024 + i * 50,
            installTime=0.5 + (i % 4) * 0.1,
            cacheHitRate=85 + (i % 3) * 5
        ))
    
    comparison = [
        ComparisonData(
            tool="pip",
            avgInstallTime=6.5,
            cacheEfficiency=45,
            resourceUsage=65
        ),
        ComparisonData(
            tool="uv",
            avgInstallTime=0.5,
            cacheEfficiency=92,
            resourceUsage=25
        )
    ]
    
    return MetricsResponse(metrics=metrics, comparison=comparison)

@app.get("/health")
async def health_check():
    return {"status": "healthy"}
```

#### SubTask 4.1.3: ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬í˜„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```python
#!/usr/bin/env python3
# scripts/alerting_system.py

import asyncio
import aiohttp
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
import json
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import logging

class AlertSeverity(Enum):
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

class AlertType(Enum):
    PERFORMANCE_DEGRADATION = "performance_degradation"
    HIGH_ERROR_RATE = "high_error_rate"
    CACHE_ISSUE = "cache_issue"
    RESOURCE_EXHAUSTION = "resource_exhaustion"
    INSTALLATION_FAILURE = "installation_failure"

@dataclass
class Alert:
    id: str
    type: AlertType
    severity: AlertSeverity
    title: str
    message: str
    timestamp: datetime
    metrics: Dict[str, Any]
    resolved: bool = False
    resolved_at: Optional[datetime] = None

@dataclass
class AlertRule:
    name: str
    type: AlertType
    condition: str
    threshold: float
    duration: timedelta
    severity: AlertSeverity
    
class AlertingSystem:
    """Alert management and notification system"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.alerts: Dict[str, Alert] = {}
        self.alert_rules: List[AlertRule] = self._initialize_rules()
        self.logger = logging.getLogger(__name__)
        
    def _initialize_rules(self) -> List[AlertRule]:
        """Initialize default alert rules"""
        return [
            AlertRule(
                name="slow_installation",
                type=AlertType.PERFORMANCE_DEGRADATION,
                condition="avg_install_time > threshold",
                threshold=5.0,  # seconds
                duration=timedelta(minutes=5),
                severity=AlertSeverity.WARNING
            ),
            AlertRule(
                name="high_error_rate",
                type=AlertType.HIGH_ERROR_RATE,
                condition="error_rate > threshold",
                threshold=0.05,  # 5%
                duration=timedelta(minutes=2),
                severity=AlertSeverity.ERROR
            ),
            AlertRule(
                name="low_cache_hit_rate",
                type=AlertType.CACHE_ISSUE,
                condition="cache_hit_rate < threshold",
                threshold=0.7,  # 70%
                duration=timedelta(minutes=10),
                severity=AlertSeverity.WARNING
            ),
            AlertRule(
                name="high_cpu_usage",
                type=AlertType.RESOURCE_EXHAUSTION,
                condition="cpu_usage > threshold",
                threshold=80.0,  # 80%
                duration=timedelta(minutes=5),
                severity=AlertSeverity.WARNING
            ),
            AlertRule(
                name="high_memory_usage",
                type=AlertType.RESOURCE_EXHAUSTION,
                condition="memory_usage > threshold",
                threshold=85.0,  # 85%
                duration=timedelta(minutes=5),
                severity=AlertSeverity.ERROR
            ),
            AlertRule(
                name="installation_failures",
                type=AlertType.INSTALLATION_FAILURE,
                condition="failure_count > threshold",
                threshold=3,  # 3 consecutive failures
                duration=timedelta(minutes=1),
                severity=AlertSeverity.CRITICAL
            )
        ]
    
    async def check_metrics(self, metrics: Dict[str, Any]):
        """Check metrics against alert rules"""
        for rule in self.alert_rules:
            if self._evaluate_rule(rule, metrics):
                await self._trigger_alert(rule, metrics)
            else:
                await self._resolve_alert_if_exists(rule)
    
    def _evaluate_rule(self, rule: AlertRule, metrics: Dict[str, Any]) -> bool:
        """Evaluate if a rule condition is met"""
        try:
            # Simple evaluation - in production, use a proper expression evaluator
            if rule.name == "slow_installation":
                return metrics.get('avg_install_time', 0) > rule.threshold
            elif rule.name == "high_error_rate":
                return metrics.get('error_rate', 0) > rule.threshold
            elif rule.name == "low_cache_hit_rate":
                return metrics.get('cache_hit_rate', 1) < rule.threshold
            elif rule.name == "high_cpu_usage":
                return metrics.get('cpu_usage', 0) > rule.threshold
            elif rule.name == "high_memory_usage":
                return metrics.get('memory_usage', 0) > rule.threshold
            elif rule.name == "installation_failures":
                return metrics.get('failure_count', 0) > rule.threshold
            
            return False
            
        except Exception as e:
            self.logger.error(f"Error evaluating rule {rule.name}: {e}")
            return False
    
    async def _trigger_alert(self, rule: AlertRule, metrics: Dict[str, Any]):
        """Trigger a new alert or update existing one"""
        alert_id = f"{rule.name}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        
        # Check if similar alert already exists
        existing_alert = self._find_existing_alert(rule)
        if existing_alert and not existing_alert.resolved:
            # Update existing alert
            existing_alert.timestamp = datetime.now()
            existing_alert.metrics = metrics
            return
        
        # Create new alert
        alert = Alert(
            id=alert_id,
            type=rule.type,
            severity=rule.severity,
            title=self._generate_alert_title(rule, metrics),
            message=self._generate_alert_message(rule, metrics),
            timestamp=datetime.now(),
            metrics=metrics
        )
        
        self.alerts[alert_id] = alert
        
        # Send notifications
        await self._send_notifications(alert)
        
    def _find_existing_alert(self, rule: AlertRule) -> Optional[Alert]:
        """Find existing unresolved alert for the same rule"""
        for alert in self.alerts.values():
            if (alert.type == rule.type and 
                not alert.resolved and 
                alert.title.startswith(rule.name)):
                return alert
        return None
    
    async def _resolve_alert_if_exists(self, rule: AlertRule):
        """Resolve alert if condition is no longer met"""
        alert = self._find_existing_alert(rule)
        if alert:
            alert.resolved = True
            alert.resolved_at = datetime.now()
            await self._send_resolution_notification(alert)
    
    def _generate_alert_title(self, rule: AlertRule, metrics: Dict[str, Any]) -> str:
        """Generate alert title"""
        titles = {
            "slow_installation": f"Slow package installation detected ({metrics.get('avg_install_time', 0):.1f}s average)",
            "high_error_rate": f"High error rate detected ({metrics.get('error_rate', 0)*100:.1f}%)",
            "low_cache_hit_rate": f"Low cache hit rate ({metrics.get('cache_hit_rate', 0)*100:.1f}%)",
            "high_cpu_usage": f"High CPU usage ({metrics.get('cpu_usage', 0):.1f}%)",
            "high_memory_usage": f"High memory usage ({metrics.get('memory_usage', 0):.1f}%)",
            "installation_failures": f"Multiple installation failures ({metrics.get('failure_count', 0)} failures)"
        }
        return titles.get(rule.name, f"Alert: {rule.name}")
    
    def _generate_alert_message(self, rule: AlertRule, metrics: Dict[str, Any]) -> str:
        """Generate detailed alert message"""
        messages = {
            "slow_installation": f"""
Package installations are taking longer than expected.
Current average: {metrics.get('avg_install_time', 0):.1f}s
Threshold: {rule.threshold}s
Consider checking network connectivity or cache status.
""",
            "high_error_rate": f"""
Error rate has exceeded acceptable threshold.
Current rate: {metrics.get('error_rate', 0)*100:.1f}%
Threshold: {rule.threshold*100:.1f}%
Recent errors: {metrics.get('recent_errors', [])}
""",
            "low_cache_hit_rate": f"""
Cache hit rate is below optimal level.
Current rate: {metrics.get('cache_hit_rate', 0)*100:.1f}%
Threshold: {rule.threshold*100:.1f}%
This may impact installation performance.
"""
        }
        return messages.get(rule.name, f"Alert condition met: {rule.condition}")
    
    async def _send_notifications(self, alert: Alert):
        """Send alert notifications through configured channels"""
        tasks = []
        
        if self.config.get('slack_webhook'):
            tasks.append(self._send_slack_notification(alert))
        
        if self.config.get('email_enabled'):
            tasks.append(self._send_email_notification(alert))
        
        if self.config.get('webhook_url'):
            tasks.append(self._send_webhook_notification(alert))
        
        if self.config.get('pagerduty_enabled') and alert.severity == AlertSeverity.CRITICAL:
            tasks.append(self._send_pagerduty_notification(alert))
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _send_slack_notification(self, alert: Alert):
        """Send notification to Slack"""
        webhook_url = self.config['slack_webhook']
        
        color_map = {
            AlertSeverity.INFO: "#36a64f",
            AlertSeverity.WARNING: "#ff9800",
            AlertSeverity.ERROR: "#f44336",
            AlertSeverity.CRITICAL: "#d32f2f"
        }
        
        payload = {
            "attachments": [{
                "color": color_map[alert.severity],
                "title": f"{alert.severity.value.upper()}: {alert.title}",
                "text": alert.message,
                "fields": [
                    {
                        "title": "Alert Type",
                        "value": alert.type.value,
                        "short": True
                    },
                    {
                        "title": "Timestamp",
                        "value": alert.timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                        "short": True
                    }
                ],
                "footer": "uv Monitoring System",
                "ts": int(alert.timestamp.timestamp())
            }]
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(webhook_url, json=payload) as response:
                if response.status != 200:
                    self.logger.error(f"Failed to send Slack notification: {response.status}")
    
    async def _send_email_notification(self, alert: Alert):
        """Send email notification"""
        if not all(k in self.config for k in ['smtp_host', 'smtp_port', 'email_from', 'email_to']):
            return
        
        msg = MIMEMultipart()
        msg['From'] = self.config['email_from']
        msg['To'] = ', '.join(self.config['email_to'])
        msg['Subject'] = f"[{alert.severity.value.upper()}] {alert.title}"
        
        body = f"""
Alert Details:
--------------
Type: {alert.type.value}
Severity: {alert.severity.value}
Time: {alert.timestamp.strftime("%Y-%m-%d %H:%M:%S")}

Message:
{alert.message}

Metrics:
{json.dumps(alert.metrics, indent=2)}

--
uv Monitoring System
"""
        
        msg.attach(MIMEText(body, 'plain'))
        
        try:
            with smtplib.SMTP(self.config['smtp_host'], self.config['smtp_port']) as server:
                if self.config.get('smtp_tls'):
                    server.starttls()
                if self.config.get('smtp_user') and self.config.get('smtp_password'):
                    server.login(self.config['smtp_user'], self.config['smtp_password'])
                server.send_message(msg)
        except Exception as e:
            self.logger.error(f"Failed to send email notification: {e}")
    
    async def _send_webhook_notification(self, alert: Alert):
        """Send generic webhook notification"""
        webhook_url = self.config['webhook_url']
        
        payload = {
            "alert_id": alert.id,
            "type": alert.type.value,
            "severity": alert.severity.value,
            "title": alert.title,
            "message": alert.message,
            "timestamp": alert.timestamp.isoformat(),
            "metrics": alert.metrics
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(webhook_url, json=payload) as response:
                if response.status != 200:
                    self.logger.error(f"Failed to send webhook notification: {response.status}")
    
    async def _send_pagerduty_notification(self, alert: Alert):
        """Send PagerDuty notification for critical alerts"""
        if not self.config.get('pagerduty_routing_key'):
            return
        
        url = "https://events.pagerduty.com/v2/enqueue"
        
        payload = {
            "routing_key": self.config['pagerduty_routing_key'],
            "event_action": "trigger",
            "dedup_key": alert.id,
            "payload": {
                "summary": alert.title,
                "severity": "error",
                "source": "uv-monitoring",
                "custom_details": {
                    "message": alert.message,
                    "metrics": alert.metrics
                }
            }
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload) as response:
                if response.status != 202:
                    self.logger.error(f"Failed to send PagerDuty notification: {response.status}")
    
    async def _send_resolution_notification(self, alert: Alert):
        """Send notification when alert is resolved"""
        resolution_message = f"""
Alert Resolved: {alert.title}
Duration: {(alert.resolved_at - alert.timestamp).total_seconds() / 60:.1f} minutes
Resolved at: {alert.resolved_at.strftime("%Y-%m-%d %H:%M:%S")}
"""
        
        # Create a resolution alert
        resolution_alert = Alert(
            id=f"{alert.id}_resolved",
            type=alert.type,
            severity=AlertSeverity.INFO,
            title=f"RESOLVED: {alert.title}",
            message=resolution_message,
            timestamp=alert.resolved_at,
            metrics=alert.metrics,
            resolved=True,
            resolved_at=alert.resolved_at
        )
        
        await self._send_notifications(resolution_alert)
    
    def get_active_alerts(self) -> List[Alert]:
        """Get all active (unresolved) alerts"""
        return [alert for alert in self.alerts.values() if not alert.resolved]
    
    def get_alert_history(self, 
                         start_time: Optional[datetime] = None,
                         end_time: Optional[datetime] = None,
                         severity: Optional[AlertSeverity] = None,
                         alert_type: Optional[AlertType] = None) -> List[Alert]:
        """Get alert history with optional filters"""
        alerts = list(self.alerts.values())
        
        if start_time:
            alerts = [a for a in alerts if a.timestamp >= start_time]
        
        if end_time:
            alerts = [a for a in alerts if a.timestamp <= end_time]
        
        if severity:
            alerts = [a for a in alerts if a.severity == severity]
        
        if alert_type:
            alerts = [a for a in alerts if a.type == alert_type]
        
        return sorted(alerts, key=lambda a: a.timestamp, reverse=True)
    
    def get_alert_statistics(self) -> Dict[str, Any]:
        """Get alert statistics"""
        total_alerts = len(self.alerts)
        active_alerts = len(self.get_active_alerts())
        
        severity_counts = {}
        type_counts = {}
        
        for alert in self.alerts.values():
            severity_counts[alert.severity.value] = severity_counts.get(alert.severity.value, 0) + 1
            type_counts[alert.type.value] = type_counts.get(alert.type.value, 0) + 1
        
        # Calculate MTTR (Mean Time To Resolution)
        resolved_alerts = [a for a in self.alerts.values() if a.resolved and a.resolved_at]
        if resolved_alerts:
            total_resolution_time = sum(
                (a.resolved_at - a.timestamp).total_seconds() 
                for a in resolved_alerts
            )
            mttr = total_resolution_time / len(resolved_alerts) / 60  # in minutes
        else:
            mttr = 0
        
        return {
            "total_alerts": total_alerts,
            "active_alerts": active_alerts,
            "resolved_alerts": total_alerts - active_alerts,
            "severity_distribution": severity_counts,
            "type_distribution": type_counts,
            "mean_time_to_resolution_minutes": round(mttr, 2)
        }


# Alert configuration example
ALERT_CONFIG = {
    # Slack
    "slack_webhook": "https://hooks.slack.com/services/YOUR/WEBHOOK/URL",
    
    # Email
    "email_enabled": True,
    "smtp_host": "smtp.gmail.com",
    "smtp_port": 587,
    "smtp_tls": True,
    "smtp_user": "alerts@example.com",
    "smtp_password": "your-password",
    "email_from": "alerts@example.com",
    "email_to": ["devops@example.com", "oncall@example.com"],
    
    # Generic webhook
    "webhook_url": "https://your-monitoring-system.com/webhooks/alerts",
    
    # PagerDuty
    "pagerduty_enabled": True,
    "pagerduty_routing_key": "YOUR_PAGERDUTY_ROUTING_KEY"
}


# CLI for testing alerts
async def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='uv Alert System')
    parser.add_argument('--test', action='store_true', help='Test alert system')
    parser.add_argument('--check-metrics', help='Check metrics from file')
    parser.add_argument('--list-alerts', action='store_true', help='List active alerts')
    parser.add_argument('--stats', action='store_true', help='Show alert statistics')
    
    args = parser.parse_args()
    
    # Initialize alerting system
    alerting = AlertingSystem(ALERT_CONFIG)
    
    if args.test:
        # Test with sample metrics
        test_metrics = {
            "avg_install_time": 7.5,  # Trigger slow installation
            "error_rate": 0.08,       # Trigger high error rate
            "cache_hit_rate": 0.65,   # Trigger low cache hit
            "cpu_usage": 45,
            "memory_usage": 60,
            "failure_count": 1
        }
        
        print("Testing alert system with sample metrics...")
        await alerting.check_metrics(test_metrics)
        
        print("\nActive alerts:")
        for alert in alerting.get_active_alerts():
            print(f"- [{alert.severity.value}] {alert.title}")
        
    elif args.check_metrics:
        # Load metrics from file
        with open(args.check_metrics, 'r') as f:
            metrics = json.load(f)
        
        await alerting.check_metrics(metrics)
        
    elif args.list_alerts:
        # List active alerts
        active_alerts = alerting.get_active_alerts()
        if active_alerts:
            print("Active Alerts:")
            for alert in active_alerts:
                print(f"\nID: {alert.id}")
                print(f"Type: {alert.type.value}")
                print(f"Severity: {alert.severity.value}")
                print(f"Title: {alert.title}")
                print(f"Time: {alert.timestamp}")
        else:
            print("No active alerts")
    
    elif args.stats:
        # Show statistics
        stats = alerting.get_alert_statistics()
        print("Alert Statistics:")
        print(f"Total alerts: {stats['total_alerts']}")
        print(f"Active alerts: {stats['active_alerts']}")
        print(f"Resolved alerts: {stats['resolved_alerts']}")
        print(f"MTTR: {stats['mean_time_to_resolution_minutes']} minutes")
        print("\nSeverity distribution:")
        for severity, count in stats['severity_distribution'].items():
            print(f"  {severity}: {count}")
        print("\nType distribution:")
        for alert_type, count in stats['type_distribution'].items():
            print(f"  {alert_type}: {count}")


if __name__ == '__main__':
    asyncio.run(main())
```

### Task 4.2: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ìµœì í™”

#### SubTask 4.2.1: ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ êµ¬í˜„
**ë‹´ë‹¹ì**: ì„±ëŠ¥ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 4ì‹œê°„

```python
#!/usr/bin/env python3
# scripts/benchmark_suite.py

import asyncio
import time
import tempfile
import subprocess
import json
import statistics
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, asdict
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from datetime import datetime
import psutil
import numpy as np

@dataclass
class BenchmarkResult:
    tool: str
    operation: str
    duration: float
    cpu_usage: float
    memory_usage: float
    cache_hits: int
    cache_misses: int
    packages_installed: int
    success: bool
    error: Optional[str] = None
    
@dataclass
class BenchmarkScenario:
    name: str
    description: str
    requirements: List[str]
    clean_cache: bool = False
    cold_start: bool = False
    parallel: bool = False
    
class BenchmarkSuite:
    """Comprehensive benchmark suite for pip vs uv"""
    
    def __init__(self, output_dir: str = "./benchmark_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.results: List[BenchmarkResult] = []
        self.scenarios = self._define_scenarios()
        
    def _define_scenarios(self) -> List[BenchmarkScenario]:
        """Define benchmark scenarios"""
        return [
            BenchmarkScenario(
                name="small_project",
                description="Small project with common dependencies",
                requirements=[
                    "requests==2.31.0",
                    "click==8.1.7",
                    "pydantic==2.5.0"
                ]
            ),
            BenchmarkScenario(
                name="medium_project",
                description="Medium project with mixed dependencies",
                requirements=[
                    "fastapi==0.104.1",
                    "sqlalchemy==2.0.23",
                    "pytest==7.4.3",
                    "black==23.11.0",
                    "mypy==1.7.1"
                ]
            ),
            BenchmarkScenario(
                name="large_project",
                description="Large project with many dependencies",
                requirements=[
                    "numpy==1.24.3",
                    "pandas==2.0.3",
                    "scikit-learn==1.3.0",
                    "matplotlib==3.7.2",
                    "jupyter==1.0.0",
                    "tensorflow==2.13.0"
                ]
            ),
            BenchmarkScenario(
                name="cold_cache",
                description="Installation with empty cache",
                requirements=[
                    "django==4.2.7",
                    "celery==5.3.4",
                    "redis==5.0.1"
                ],
                clean_cache=True,
                cold_start=True
            ),
            BenchmarkScenario(
                name="warm_cache",
                description="Installation with populated cache",
                requirements=[
                    "django==4.2.7",
                    "celery==5.3.4",
                    "redis==5.0.1"
                ],
                clean_cache=False
            ),
            BenchmarkScenario(
                name="parallel_install",
                description="Parallel installation test",
                requirements=[
                    "aiohttp==3.9.0",
                    "httpx==0.25.1",
                    "requests==2.31.0",
                    "urllib3==2.1.0"
                ],
                parallel=True
            ),
            BenchmarkScenario(
                name="complex_deps",
                description="Complex dependency resolution",
                requirements=[
                    "apache-airflow==2.7.3",
                    "dask==2023.11.0",
                    "ray==2.8.0"
                ]
            )
        ]
    
    async def run_benchmark(self, tool: str, scenario: BenchmarkScenario) -> BenchmarkResult:
        """Run a single benchmark"""
        # Create temporary directory for isolated environment
        with tempfile.TemporaryDirectory() as tmpdir:
            venv_path = Path(tmpdir) / "venv"
            
            # Clean cache if requested
            if scenario.clean_cache:
                self._clean_cache(tool)
            
            # Create virtual environment
            if tool == "uv":
                await self._run_command(["uv", "venv", str(venv_path)])
                pip_cmd = [str(venv_path / "bin" / "pip")]
                install_cmd = ["uv", "pip", "install", "--python", str(venv_path / "bin" / "python")]
            else:
                await self._run_command(["python", "-m", "venv", str(venv_path)])
                pip_cmd = [str(venv_path / "bin" / "pip")]
                install_cmd = pip_cmd + ["install"]
            
            # Create requirements file
            req_file = Path(tmpdir) / "requirements.txt"
            req_file.write_text("\n".join(scenario.requirements))
            
            # Measure installation
            start_time = time.time()
            process_start = psutil.Process()
            
            try:
                # Run installation
                if scenario.parallel and tool == "uv":
                    # uv supports parallel downloads by default
                    result = await self._run_command(
                        install_cmd + ["-r", str(req_file)],
                        capture_output=True
                    )
                else:
                    result = await self._run_command(
                        install_cmd + ["-r", str(req_file)],
                        capture_output=True
                    )
                
                duration = time.time() - start_time
                
                # Collect metrics
                cpu_percent = process_start.cpu_percent()
                memory_info = process_start.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024
                
                # Parse output for cache statistics (uv specific)
                cache_hits = 0
                cache_misses = 0
                if tool == "uv" and result.stdout:
                    cache_hits, cache_misses = self._parse_cache_stats(result.stdout)
                
                # Count installed packages
                list_result = await self._run_command(
                    pip_cmd + ["list", "--format=json"],
                    capture_output=True
                )
                packages = json.loads(list_result.stdout) if list_result.stdout else []
                
                return BenchmarkResult(
                    tool=tool,
                    operation=scenario.name,
                    duration=duration,
                    cpu_usage=cpu_percent,
                    memory_usage=memory_mb,
                    cache_hits=cache_hits,
                    cache_misses=cache_misses,
                    packages_installed=len(packages),
                    success=True
                )
                
            except Exception as e:
                return BenchmarkResult(
                    tool=tool,
                    operation=scenario.name,
                    duration=time.time() - start_time,
                    cpu_usage=0,
                    memory_usage=0,
                    cache_hits=0,
                    cache_misses=0,
                    packages_installed=0,
                    success=False,
                    error=str(e)
                )
    
    async def _run_command(self, cmd: List[str], capture_output: bool = False):
        """Run a command asynchronously"""
        if capture_output:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await process.communicate()
            return type('Result', (), {
                'stdout': stdout.decode() if stdout else '',
                'stderr': stderr.decode() if stderr else '',
                'returncode': process.returncode
            })()
        else:
            process = await asyncio.create_subprocess_exec(*cmd)
            await process.wait()
            return type('Result', (), {'returncode': process.returncode})()
    
    def _clean_cache(self, tool: str):
        """Clean tool cache"""
        if tool == "uv":
            cache_dir = Path.home() / ".cache" / "uv"
        else:
            cache_dir = Path.home() / ".cache" / "pip"
        
        if cache_dir.exists():
            import shutil
            shutil.rmtree(cache_dir)
    
    def _parse_cache_stats(self, output: str) -> Tuple[int, int]:
        """Parse cache statistics from uv output"""
        # This would need to be implemented based on actual uv output
        # For now, return mock data
        import re
        
        hits = len(re.findall(r'Using cached', output))
        misses = len(re.findall(r'Downloading', output))
        
        return hits, misses
    
    async def run_all_benchmarks(self, iterations: int = 3):
        """Run all benchmark scenarios"""
        print(f"Running benchmark suite with {iterations} iterations per scenario...")
        
        for scenario in self.scenarios:
            print(f"\nğŸ“Š Scenario: {scenario.name}")
            print(f"   {scenario.description}")
            
            for tool in ["pip", "uv"]:
                tool_results = []
                
                for i in range(iterations):
                    print(f"   Running {tool} iteration {i+1}/{iterations}...")
                    result = await self.run_benchmark(tool, scenario)
                    tool_results.append(result)
                    self.results.append(result)
                    
                    if not result.success:
                        print(f"   âš ï¸  Failed: {result.error}")
                
                # Print summary for this tool/scenario
                if tool_results:
                    successful_results = [r for r in tool_results if r.success]
                    if successful_results:
                        avg_duration = statistics.mean(r.duration for r in successful_results)
                        print(f"   {tool}: {avg_duration:.2f}s average")
    
    def analyze_results(self) -> Dict[str, Any]:
        """Analyze benchmark results"""
        if not self.results:
            return {}
        
        # Convert to DataFrame for easier analysis
        df = pd.DataFrame([asdict(r) for r in self.results])
        
        analysis = {
            "summary": {},
            "by_scenario": {},
            "speedup": {}
        }
        
        # Overall summary
        for tool in ["pip", "uv"]:
            tool_df = df[(df['tool'] == tool) & (df['success'] == True)]
            if not tool_df.empty:
                analysis["summary"][tool] = {
                    "avg_duration": tool_df['duration'].mean(),
                    "median_duration": tool_df['duration'].median(),
                    "std_duration": tool_df['duration'].std(),
                    "min_duration": tool_df['duration'].min(),
                    "max_duration": tool_df['duration'].max(),
                    "avg_cpu": tool_df['cpu_usage'].mean(),
                    "avg_memory": tool_df['memory_usage'].mean(),
                    "success_rate": len(tool_df) / len(df[df['tool'] == tool]) * 100
                }
        
        # Analysis by scenario
        for scenario in df['operation'].unique():
            scenario_df = df[df['operation'] == scenario]
            analysis["by_scenario"][scenario] = {}
            
            for tool in ["pip", "uv"]:
                tool_scenario_df = scenario_df[(scenario_df['tool'] == tool) & (scenario_df['success'] == True)]
                if not tool_scenario_df.empty:
                    analysis["by_scenario"][scenario][tool] = {
                        "avg_duration": tool_scenario_df['duration'].mean(),
                        "avg_cpu": tool_scenario_df['cpu_usage'].mean(),
                        "avg_memory": tool_scenario_df['memory_usage'].mean()
                    }
            
            # Calculate speedup
            if "pip" in analysis["by_scenario"][scenario] and "uv" in analysis["by_scenario"][scenario]:
                pip_time = analysis["by_scenario"][scenario]["pip"]["avg_duration"]
                uv_time = analysis["by_scenario"][scenario]["uv"]["avg_duration"]
                if uv_time > 0:
                    analysis["speedup"][scenario] = pip_time / uv_time
        
        return analysis
    
    def generate_report(self):
        """Generate comprehensive benchmark report"""
        analysis = self.analyze_results()
        
        if not analysis:
            print("No results to analyze")
            return
        
        # Create report
        report_path = self.output_dir / f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        with open(report_path, 'w') as f:
            f.write("# uv vs pip Benchmark Report\n\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # Summary
            f.write("## Executive Summary\n\n")
            
            if "pip" in analysis["summary"] and "uv" in analysis["summary"]:
                pip_avg = analysis["summary"]["pip"]["avg_duration"]
                uv_avg = analysis["summary"]["uv"]["avg_duration"]
                overall_speedup = pip_avg / uv_avg if uv_avg > 0 else 0
                
                f.write(f"- **Overall speedup**: {overall_speedup:.1f}x\n")
                f.write(f"- **pip average**: {pip_avg:.2f}s\n")
                f.write(f"- **uv average**: {uv_avg:.2f}s\n")
                f.write(f"- **Time saved**: {pip_avg - uv_avg:.2f}s ({(1 - uv_avg/pip_avg)*100:.1f}%)\n\n")
            
            # Detailed results by scenario
            f.write("## Results by Scenario\n\n")
            
            for scenario, data in analysis["by_scenario"].items():
                f.write(f"### {scenario}\n\n")
                
                if "pip" in data and "uv" in data:
                    speedup = analysis["speedup"].get(scenario, 0)
                    f.write(f"**Speedup: {speedup:.1f}x**\n\n")
                    
                    f.write("| Metric | pip | uv | Improvement |\n")
                    f.write("|--------|-----|----|--------------|\n")
                    f.write(f"| Duration | {data['pip']['avg_duration']:.2f}s | {data['uv']['avg_duration']:.2f}s | {speedup:.1f}x |\n")
                    f.write(f"| CPU Usage | {data['pip']['avg_cpu']:.1f}% | {data['uv']['avg_cpu']:.1f}% | {(data['pip']['avg_cpu'] - data['uv']['avg_cpu']) / data['pip']['avg_cpu'] * 100:.1f}% |\n")
                    f.write(f"| Memory | {data['pip']['avg_memory']:.1f}MB | {data['uv']['avg_memory']:.1f}MB | {(data['pip']['avg_memory'] - data['uv']['avg_memory']) / data['pip']['avg_memory'] * 100:.1f}% |\n\n")
            
            # Recommendations
            f.write("## Recommendations\n\n")
            f.write("Based on the benchmark results:\n\n")
            
            if overall_speedup > 5:
                f.write("1. **Strongly recommend** switching to uv for all environments\n")
            elif overall_speedup > 2:
                f.write("1. **Recommend** switching to uv, especially for CI/CD pipelines\n")
            else:
                f.write("1. Consider switching to uv for improved performance\n")
            
            f.write("2. uv shows the most improvement in scenarios with:\n")
            
            # Find scenarios with best improvement
            sorted_speedups = sorted(analysis["speedup"].items(), key=lambda x: x[1], reverse=True)
            for scenario, speedup in sorted_speedups[:3]:
                f.write(f"   - {scenario}: {speedup:.1f}x faster\n")
        
        print(f"\nğŸ“„ Report generated: {report_path}")
        
        # Generate visualizations
        self._generate_visualizations(analysis)
    
    def _generate_visualizations(self, analysis: Dict[str, Any]):
        """Generate benchmark visualizations"""
        # Set style
        plt.style.use('seaborn-v0_8-darkgrid')
        
        # Create figure with subplots
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('uv vs pip Benchmark Results', fontsize=16)
        
        # 1. Overall comparison bar chart
        ax1 = axes[0, 0]
        tools = ['pip', 'uv']
        durations = [
            analysis["summary"].get(tool, {}).get("avg_duration", 0) 
            for tool in tools
        ]
        bars = ax1.bar(tools, durations, color=['#FF6B6B', '#4ECDC4'])
        ax1.set_ylabel('Average Duration (seconds)')
        ax1.set_title('Overall Performance Comparison')
        
        # Add value labels
        for bar, duration in zip(bars, durations):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                    f'{duration:.1f}s', ha='center', va='bottom')
        
        # 2. Speedup by scenario
        ax2 = axes[0, 1]
        scenarios = list(analysis["speedup"].keys())
        speedups = list(analysis["speedup"].values())
        
        bars = ax2.barh(scenarios, speedups, color='#95E1D3')
        ax2.set_xlabel('Speedup Factor')
        ax2.set_title('Speedup by Scenario')
        ax2.axvline(x=1, color='red', linestyle='--', alpha=0.5)
        
        # Add value labels
        for bar, speedup in zip(bars, speedups):
            ax2.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,
                    f'{speedup:.1f}x', ha='left', va='center')
        
        # 3. Resource usage comparison
        ax3 = axes[1, 0]
        df = pd.DataFrame([asdict(r) for r in self.results if r.success])
        
        if not df.empty:
            resource_data = df.groupby('tool')[['cpu_usage', 'memory_usage']].mean()
            resource_data.plot(kind='bar', ax=ax3, rot=0)
            ax3.set_ylabel('Usage')
            ax3.set_title('Average Resource Usage')
            ax3.legend(['CPU (%)', 'Memory (MB)'])
        
        # 4. Duration distribution
        ax4 = axes[1, 1]
        if not df.empty:
            pip_durations = df[df['tool'] == 'pip']['duration']
            uv_durations = df[df['tool'] == 'uv']['duration']
            
            ax4.hist([pip_durations, uv_durations], label=['pip', 'uv'], 
                    bins=15, alpha=0.7, color=['#FF6B6B', '#4ECDC4'])
            ax4.set_xlabel('Duration (seconds)')
            ax4.set_ylabel('Frequency')
            ax4.set_title('Duration Distribution')
            ax4.legend()
        
        plt.tight_layout()
        
        # Save figure
        plot_path = self.output_dir / f"benchmark_plots_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š Visualizations saved: {plot_path}")
    
    def export_results(self, format: str = "json"):
        """Export results in various formats"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if format == "json":
            output_file = self.output_dir / f"benchmark_results_{timestamp}.json"
            with open(output_file, 'w') as f:
                json.dump(
                    [asdict(r) for r in self.results],
                    f,
                    indent=2,
                    default=str
                )
        
        elif format == "csv":
            output_file = self.output_dir / f"benchmark_results_{timestamp}.csv"
            df = pd.DataFrame([asdict(r) for r in self.results])
            df.to_csv(output_file, index=False)
        
        print(f"ğŸ’¾ Results exported: {output_file}")


async def main():
    """Run benchmark suite"""
    import argparse
    
    parser = argparse.ArgumentParser(description='uv vs pip benchmark suite')
    parser.add_argument('--iterations', type=int, default=3, 
                       help='Number of iterations per scenario')
    parser.add_argument('--scenarios', nargs='+', 
                       help='Specific scenarios to run')
    parser.add_argument('--output-dir', default='./benchmark_results',
                       help='Output directory for results')
    parser.add_argument('--quick', action='store_true',
                       help='Run quick benchmark with fewer scenarios')
    
    args = parser.parse_args()
    
    # Create benchmark suite
    suite = BenchmarkSuite(output_dir=args.output_dir)
    
    # Filter scenarios if specified
    if args.scenarios:
        suite.scenarios = [s for s in suite.scenarios if s.name in args.scenarios]
    elif args.quick:
        # Quick mode - only run small and medium scenarios
        suite.scenarios = [s for s in suite.scenarios if s.name in ['small_project', 'medium_project']]
    
    # Run benchmarks
    await suite.run_all_benchmarks(iterations=args.iterations)
    
    # Generate report and visualizations
    suite.generate_report()
    
    # Export results
    suite.export_results("json")
    suite.export_results("csv")
    
    print("\nâœ… Benchmark suite completed!")


if __name__ == '__main__':
    asyncio.run(main())
```

#### SubTask 4.2.2: ìµœì í™” ê°€ì´ë“œë¼ì¸
**ë‹´ë‹¹ì**: ì„±ëŠ¥ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```markdown
# uv Performance Optimization Guide

## ğŸ“Š Performance Optimization Best Practices

### 1. Cache Optimization

#### Enable Persistent Cache
```bash
# Set cache directory (default: ~/.cache/uv)
export UV_CACHE_DIR=/path/to/fast/ssd/cache

# Increase cache size limit
export UV_CACHE_SIZE=10GB
```

#### Pre-warm Cache
```python
#!/usr/bin/env python3
# scripts/prewarm_cache.py

import subprocess
import asyncio
from pathlib import Path

async def prewarm_cache(requirements_file: str):
    """Pre-download packages to warm uv cache"""
    
    # Read requirements
    requirements = Path(requirements_file).read_text().splitlines()
    
    # Download without installing
    for req in requirements:
        if req and not req.startswith('#'):
            print(f"Pre-warming cache for: {req}")
            process = await asyncio.create_subprocess_exec(
                'uv', 'pip', 'download', '--no-deps', req,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await process.communicate()

# Usage
asyncio.run(prewarm_cache('requirements.txt'))
```

### 2. Network Optimization

#### Use Local Package Index
```bash
# Configure local PyPI mirror
export UV_INDEX_URL=https://pypi.company.internal/simple/

# Use multiple indexes
export UV_EXTRA_INDEX_URL=https://pypi.org/simple/
```

#### Parallel Downloads
```toml
# pyproject.toml
[tool.uv]
parallel-downloads = 10  # Increase for faster networks
connection-timeout = 30  # Adjust based on network latency
```

### 3. Dependency Resolution

#### Lock Dependencies
```bash
# Generate locked requirements
uv pip compile requirements.in -o requirements.lock

# Install from locked file (faster)
uv pip sync requirements.lock
```

#### Minimize Dependency Tree
```python
# analyze_dependencies.py
import subprocess
import json

def analyze_dependency_tree():
    """Analyze and optimize dependency tree"""
    
    # Get dependency tree
    result = subprocess.run(
        ['uv', 'pip', 'tree', '--json'],
        capture_output=True,
        text=True
    )
    
    tree = json.loads(result.stdout)
    
    # Find duplicate dependencies
    packages = {}
    for pkg in tree['packages']:
        name = pkg['name']
        if name in packages:
            packages[name].append(pkg['version'])
        else:
            packages[name] = [pkg['version']]
    
    # Report duplicates
    duplicates = {k: v for k, v in packages.items() if len(set(v)) > 1}
    
    if duplicates:
        print("âš ï¸  Duplicate packages with different versions:")
        for pkg, versions in duplicates.items():
            print(f"  {pkg}: {', '.join(set(versions))}")
```

### 4. CI/CD Optimization

#### GitHub Actions
```yaml
name: Optimized CI with uv

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
    
    - name: Cache uv
      uses: actions/cache@v3
      with:
        path: |
          ~/.cache/uv
          .venv
        key: ${{ runner.os }}-uv-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-uv-
    
    - name: Install dependencies
      run: |
        uv venv
        source .venv/bin/activate
        uv pip sync requirements.lock  # Use locked deps
```

#### Docker Optimization
```dockerfile
# Multi-stage build with cache mount
FROM python:3.11-slim as builder

RUN pip install uv

# Cache mount for uv
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=requirements.txt,target=requirements.txt \
    uv pip install --prefix=/install -r requirements.txt

FROM python:3.11-slim
COPY --from=builder /install /usr/local
```

### 5. Development Workflow

#### Fast Reinstalls
```bash
# Use --reinstall-package for specific packages
uv pip install --reinstall-package mypackage

# Use --force-reinstall sparingly
uv pip install --force-reinstall -r requirements.txt
```

#### Editable Installs
```bash
# Fast editable installs for development
uv pip install -e ./my-package
uv pip install -e git+https://github.com/user/repo.git#egg=package
```

### 6. Monitoring and Profiling

#### Profile Installation
```python
# profile_install.py
import cProfile
import pstats
import subprocess

def profile_installation():
    """Profile uv installation performance"""
    
    profiler = cProfile.Profile()
    profiler.enable()
    
    subprocess.run(['uv', 'pip', 'install', '-r', 'requirements.txt'])
    
    profiler.disable()
    
    # Analyze results
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(20)  # Top 20 functions
```

#### Monitor Cache Performance
```bash
# Check cache statistics
uv cache stats

# Clean cache selectively
uv cache clean --package requests
uv cache clean --all  # Full clean
```

### 7. Platform-Specific Optimizations

#### Linux
```bash
# Use faster filesystem for cache
mount -t tmpfs -o size=5G tmpfs /tmp/uv-cache
export UV_CACHE_DIR=/tmp/uv-cache

# Increase file descriptors
ulimit -n 4096
```

#### macOS
```bash
# Disable Spotlight indexing for cache
sudo mdutil -i off ~/.cache/uv

# Use APFS features
# Cache is automatically optimized on APFS
```

#### Windows
```powershell
# Use faster drive for cache
$env:UV_CACHE_DIR = "D:\uv-cache"

# Disable Windows Defender for cache folder
Add-MpPreference -ExclusionPath "$env:UV_CACHE_DIR"
```

### 8. Common Performance Issues

#### Issue: Slow Resolution
```bash
# Solution: Use stricter version constraints
# Bad:  package>=1.0
# Good: package>=1.0,<2.0

# Use == for known good versions
package==1.2.3
```

#### Issue: Large Docker Images
```dockerfile
# Solution: Multi-stage builds
FROM python:3.11-slim as builder
# ... build stage ...

FROM python:3.11-slim
# Copy only necessary files
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
```

#### Issue: Redundant Installations
```bash
# Solution: Use workspace installations
uv pip install -e .[dev,test,docs]  # Install all extras at once
```

### 9. Benchmarking Your Setup

```python
#!/usr/bin/env python3
# benchmark_my_setup.py

import time
import subprocess
import statistics

def benchmark_command(cmd, runs=5):
    """Benchmark a command multiple times"""
    times = []
    
    for i in range(runs):
        start = time.time()
        subprocess.run(cmd, shell=True, capture_output=True)
        duration = time.time() - start
        times.append(duration)
        print(f"Run {i+1}: {duration:.2f}s")
    
    print(f"\nAverage: {statistics.mean(times):.2f}s")
    print(f"Median: {statistics.median(times):.2f}s")
    print(f"Std Dev: {statistics.stdev(times):.2f}s")

# Benchmark your requirements
benchmark_command("uv pip install -r requirements.txt --dry-run")
```

### 10. Optimization Checklist

- [ ] **Cache Configuration**
  - [ ] Cache on SSD
  - [ ] Adequate cache size
  - [ ] Persistent cache location

- [ ] **Network Setup**
  - [ ] Local PyPI mirror configured
  - [ ] Appropriate timeout settings
  - [ ] Parallel downloads enabled

- [ ] **Dependencies**
  - [ ] Requirements are locked
  - [ ] No duplicate packages
  - [ ] Minimal dependency tree

- [ ] **CI/CD Pipeline**
  - [ ] Cache properly configured
  - [ ] Using locked dependencies
  - [ ] Multi-stage Docker builds

- [ ] **Development Environment**
  - [ ] Fast filesystem for cache
  - [ ] Antivirus exclusions set
  - [ ] File descriptor limits increased

## ğŸ“ˆ Expected Performance Gains

| Optimization | Typical Improvement | Impact |
|--------------|-------------------|---------|
| Cache on SSD | 2-3x faster | High |
| Locked dependencies | 1.5-2x faster | High |
| Local PyPI mirror | 3-5x faster | High |
| Parallel downloads | 1.5-2x faster | Medium |
| Pre-warmed cache | 10-20x faster | High |
| Docker cache mounts | 2-4x faster | High |

## ğŸš€ Advanced Optimizations

### Custom Index Server
```python
# Run a local caching proxy
from flask import Flask, request, redirect
import requests

app = Flask(__name__)

@app.route('/<path:package>')
def proxy_package(package):
    """Simple caching proxy for PyPI"""
    # Implementation details...
    pass
```

### Distributed Cache
```yaml
# Share cache across team using Redis
version: '3'
services:
  uv-cache:
    image: redis:alpine
    volumes:
      - uv-cache-data:/data
    ports:
      - "6379:6379"
```

Remember: Always measure before and after optimizations to ensure they provide real benefits in your specific environment.
```

#### SubTask 4.2.3: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìë™í™”
**ë‹´ë‹¹ì**: ìë™í™” ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```python
#!/usr/bin/env python3
# scripts/performance_monitor.py

import asyncio
import time
import json
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import aiohttp
import schedule
from dataclasses import dataclass, asdict
import matplotlib.pyplot as plt
from matplotlib.dates import DateFormatter
import pandas as pd

@dataclass
class PerformanceMetric:
    timestamp: datetime
    operation: str
    tool: str
    duration: float
    packages_count: int
    cache_hit_rate: float
    error_rate: float
    
class PerformanceMonitor:
    """Automated performance monitoring for uv"""
    
    def __init__(self, db_path: str = "performance_metrics.db"):
        self.db_path = db_path
        self._init_database()
        self.alert_thresholds = {
            'duration': 10.0,  # seconds
            'error_rate': 0.05,  # 5%
            'cache_hit_rate': 0.7  # 70%
        }
        
    def _init_database(self):
        """Initialize SQLite database for metrics storage"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS performance_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                operation TEXT,
                tool TEXT,
                duration REAL,
                packages_count INTEGER,
                cache_hit_rate REAL,
                error_rate REAL
            )
        ''')
        
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_timestamp 
            ON performance_metrics(timestamp)
        ''')
        
        conn.commit()
        conn.close()
    
    async def monitor_installation(self, requirements_file: str) -> PerformanceMetric:
        """Monitor a single installation operation"""
        start_time = time.time()
        cache_hits = 0
        cache_total = 0
        errors = 0
        
        # Create temporary venv
        import tempfile
        with tempfile.TemporaryDirectory() as tmpdir:
            venv_path = Path(tmpdir) / "venv"
            
            # Run installation
            process = await asyncio.create_subprocess_exec(
                'uv', 'venv', str(venv_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            await process.communicate()
            
            # Install packages
            process = await asyncio.create_subprocess_exec(
                'uv', 'pip', 'install', '-r', requirements_file,
                '--python', str(venv_path / 'bin' / 'python'),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            duration = time.time() - start_time
            
            # Parse output for metrics
            output = stdout.decode() + stderr.decode()
            
            # Count cache hits (this is simplified - real implementation would parse actual output)
            cache_hits = output.count('Using cached')
            cache_total = output.count('Downloading') + cache_hits
            cache_hit_rate = cache_hits / cache_total if cache_total > 0 else 0
            
            # Check for errors
            error_rate = 1.0 if process.returncode != 0 else 0.0
            
            # Count installed packages
            list_process = await asyncio.create_subprocess_exec(
                str(venv_path / 'bin' / 'pip'), 'list', '--format=json',
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            list_stdout, _ = await list_process.communicate()
            
            try:
                packages = json.loads(list_stdout.decode())
                packages_count = len(packages)
            except:
                packages_count = 0
        
        metric = PerformanceMetric(
            timestamp=datetime.now(),
            operation='install',
            tool='uv',
            duration=duration,
            packages_count=packages_count,
            cache_hit_rate=cache_hit_rate,
            error_rate=error_rate
        )
        
        # Store in database
        self._store_metric(metric)
        
        # Check for alerts
        self._check_alerts(metric)
        
        return metric
    
    def _store_metric(self, metric: PerformanceMetric):
        """Store metric in database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO performance_metrics 
            (timestamp, operation, tool, duration, packages_count, cache_hit_rate, error_rate)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            metric.timestamp,
            metric.operation,
            metric.tool,
            metric.duration,
            metric.packages_count,
            metric.cache_hit_rate,
            metric.error_rate
        ))
        
        conn.commit()
        conn.close()
    
    def _check_alerts(self, metric: PerformanceMetric):
        """Check if metric triggers any alerts"""
        alerts = []
        
        if metric.duration > self.alert_thresholds['duration']:
            alerts.append(f"High duration: {metric.duration:.1f}s (threshold: {self.alert_thresholds['duration']}s)")
        
        if metric.error_rate > self.alert_thresholds['error_rate']:
            alerts.append(f"High error rate: {metric.error_rate*100:.1f}% (threshold: {self.alert_thresholds['error_rate']*100}%)")
        
        if metric.cache_hit_rate < self.alert_thresholds['cache_hit_rate']:
            alerts.append(f"Low cache hit rate: {metric.cache_hit_rate*100:.1f}% (threshold: {self.alert_thresholds['cache_hit_rate']*100}%)")
        
        if alerts:
            self._send_alerts(alerts, metric)
    
    def _send_alerts(self, alerts: List[str], metric: PerformanceMetric):
        """Send alerts (implement your notification logic here)"""
        print(f"\nâš ï¸  PERFORMANCE ALERTS at {metric.timestamp}:")
        for alert in alerts:
            print(f"  - {alert}")
    
    def get_metrics(self, 
                   start_time: Optional[datetime] = None,
                   end_time: Optional[datetime] = None,
                   operation: Optional[str] = None) -> pd.DataFrame:
        """Retrieve metrics from database"""
        conn = sqlite3.connect(self.db_path)
        
        query = "SELECT * FROM performance_metrics WHERE 1=1"
        params = []
        
        if start_time:
            query += " AND timestamp >= ?"
            params.append(start_time)
        
        if end_time:
            query += " AND timestamp <= ?"
            params.append(end_time)
        
        if operation:
            query += " AND operation = ?"
            params.append(operation)
        
        query += " ORDER BY timestamp"
        
        df = pd.read_sql_query(query, conn, params=params)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        conn.close()
        return df
    
    def generate_report(self, period_days: int = 7):
        """Generate performance report for the specified period"""
        end_time = datetime.now()
        start_time = end_time - timedelta(days=period_days)
        
        df = self.get_metrics(start_time=start_time, end_time=end_time)
        
        if df.empty:
            print("No metrics found for the specified period")
            return
        
        # Calculate statistics
        stats = {
            'total_operations': len(df),
            'avg_duration': df['duration'].mean(),
            'median_duration': df['duration'].median(),
            'max_duration': df['duration'].max(),
            'min_duration': df['duration'].min(),
            'avg_cache_hit_rate': df['cache_hit_rate'].mean(),
            'error_rate': df['error_rate'].mean(),
            'total_packages_installed': df['packages_count'].sum()
        }
        
        # Generate plots
        self._generate_plots(df, period_days)
        
        # Print report
        print(f"\nğŸ“Š Performance Report ({period_days} days)")
        print("=" * 50)
        print(f"Total operations: {stats['total_operations']}")
        print(f"Average duration: {stats['avg_duration']:.2f}s")
        print(f"Median duration: {stats['median_duration']:.2f}s")
        print(f"Duration range: {stats['min_duration']:.2f}s - {stats['max_duration']:.2f}s")
        print(f"Average cache hit rate: {stats['avg_cache_hit_rate']*100:.1f}%")
        print(f"Error rate: {stats['error_rate']*100:.2f}%")
        print(f"Total packages installed: {stats['total_packages_installed']}")
        
        # Identify trends
        self._analyze_trends(df)
        
        return stats
    
    def _generate_plots(self, df: pd.DataFrame, period_days: int):
        """Generate performance plots"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'uv Performance Metrics ({period_days} days)', fontsize=16)
        
        # 1. Duration over time
        ax1 = axes[0, 0]
        df.plot(x='timestamp', y='duration', ax=ax1, marker='o', markersize=4)
        ax1.set_ylabel('Duration (seconds)')
        ax1.set_title('Installation Duration Over Time')
        ax1.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))
        
        # Add rolling average
        window = min(10, len(df) // 3)
        if window > 1:
            df['duration_ma'] = df['duration'].rolling(window=window).mean()
            df.plot(x='timestamp', y='duration_ma', ax=ax1, color='red', 
                   label=f'{window}-point Moving Average')
        
        # 2. Cache hit rate
        ax2 = axes[0, 1]
        df.plot(x='timestamp', y='cache_hit_rate', ax=ax2, marker='s', markersize=4)
        ax2.set_ylabel('Cache Hit Rate')
        ax2.set_ylim(0, 1.1)
        ax2.set_title('Cache Hit Rate Over Time')
        ax2.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))
        
        # Add threshold line
        ax2.axhline(y=self.alert_thresholds['cache_hit_rate'], 
                   color='red', linestyle='--', alpha=0.5, 
                   label=f"Threshold ({self.alert_thresholds['cache_hit_rate']*100}%)")
        
        # 3. Duration distribution
        ax3 = axes[1, 0]
        df['duration'].hist(ax=ax3, bins=20, edgecolor='black')
        ax3.set_xlabel('Duration (seconds)')
        ax3.set_ylabel('Frequency')
        ax3.set_title('Duration Distribution')
        ax3.axvline(x=df['duration'].mean(), color='red', linestyle='--', 
                   label=f'Mean: {df["duration"].mean():.1f}s')
        ax3.legend()
        
        # 4. Daily aggregates
        ax4 = axes[1, 1]
        daily = df.resample('D', on='timestamp').agg({
            'duration': 'mean',
            'cache_hit_rate': 'mean',
            'error_rate': 'mean'
        })
        
        if not daily.empty:
            daily['duration'].plot(ax=ax4, kind='bar')
            ax4.set_ylabel('Average Duration (seconds)')
            ax4.set_title('Daily Average Duration')
            ax4.set_xticklabels([d.strftime('%Y-%m-%d') for d in daily.index], rotation=45)
        
        plt.tight_layout()
        
        # Save plot
        plot_path = f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"\nğŸ“ˆ Plots saved to: {plot_path}")
    
    def _analyze_trends(self, df: pd.DataFrame):
        """Analyze performance trends"""
        print("\nğŸ” Trend Analysis:")
        
        # Check for performance degradation
        if len(df) > 10:
            first_half = df.iloc[:len(df)//2]['duration'].mean()
            second_half = df.iloc[len(df)//2:]['duration'].mean()
            
            change = (second_half - first_half) / first_half * 100
            
            if abs(change) > 10:
                trend = "degrading" if change > 0 else "improving"
                print(f"  - Performance is {trend}: {abs(change):.1f}% change")
        
        # Check cache effectiveness
        recent_cache_rate = df.tail(10)['cache_hit_rate'].mean()
        if recent_cache_rate < 0.8:
            print(f"  - Cache hit rate could be improved: {recent_cache_rate*100:.1f}%")
        
        # Check for patterns
        hourly = df.set_index('timestamp').resample('H')['duration'].mean()
        if len(hourly) > 24:
            peak_hour = hourly.idxmax()
            print(f"  - Peak usage hour: {peak_hour.hour}:00")


class ContinuousMonitor:
    """Continuous monitoring service"""
    
    def __init__(self, monitor: PerformanceMonitor, requirements_file: str):
        self.monitor = monitor
        self.requirements_file = requirements_file
        self.is_running = False
        
    async def start(self, interval_minutes: int = 30):
        """Start continuous monitoring"""
        self.is_running = True
        print(f"ğŸš€ Starting continuous monitoring (every {interval_minutes} minutes)")
        
        while self.is_running:
            try:
                # Run monitoring
                metric = await self.monitor.monitor_installation(self.requirements_file)
                
                print(f"\nâœ… Monitoring completed at {metric.timestamp}")
                print(f"   Duration: {metric.duration:.2f}s")
                print(f"   Cache hit rate: {metric.cache_hit_rate*100:.1f}%")
                print(f"   Packages: {metric.packages_count}")
                
            except Exception as e:
                print(f"\nâŒ Monitoring failed: {e}")
            
            # Wait for next iteration
            await asyncio.sleep(interval_minutes * 60)
    
    def stop(self):
        """Stop monitoring"""
        self.is_running = False
        print("\nğŸ›‘ Monitoring stopped")


async def main():
    """Main function for performance monitoring"""
    import argparse
    
    parser = argparse.ArgumentParser(description='uv performance monitor')
    parser.add_argument('requirements', help='Requirements file to monitor')
    parser.add_argument('--continuous', action='store_true', 
                       help='Run continuous monitoring')
    parser.add_argument('--interval', type=int, default=30,
                       help='Monitoring interval in minutes')
    parser.add_argument('--report', action='store_true',
                       help='Generate performance report')
    parser.add_argument('--days', type=int, default=7,
                       help='Number of days for report')
    
    args = parser.parse_args()
    
    # Initialize monitor
    monitor = PerformanceMonitor()
    
    if args.report:
        # Generate report
        monitor.generate_report(period_days=args.days)
        
    elif args.continuous:
        # Start continuous monitoring
        continuous = ContinuousMonitor(monitor, args.requirements)
        
        try:
            await continuous.start(interval_minutes=args.interval)
        except KeyboardInterrupt:
            continuous.stop()
    
    else:
        # Single monitoring run
        metric = await monitor.monitor_installation(args.requirements)
        
        print(f"\nMonitoring Results:")
        print(f"  Timestamp: {metric.timestamp}")
        print(f"  Duration: {metric.duration:.2f}s")
        print(f"  Packages: {metric.packages_count}")
        print(f"  Cache hit rate: {metric.cache_hit_rate*100:.1f}%")
        print(f"  Error rate: {metric.error_rate*100:.1f}%")


if __name__ == '__main__':
    asyncio.run(main())
```

### Task 4.3: ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ ë° ë¬¸ì„œí™”

#### SubTask 4.3.1: ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸
**ë‹´ë‹¹ì**: í”„ë¡œì íŠ¸ ë¦¬ë”  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```markdown
# uv Migration Final Checklist

## ğŸ“‹ Pre-Production Checklist

### 1. Infrastructure Readiness
- [ ] **uv Installation**
  - [ ] All development machines have uv installed
  - [ ] CI/CD runners have uv installed
  - [ ] Production Docker images include uv
  - [ ] Version consistency across environments

- [ ] **Cache Configuration**
  - [ ] Cache directories configured on fast storage (SSD)
  - [ ] Cache size limits set appropriately
  - [ ] Cache persistence configured for CI/CD
  - [ ] Backup strategy for cache directories

- [ ] **Network Setup**
  - [ ] Proxy configurations tested (if applicable)
  - [ ] Private PyPI repository accessible
  - [ ] SSL certificates configured
  - [ ] Firewall rules updated

### 2. Code and Configuration
- [ ] **Dependency Files**
  - [ ] All requirements.txt files migrated
  - [ ] pyproject.toml created and validated
  - [ ] Lock files generated (requirements.lock)
  - [ ] Development dependencies separated

- [ ] **Scripts and Automation**
  - [ ] Build scripts updated to use uv
  - [ ] Deployment scripts tested
  - [ ] Makefile targets working
  - [ ] Pre-commit hooks updated

- [ ] **Documentation**
  - [ ] README.md updated with uv instructions
  - [ ] Developer onboarding guide updated
  - [ ] Troubleshooting guide created
  - [ ] API documentation reflects changes

### 3. CI/CD Pipeline
- [ ] **GitHub Actions / GitLab CI**
  - [ ] All workflows updated to use uv
  - [ ] Cache strategy implemented
  - [ ] Build times improved (measure and document)
  - [ ] All tests passing

- [ ] **Docker**
  - [ ] Dockerfiles optimized with uv
  - [ ] Multi-stage builds implemented
  - [ ] Image sizes reduced (measure and document)
  - [ ] Container startup times improved

- [ ] **Deployment**
  - [ ] Staging deployments successful
  - [ ] Rollback procedures tested
  - [ ] Monitoring integrated
  - [ ] Alerts configured

### 4. Testing
- [ ] **Functional Testing**
  - [ ] All unit tests passing
  - [ ] Integration tests passing
  - [ ] End-to-end tests passing
  - [ ] Performance benchmarks completed

- [ ] **Compatibility Testing**
  - [ ] Python version compatibility verified
  - [ ] OS compatibility tested (Linux, macOS, Windows)
  - [ ] Package compatibility confirmed
  - [ ] No regression in functionality

- [ ] **Load Testing**
  - [ ] Installation under load tested
  - [ ] Concurrent builds tested
  - [ ] Cache performance under load verified
  - [ ] Resource usage acceptable

### 5. Team Readiness
- [ ] **Training**
  - [ ] All developers trained on uv
  - [ ] DevOps team familiar with uv operations
  - [ ] Support team briefed on common issues
  - [ ] Documentation accessible to all

- [ ] **Communication**
  - [ ] Migration timeline communicated
  - [ ] Stakeholders informed
  - [ ] Success metrics defined
  - [ ] Feedback channels established

## ğŸš€ Production Migration Steps

### Phase 1: Pilot (Week 1)
```bash
# 1. Select pilot project
PROJECT="microservice-a"

# 2. Create feature branch
git checkout -b migrate-to-uv

# 3. Update dependencies
cd $PROJECT
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt

# 4. Run full test suite
make test

# 5. Deploy to staging
make deploy-staging

# 6. Monitor for 24 hours
```

### Phase 2: Gradual Rollout (Week 2-3)
```yaml
# Progressive rollout plan
rollout_schedule:
  week_2:
    - microservice-b
    - microservice-c
    - internal-tools
  week_3:
    - api-gateway
    - worker-services
    - batch-jobs
```

### Phase 3: Full Migration (Week 4)
```bash
# Final migration checklist
for service in $(cat services.txt); do
  echo "Migrating $service..."
  ./scripts/migrate-service.sh $service
  ./scripts/verify-migration.sh $service
done
```

## ğŸ“Š Success Metrics

### Performance Metrics
| Metric | Target | Actual | Status |
|--------|--------|---------|---------|
| CI/CD build time reduction | >50% | ___ | â¬œ |
| Docker image build time | >60% | ___ | â¬œ |
| Local development setup | >70% | ___ | â¬œ |
| Cache hit rate | >80% | ___ | â¬œ |

### Operational Metrics
| Metric | Target | Actual | Status |
|--------|--------|---------|---------|
| Installation success rate | >99% | ___ | â¬œ |
| Developer satisfaction | >4/5 | ___ | â¬œ |
| Support tickets | <10/week | ___ | â¬œ |
| Rollback incidents | 0 | ___ | â¬œ |

## ğŸ”„ Rollback Plan

### Immediate Rollback (< 5 minutes)
```bash
#!/bin/bash
# rollback-to-pip.sh

# 1. Stop deployments
kubectl scale deployment --all --replicas=0 -n production

# 2. Revert CI/CD configs
git revert --no-edit HEAD
git push origin main

# 3. Rebuild with pip
docker build -f Dockerfile.pip -t app:rollback .
docker push app:rollback

# 4. Restart services
kubectl set image deployment/app app=app:rollback -n production
kubectl scale deployment --all --replicas=3 -n production
```

### Gradual Rollback
1. Identify affected services
2. Revert configuration service by service
3. Monitor each rollback
4. Document issues for resolution

## ğŸ“ Sign-off Requirements

### Technical Sign-off
- [ ] **Development Lead**: All code changes reviewed and approved
- [ ] **DevOps Lead**: Infrastructure and deployment verified
- [ ] **QA Lead**: All tests passing, no regressions
- [ ] **Security Lead**: No new vulnerabilities introduced

### Business Sign-off
- [ ] **Product Manager**: No impact on product functionality
- [ ] **Engineering Manager**: Team trained and ready
- [ ] **CTO/VP Engineering**: Strategic alignment confirmed

## ğŸ¯ Post-Migration Tasks

### Week 1 After Migration
- [ ] Monitor performance metrics daily
- [ ] Collect developer feedback
- [ ] Address any urgent issues
- [ ] Document lessons learned

### Week 2-4 After Migration
- [ ] Optimize cache settings based on usage
- [ ] Fine-tune CI/CD pipelines
- [ ] Update best practices documentation
- [ ] Plan knowledge sharing session

### Month 2 After Migration
- [ ] Full performance analysis
- [ ] ROI calculation
- [ ] Case study preparation
- [ ] Plan for future optimizations

## âš ï¸ Risk Mitigation

### High-Risk Areas
1. **Complex Dependencies**
   - Monitor: TensorFlow, SciPy, NumPy installations
   - Mitigation: Pre-test in isolated environment

2. **Private Packages**
   - Monitor: Internal package resolution
   - Mitigation: Ensure index URLs are correct

3. **Platform-Specific Packages**
   - Monitor: OS-specific wheel installations
   - Mitigation: Test on all target platforms

### Monitoring Alerts
```yaml
alerts:
  - name: high_installation_failure_rate
    condition: failure_rate > 5%
    action: notify_oncall
    
  - name: cache_degradation
    condition: cache_hit_rate < 70%
    action: investigate_cache
    
  - name: performance_regression
    condition: install_time > baseline * 1.5
    action: rollback_consideration
```

## âœ… Final Validation

Run the validation script before marking migration complete:

```bash
#!/bin/bash
# validate-migration.sh

echo "ğŸ” Validating uv migration..."

# Check all services
for service in $(cat services.txt); do
  echo -n "Checking $service... "
  
  # Verify uv is being used
  if grep -q "uv pip install" $service/Dockerfile; then
    echo "âœ“"
  else
    echo "âœ— Still using pip!"
    exit 1
  fi
done

# Verify CI/CD
echo -n "Checking CI/CD pipelines... "
if grep -q "uv pip" .github/workflows/*.yml; then
  echo "âœ“"
else
  echo "âœ— CI/CD not updated!"
  exit 1
fi

# Performance check
echo -n "Checking performance improvements... "
NEW_TIME=$(make benchmark-ci | grep "Total time" | awk '{print $3}')
OLD_TIME=$(git show main:benchmark-results.txt | grep "Total time" | awk '{print $3}')

if (( $(echo "$NEW_TIME < $OLD_TIME * 0.5" | bc -l) )); then
  echo "âœ“ ${NEW_TIME}s (was ${OLD_TIME}s)"
else
  echo "âœ— Insufficient improvement"
  exit 1
fi

echo ""
echo "âœ… All validation checks passed!"
echo "ğŸ‰ Migration to uv is complete!"
```

---

**Migration Coordinator Sign-off**

Name: _______________________  
Date: _______________________  
Signature: _______________________

**Notes**: 
_________________________________________________________________
_________________________________________________________________
_________________________________________________________________
```

#### SubTask 4.3.2: ì‚¬ìš©ì ê°€ì´ë“œ ì‘ì„±
**ë‹´ë‹¹ì**: ê¸°ìˆ  ë¬¸ì„œ ì‘ì„±ì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 3ì‹œê°„

```markdown
# uv ì‚¬ìš©ì ê°€ì´ë“œ

## ëª©ì°¨
1. [ì‹œì‘í•˜ê¸°](#ì‹œì‘í•˜ê¸°)
2. [ê¸°ë³¸ ì‚¬ìš©ë²•](#ê¸°ë³¸-ì‚¬ìš©ë²•)
3. [ê³ ê¸‰ ê¸°ëŠ¥](#ê³ ê¸‰-ê¸°ëŠ¥)
4. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)
5. [íŒê³¼ íŠ¸ë¦­](#íŒê³¼-íŠ¸ë¦­)
6. [FAQ](#faq)

---

## ì‹œì‘í•˜ê¸°

### uvë€?
uvëŠ” Rustë¡œ ì‘ì„±ëœ ì´ˆê³ ì† Python íŒ¨í‚¤ì§€ ë§¤ë‹ˆì €ì…ë‹ˆë‹¤. pipë³´ë‹¤ 10-100ë°° ë¹ ë¥¸ ì„±ëŠ¥ì„ ì œê³µí•˜ë©°, ë” ë‚˜ì€ ì˜ì¡´ì„± í•´ê²°ê³¼ ìºì‹±ì„ ì§€ì›í•©ë‹ˆë‹¤.

### ì„¤ì¹˜

#### macOS/Linux
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

#### Windows
```powershell
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

#### ì„¤ì¹˜ í™•ì¸
```bash
uv --version
# uv 0.1.0
```

### ì²« í”„ë¡œì íŠ¸ ì„¤ì •

```bash
# 1. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir my-project
cd my-project

# 2. ê°€ìƒí™˜ê²½ ìƒì„±
uv venv

# 3. ê°€ìƒí™˜ê²½ í™œì„±í™”
source .venv/bin/activate  # Unix/macOS
# ë˜ëŠ”
.venv\Scripts\activate  # Windows

# 4. íŒ¨í‚¤ì§€ ì„¤ì¹˜
uv pip install requests pandas flask
```

---

## ê¸°ë³¸ ì‚¬ìš©ë²•

### íŒ¨í‚¤ì§€ ì„¤ì¹˜

#### ë‹¨ì¼ íŒ¨í‚¤ì§€
```bash
uv pip install package_name
uv pip install package_name==1.2.3  # íŠ¹ì • ë²„ì „
uv pip install package_name>=1.0,<2.0  # ë²„ì „ ë²”ìœ„
```

#### requirements.txtì—ì„œ ì„¤ì¹˜
```bash
uv pip install -r requirements.txt
```

#### ê°œë°œ ëª¨ë“œ ì„¤ì¹˜
```bash
uv pip install -e .  # í˜„ì¬ ë””ë ‰í† ë¦¬
uv pip install -e /path/to/project  # íŠ¹ì • ê²½ë¡œ
```

### íŒ¨í‚¤ì§€ ì œê±°
```bash
uv pip uninstall package_name
uv pip uninstall -r requirements.txt  # ì—¬ëŸ¬ íŒ¨í‚¤ì§€
```

### íŒ¨í‚¤ì§€ ëª©ë¡ í™•ì¸
```bash
uv pip list  # ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ ëª©ë¡
uv pip show package_name  # íŠ¹ì • íŒ¨í‚¤ì§€ ì •ë³´
uv pip freeze  # requirements.txt í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
```

### íŒ¨í‚¤ì§€ ì—…ê·¸ë ˆì´ë“œ
```bash
uv pip install --upgrade package_name
uv pip install --upgrade -r requirements.txt
```

---

## ê³ ê¸‰ ê¸°ëŠ¥

### ì˜ì¡´ì„± ì»´íŒŒì¼ ë° ë™ê¸°í™”

#### requirements.txt ì»´íŒŒì¼
```bash
# requirements.inì—ì„œ requirements.txt ìƒì„±
uv pip compile requirements.in -o requirements.txt

# í•´ì‹œ í¬í•¨ (ë³´ì•ˆ ê°•í™”)
uv pip compile --generate-hashes requirements.in -o requirements.txt
```

#### ì˜ì¡´ì„± ë™ê¸°í™”
```bash
# ì •í™•íˆ requirements.txtì— ëª…ì‹œëœ íŒ¨í‚¤ì§€ë§Œ ì„¤ì¹˜
uv pip sync requirements.txt
```

### ìºì‹œ ê´€ë¦¬

#### ìºì‹œ ì •ë³´ í™•ì¸
```bash
uv cache dir  # ìºì‹œ ë””ë ‰í† ë¦¬ ìœ„ì¹˜
uv cache info  # ìºì‹œ í†µê³„
```

#### ìºì‹œ ì •ë¦¬
```bash
uv cache clean  # ì „ì²´ ìºì‹œ ì‚­ì œ
uv cache clean package_name  # íŠ¹ì • íŒ¨í‚¤ì§€ ìºì‹œ ì‚­ì œ
```

### í”„ë¡œì íŠ¸ë³„ ì„¤ì •

#### pyproject.toml ì„¤ì •
```toml
[project]
name = "my-project"
version = "0.1.0"
dependencies = [
    "fastapi>=0.104.0",
    "pydantic>=2.0.0",
    "sqlalchemy>=2.0.0"
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "black>=23.0.0",
    "mypy>=1.0.0"
]

[tool.uv]
index-url = "https://pypi.org/simple"
find-links = ["https://download.pytorch.org/whl/torch_stable.html"]
```

### í™˜ê²½ ë³€ìˆ˜

```bash
# ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
export UV_CACHE_DIR=/path/to/cache

# ì¸ë±ìŠ¤ URL ì„¤ì •
export UV_INDEX_URL=https://pypi.company.com/simple

# íƒ€ì„ì•„ì›ƒ ì„¤ì •
export UV_HTTP_TIMEOUT=60

# ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ìˆ˜
export UV_CONCURRENT_DOWNLOADS=10
```

---

## ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

#### 1. "uv: command not found"
```bash
# PATHì— ì¶”ê°€
echo 'export PATH="$HOME/.cargo/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc
```

#### 2. SSL ì¸ì¦ì„œ ì˜¤ë¥˜
```bash
# íšŒì‚¬ í”„ë¡ì‹œ í™˜ê²½
export REQUESTS_CA_BUNDLE=/path/to/corporate/cert.pem
export SSL_CERT_FILE=/path/to/corporate/cert.pem
```

#### 3. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤íŒ¨
```bash
# ìƒì„¸ ë¡œê·¸ í™•ì¸
uv pip install problematic-package -v

# ìºì‹œ ì‚­ì œ í›„ ì¬ì‹œë„
uv cache clean problematic-package
uv pip install problematic-package

# íŠ¹ì • ì¸ë±ìŠ¤ ì‚¬ìš©
uv pip install problematic-package --index-url https://pypi.org/simple
```

#### 4. ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ë™ì‹œ ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¤„ì´ê¸°
export UV_CONCURRENT_DOWNLOADS=1
uv pip install large-package
```

### ë””ë²„ê¹…

#### ìƒì„¸ ë¡œê·¸ ì¶œë ¥
```bash
uv pip install package -vvv  # ë§¤ìš° ìƒì„¸í•œ ë¡œê·¸
```

#### ë“œë¼ì´ëŸ° (ì‹¤ì œ ì„¤ì¹˜í•˜ì§€ ì•ŠìŒ)
```bash
uv pip install --dry-run package
```

#### ì˜ì¡´ì„± íŠ¸ë¦¬ í™•ì¸
```bash
uv pip tree  # ì˜ì¡´ì„± íŠ¸ë¦¬ í‘œì‹œ
uv pip tree --reverse package  # ì—­ ì˜ì¡´ì„± í™•ì¸
```

---

## íŒê³¼ íŠ¸ë¦­

### ì„±ëŠ¥ ìµœì í™”

#### 1. ë¡œì»¬ ìºì‹œ ì„œë²„ ì‚¬ìš©
```bash
# devpi ì„œë²„ ì‹¤í–‰
devpi-server --start --host 0.0.0.0 --port 3141

# uvì—ì„œ ì‚¬ìš©
export UV_INDEX_URL=http://localhost:3141/root/pypi/+simple/
```

#### 2. ì‚¬ì „ ë‹¤ìš´ë¡œë“œ
```bash
# íŒ¨í‚¤ì§€ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ (ìºì‹œì— ì €ì¥)
uv pip download -r requirements.txt

# ë‚˜ì¤‘ì— ì˜¤í”„ë¼ì¸ ì„¤ì¹˜
uv pip install --no-index --find-links ./downloads -r requirements.txt
```

#### 3. CI/CD ìµœì í™”
```yaml
# GitHub Actions ì˜ˆì œ
- name: Cache uv
  uses: actions/cache@v3
  with:
    path: ~/.cache/uv
    key: ${{ runner.os }}-uv-${{ hashFiles('**/requirements.txt') }}
```

### ì›Œí¬í”Œë¡œìš° ê°œì„ 

#### 1. ê°œë°œ í™˜ê²½ ìë™í™”
```bash
# Makefile
.PHONY: dev-setup
dev-setup:
    uv venv
    . .venv/bin/activate && uv pip install -e ".[dev]"
    . .venv/bin/activate && pre-commit install
```

#### 2. ì˜ì¡´ì„± ì—…ë°ì´íŠ¸ ì›Œí¬í”Œë¡œìš°
```bash
# ì˜¤ë˜ëœ íŒ¨í‚¤ì§€ í™•ì¸
uv pip list --outdated

# ì•ˆì „í•œ ì—…ë°ì´íŠ¸
uv pip compile --upgrade-package package_name requirements.in -o requirements.txt
uv pip sync requirements.txt
```

#### 3. ë©€í‹° í™˜ê²½ ê´€ë¦¬
```bash
# Python ë²„ì „ë³„ í™˜ê²½
uv venv --python python3.11 .venv311
uv venv --python python3.12 .venv312

# í™˜ê²½ ì „í™˜
source .venv311/bin/activate  # Python 3.11
source .venv312/bin/activate  # Python 3.12
```

---

## FAQ

### Q: pipì™€ uvë¥¼ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?
A: ë„¤, ê°€ëŠ¥í•©ë‹ˆë‹¤. uvëŠ” pipì™€ í˜¸í™˜ë˜ë¯€ë¡œ ê°™ì€ ê°€ìƒí™˜ê²½ì—ì„œ ë²ˆê°ˆì•„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ê´€ì„±ì„ ìœ„í•´ í•˜ë‚˜ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

### Q: ê¸°ì¡´ requirements.txtë¥¼ ìˆ˜ì •í•´ì•¼ í•˜ë‚˜ìš”?
A: ì•„ë‹ˆìš”, uvëŠ” pipì˜ requirements.txt í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

### Q: uvê°€ ì§€ì›í•˜ì§€ ì•ŠëŠ” pip ê¸°ëŠ¥ì´ ìˆë‚˜ìš”?
A: ëŒ€ë¶€ë¶„ì˜ pip ê¸°ëŠ¥ì„ ì§€ì›í•˜ì§€ë§Œ, ì¼ë¶€ ê³ ê¸‰ ê¸°ëŠ¥(ì˜ˆ: --user ì„¤ì¹˜)ì€ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì „ì²´ ëª©ë¡ì€ ê³µì‹ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### Q: íšŒì‚¬ í”„ë¡ì‹œ ë’¤ì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´?
A: í‘œì¤€ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”:
```bash
export HTTP_PROXY=http://proxy.company.com:8080
export HTTPS_PROXY=http://proxy.company.com:8080
export NO_PROXY=localhost,127.0.0.1,.company.com
```

### Q: Dockerì—ì„œ uvë¥¼ ì‚¬ìš©í•˜ëŠ” ìµœì„ ì˜ ë°©ë²•ì€?
A: Multi-stage ë¹Œë“œì™€ ìºì‹œ ë§ˆìš´íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:
```dockerfile
# syntax=docker/dockerfile:1.4
FROM python:3.11-slim AS builder
RUN pip install uv
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=requirements.txt,target=requirements.txt \
    uv pip install --prefix=/install -r requirements.txt

FROM python:3.11-slim
COPY --from=builder /install /usr/local
```

### Q: uv ì—…ë°ì´íŠ¸ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
A: ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

---

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [ê³µì‹ ë¬¸ì„œ](https://github.com/astral-sh/uv)
- [ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬](./benchmarks.md)
- [ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ](./migration-guide.md)
- [íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ](./troubleshooting.md)

## ì§€ì›

ë¬¸ì œê°€ ìˆìœ¼ì‹ ê°€ìš”?

1. ë‚´ë¶€ Slack ì±„ë„: #uv-support
2. ì´ë©”ì¼: dev-support@company.com
3. ìœ„í‚¤: https://wiki.company.com/uv

---

*ìµœì¢… ì—…ë°ì´íŠ¸: 2024ë…„ 12ì›”*
```

#### SubTask 4.3.3: í”„ë¡œì íŠ¸ íšŒê³  ë° ê°œì„ ì‚¬í•­
**ë‹´ë‹¹ì**: í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 2ì‹œê°„

```markdown
# uv ë§ˆì´ê·¸ë ˆì´ì…˜ í”„ë¡œì íŠ¸ íšŒê³ 

## ğŸ“… í”„ë¡œì íŠ¸ ê°œìš”

- **ê¸°ê°„**: 2024ë…„ 11ì›” 1ì¼ - 2024ë…„ 11ì›” 21ì¼ (3ì£¼)
- **íŒ€ ê·œëª¨**: 15ëª… (ê°œë°œ 10ëª…, DevOps 3ëª…, QA 2ëª…)
- **ì˜í–¥ ë²”ìœ„**: 25ê°œ ì„œë¹„ìŠ¤, 100+ ê°œë°œì
- **ì˜ˆì‚°**: $50,000

## ğŸ¯ ëª©í‘œ ë‹¬ì„±ë„

### ì •ëŸ‰ì  ëª©í‘œ

| ëª©í‘œ | ëª©í‘œì¹˜ | ì‹¤ì œ ë‹¬ì„± | ë‹¬ì„±ë¥  |
|------|--------|----------|---------|
| CI/CD ë¹Œë“œ ì‹œê°„ ë‹¨ì¶• | 50% | 67% | 134% âœ… |
| Docker ì´ë¯¸ì§€ ë¹Œë“œ ì‹œê°„ | 60% | 72% | 120% âœ… |
| ë¡œì»¬ ê°œë°œ í™˜ê²½ ì„¤ì • ì‹œê°„ | 70% | 85% | 121% âœ… |
| ìºì‹œ ì ì¤‘ë¥  | 80% | 92% | 115% âœ… |
| ì„¤ì¹˜ ì„±ê³µë¥  | 99% | 99.7% | 100.7% âœ… |

### ì •ì„±ì  ëª©í‘œ

- âœ… **ê°œë°œì ê²½í—˜ ê°œì„ **: ì„¤ë¬¸ì¡°ì‚¬ ê²°ê³¼ 4.6/5.0
- âœ… **ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ**: ì˜ì¡´ì„± ê´€ë¦¬ ì´ìŠˆ 75% ê°ì†Œ
- âœ… **í‘œì¤€í™”**: ëª¨ë“  í”„ë¡œì íŠ¸ê°€ ë™ì¼í•œ ë„êµ¬ ì‚¬ìš©

## ğŸ’¡ ì£¼ìš” ì„±ê³¼

### 1. ì„±ëŠ¥ ê°œì„ 
- **í‰ê·  íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹œê°„**: 180ì´ˆ â†’ 15ì´ˆ (12ë°° ê°œì„ )
- **CI/CD íŒŒì´í”„ë¼ì¸**: í‰ê·  15ë¶„ â†’ 5ë¶„
- **Docker ì´ë¯¸ì§€ ë¹Œë“œ**: í‰ê·  10ë¶„ â†’ 2.5ë¶„

### 2. ë¹„ìš© ì ˆê°
- **CI/CD ë¹„ìš©**: ì›” $3,000 ì ˆê° (ì»´í“¨íŒ… ì‹œê°„ ê°ì†Œ)
- **ê°œë°œì ìƒì‚°ì„±**: ì£¼ë‹¹ 2ì‹œê°„ ì ˆì•½ Ã— 100ëª… = 200ì‹œê°„/ì£¼
- **ì—°ê°„ ì˜ˆìƒ ì ˆê°ì•¡**: $250,000

### 3. ê¸°ìˆ ì  ê°œì„ 
- ì˜ì¡´ì„± ì¶©ëŒ í•´ê²° ì‹œê°„ 90% ë‹¨ì¶•
- ì¬í˜„ ê°€ëŠ¥í•œ ë¹Œë“œ 100% ë‹¬ì„±
- ë³´ì•ˆ ì·¨ì•½ì  ìŠ¤ìº” ì‹œê°„ 80% ë‹¨ì¶•

## ğŸ“ êµí›ˆ (Lessons Learned)

### ì˜ëœ ì  (What Went Well)

1. **ë‹¨ê³„ì  ë¡¤ì•„ì›ƒ ì „ëµ**
   - íŒŒì¼ëŸ¿ í”„ë¡œì íŠ¸ë¡œ ì‹œì‘í•˜ì—¬ ë¦¬ìŠ¤í¬ ìµœì†Œí™”
   - ê° ë‹¨ê³„ì—ì„œ í”¼ë“œë°± ìˆ˜ì§‘ ë° ë°˜ì˜
   - ë¡¤ë°± ê³„íšì´ ì˜ ì‘ë™í•¨

2. **ì² ì €í•œ ì‚¬ì „ ì¤€ë¹„**
   - í¬ê´„ì ì¸ ë²¤ì¹˜ë§ˆí¬ë¡œ ROI ì…ì¦
   - ìƒì„¸í•œ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ ì œì‘
   - ìë™í™” ìŠ¤í¬ë¦½íŠ¸ë¡œ ìˆ˜ì‘ì—… ìµœì†Œí™”

3. **íŒ€ í˜‘ì—…**
   - ê°œë°œ, DevOps, QA íŒ€ ê°„ ì›í™œí•œ ì†Œí†µ
   - ì¼ì¼ ìŠ¤íƒ ë“œì—…ìœ¼ë¡œ ì´ìŠˆ ë¹ ë¥¸ í•´ê²°
   - ì§€ì‹ ê³µìœ  ì„¸ì…˜ íš¨ê³¼ì 

### ê°œì„ ì´ í•„ìš”í–ˆë˜ ì  (What Could Be Improved)

1. **ì´ˆê¸° êµìœ¡ ë¶€ì¡±**
   - ë¬¸ì œ: ì¼ë¶€ ê°œë°œìê°€ uv ê°œë… ì´í•´ì— ì–´ë ¤ì›€
   - í•´ê²°: ì¶”ê°€ êµìœ¡ ì„¸ì…˜ ë° 1:1 ë©˜í† ë§ ì œê³µ
   - ê°œì„ ì•ˆ: ì‚¬ì „ êµìœ¡ ìë£Œ ë” ì¶©ì‹¤íˆ ì¤€ë¹„

2. **Windows í™˜ê²½ ì´ìŠˆ**
   - ë¬¸ì œ: Windows ê°œë°œìë“¤ì´ ë” ë§ì€ ì´ìŠˆ ê²½í—˜
   - í•´ê²°: Windows ì „ìš© íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ ì‘ì„±
   - ê°œì„ ì•ˆ: ì´ˆê¸°ë¶€í„° ëª¨ë“  OS ë™ë“±í•˜ê²Œ í…ŒìŠ¤íŠ¸

3. **ëª¨ë‹ˆí„°ë§ ì§€ì—°**
   - ë¬¸ì œ: ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•ì´ ë§ˆì´ê·¸ë ˆì´ì…˜ í›„ ì§„í–‰
   - í•´ê²°: ì„ì‹œ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
   - ê°œì„ ì•ˆ: ëª¨ë‹ˆí„°ë§ì„ ë§ˆì´ê·¸ë ˆì´ì…˜ê³¼ ë™ì‹œ ì§„í–‰

## ğŸ”„ ê°œì„  ì‚¬í•­ ë° í–¥í›„ ê³„íš

### ë‹¨ê¸° ê°œì„  ì‚¬í•­ (1ê°œì›” ë‚´)

1. **ë¬¸ì„œí™” ê°•í™”**
   ```markdown
   - [ ] ë¹„ë””ì˜¤ íŠœí† ë¦¬ì–¼ ì œì‘
   - [ ] ì¸í„°ë™í‹°ë¸Œ í•™ìŠµ í”Œë«í¼ êµ¬ì¶•
   - [ ] íŒ€ë³„ ë§ì¶¤í˜• ê°€ì´ë“œ ì‘ì„±
   ```

2. **ë„êµ¬ ê°œì„ **
   ```bash
   # uv í—¬í¼ ìŠ¤í¬ë¦½íŠ¸ ê°œë°œ
   uv-helper init  # í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
   uv-helper update  # ì˜ì¡´ì„± ì—…ë°ì´íŠ¸
   uv-helper audit  # ë³´ì•ˆ ê°ì‚¬
   ```

3. **ëª¨ë‹ˆí„°ë§ í™•ëŒ€**
   - íŒ€ë³„ ëŒ€ì‹œë³´ë“œ ìƒì„±
   - ì´ìƒ íƒì§€ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
   - ì„±ëŠ¥ íšŒê·€ ìë™ ê°ì§€

### ì¤‘ê¸° ê³„íš (3-6ê°œì›”)

1. **ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©**
   - Workspace ê¸°ëŠ¥ ë„ì…
   - ëª¨ë…¸ë ˆí¬ ìµœì í™”
   - ì»¤ìŠ¤í…€ ì¸ë±ìŠ¤ ì„œë²„ êµ¬ì¶•

2. **ìë™í™” í™•ëŒ€**
   - ì˜ì¡´ì„± ì—…ë°ì´íŠ¸ ë´‡
   - ì·¨ì•½ì  ìë™ íŒ¨ì¹˜
   - ì„±ëŠ¥ ìµœì í™” ì œì•ˆ ì‹œìŠ¤í…œ

3. **ì—ì½”ì‹œìŠ¤í…œ ê¸°ì—¬**
   - uv ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬
   - ë‚´ë¶€ í”ŒëŸ¬ê·¸ì¸ ê°œë°œ
   - ì»¤ë®¤ë‹ˆí‹° ì§€ì‹ ê³µìœ 

## ğŸ“Š ROI ë¶„ì„

### íˆ¬ì ëŒ€ë¹„ ìˆ˜ìµ
- **ì´ íˆ¬ì**: $50,000 (ì¸ê±´ë¹„ + ì¸í”„ë¼)
- **ì—°ê°„ ì ˆê°ì•¡**: $250,000
- **íˆ¬ì íšŒìˆ˜ ê¸°ê°„**: 2.4ê°œì›”
- **3ë…„ ROI**: 1,400%

### ì •ì„±ì  ì´ìµ
- ê°œë°œì ë§Œì¡±ë„ ìƒìŠ¹
- ì‹ ê·œ ê°œë°œì ì˜¨ë³´ë”© ì‹œê°„ 50% ë‹¨ì¶•
- í”„ë¡œë•ì…˜ ë°°í¬ ì‹ ë¢°ì„± í–¥ìƒ

## ğŸ™ ê°ì‚¬ ì¸ì‚¬

ì´ í”„ë¡œì íŠ¸ì˜ ì„±ê³µì€ ëª¨ë“  íŒ€ì›ë“¤ì˜ í—Œì‹ ì ì¸ ë…¸ë ¥ ë•ë¶„ì…ë‹ˆë‹¤.

íŠ¹ë³„íˆ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤:
- **íŒŒì¼ëŸ¿ íŒ€**: ì´ˆê¸° ë¦¬ìŠ¤í¬ë¥¼ ê°ìˆ˜í•˜ê³  í”¼ë“œë°± ì œê³µ
- **DevOps íŒ€**: 24/7 ì§€ì› ë° ì‹ ì†í•œ ì´ìŠˆ í•´ê²°
- **ë¬¸ì„œí™” íŒ€**: í¬ê´„ì ì¸ ê°€ì´ë“œ ì‘ì„±
- **ë¦¬ë”ì‹­**: í”„ë¡œì íŠ¸ ì§€ì› ë° ë¦¬ì†ŒìŠ¤ ì œê³µ

## ğŸ‰ ì¶•í•˜ ë° ë‹¤ìŒ ë‹¨ê³„

### ì„±ê³¼ ì¶•í•˜
- íŒ€ íšŒì‹ ì˜ˆì • (11ì›” 25ì¼)
- ìš°ìˆ˜ ê¸°ì—¬ì í‘œì°½
- ì„±ê³¼ ë³´ë„ˆìŠ¤ ì§€ê¸‰

### ì§€ì‹ ê³µìœ 
- ë‚´ë¶€ í…Œí¬í†¡: "uv ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ê³µ ìŠ¤í† ë¦¬" (12ì›” 5ì¼)
- ì™¸ë¶€ ì»¨í¼ëŸ°ìŠ¤ ë°œí‘œ ì œì•ˆì„œ ì œì¶œ
- ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ë¡œ ë„êµ¬ ê³µê°œ

### ë‹¤ìŒ í”„ë¡œì íŠ¸
- Poetry â†’ uv ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€í† 
- Rust ê¸°ë°˜ ë„êµ¬ ì¶”ê°€ ë„ì… ê²€í† 
- íŒ¨í‚¤ì§€ ê´€ë¦¬ ìë™í™” í”Œë«í¼ êµ¬ì¶•

## ğŸ“Œ í•µì‹¬ ë©”ì‹œì§€

> "ì‘ì€ ë³€í™”ê°€ í° ì°¨ì´ë¥¼ ë§Œë“ ë‹¤. uvë¡œì˜ ì „í™˜ì€ ë‹¨ìˆœíˆ ë„êµ¬ë¥¼ ë°”ê¾¼ ê²ƒì´ ì•„ë‹ˆë¼, ìš°ë¦¬ì˜ ê°œë°œ ë¬¸í™”ì™€ íš¨ìœ¨ì„±ì„ í•œ ë‹¨ê³„ ì—…ê·¸ë ˆì´ë“œí•œ ê²ƒì´ë‹¤."

---

**í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €**: Jane Doe  
**ë‚ ì§œ**: 2024ë…„ 11ì›” 21ì¼  
**ìƒíƒœ**: âœ… ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ
```

---

## ğŸ¯ ë§ˆë¬´ë¦¬

ì´ë¡œì¨ pipì—ì„œ uvë¡œì˜ ì™„ì „í•œ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œê°€ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ê°€ì´ë“œëŠ” 21ì¼ ë™ì•ˆì˜ ì²´ê³„ì ì¸ ì ‘ê·¼ì„ í†µí•´ ì•ˆì „í•˜ê³  íš¨ìœ¨ì ì¸ ì „í™˜ì„ ë³´ì¥í•©ë‹ˆë‹¤.

### í•µì‹¬ ì„±ê³µ ìš”ì†Œ:
1. **ë‹¨ê³„ì  ì ‘ê·¼**: ê¸‰ì§„ì  ë³€ê²½ë³´ë‹¤ ì ì§„ì  ì „í™˜
2. **ì² ì €í•œ í…ŒìŠ¤íŠ¸**: ëª¨ë“  ë‹¨ê³„ì—ì„œ ê²€ì¦
3. **íŒ€ êµìœ¡**: ëª¨ë“  êµ¬ì„±ì›ì˜ ì¤€ë¹„ë„ í™•ë³´
4. **ìë™í™”**: ìˆ˜ì‘ì—… ìµœì†Œí™”ë¡œ ì˜¤ë¥˜ ë°©ì§€
5. **ëª¨ë‹ˆí„°ë§**: ì§€ì†ì ì¸ ì„±ëŠ¥ ì¶”ì 

uvë¡œì˜ ì „í™˜ì€ ë‹¨ìˆœí•œ ë„êµ¬ ë³€ê²½ì´ ì•„ë‹Œ, ê°œë°œ ìƒì‚°ì„±ê³¼ íŒ€ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ì „ëµì  íˆ¬ìì…ë‹ˆë‹¤. ğŸš€