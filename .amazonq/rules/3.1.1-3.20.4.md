ë„¤, í”„ë¡œì íŠ¸ ì§€ì‹ì„ í™•ì¸í•˜ì—¬ Phase 3ì˜ subtask ì‘ì—…ì§€ì‹œì„œë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤. ë¨¼ì € ì§€ì •í•˜ì‹  í•µì‹¬ íŒŒì¼ë“¤ì„ ê²€ìƒ‰í•´ë³´ê² ìŠµë‹ˆë‹¤.í”„ë¡œì íŠ¸ ì§€ì‹ì„ í™•ì¸í•œ ê²°ê³¼, Phase 3ì˜ êµ¬ì¡°ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì¬ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤. Phase 3ëŠ” "ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ êµ¬ì¶•"ì— ì§‘ì¤‘í•˜ê³ , 9ê°œì˜ ê°œë³„ ì—ì´ì „íŠ¸ êµ¬í˜„ì€ Phase 4ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.

## Phase 3: ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ êµ¬ì¶• - ì „ì²´ ê³„íš

### ğŸ“‹ Phase 3 ê°œìš”
- **ëª©í‘œ**: T-Developerì˜ 9ê°œ í•µì‹¬ ì—ì´ì „íŠ¸ë¥¼ ìœ„í•œ í¬ê´„ì ì¸ í”„ë ˆì„ì›Œí¬ êµ¬ì¶•
- **ë²”ìœ„**: 20ê°œ Tasks Ã— 4 SubTasks = 80ê°œ ì‘ì—… ë‹¨ìœ„
- **ê¸°ê°„**: ì˜ˆìƒ 10-12ì£¼
- **ì „ì œì¡°ê±´**: Phase 1 ì½”ì–´ ì¸í”„ë¼ ì™„ë£Œ, Phase 2 ë°ì´í„° ë ˆì´ì–´ ì™„ë£Œ

### ğŸ—ï¸ Phase 3 ì˜¬ë°”ë¥¸ Task êµ¬ì¡°

#### ì—ì´ì „íŠ¸ ê¸°ë³¸ í”„ë ˆì„ì›Œí¬ (Tasks 3.1-3.5)
- **Task 3.1**: ì—ì´ì „íŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤ ì„¤ê³„
- **Task 3.2**: ì—ì´ì „íŠ¸ ìƒëª…ì£¼ê¸° ê´€ë¦¬
- **Task 3.3**: ì—ì´ì „íŠ¸ ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ
- **Task 3.4**: ì—ì´ì „íŠ¸ ì„¤ì • ë° ì´ˆê¸°í™” ì‹œìŠ¤í…œ
- **Task 3.5**: ì—ì´ì „íŠ¸ ì—ëŸ¬ ì²˜ë¦¬ í”„ë ˆì„ì›Œí¬

#### ì—ì´ì „íŠ¸ í†µì‹  ë° ë©”ì‹œì§• (Tasks 3.6-3.10)
- **Task 3.6**: ì—ì´ì „íŠ¸ ê°„ í†µì‹  í”„ë¡œí† ì½œ
- **Task 3.7**: ë©”ì‹œì§€ í ì‹œìŠ¤í…œ
- **Task 3.8**: ì´ë²¤íŠ¸ ë²„ìŠ¤ êµ¬í˜„
- **Task 3.9**: ë™ê¸°/ë¹„ë™ê¸° í†µì‹  ë ˆì´ì–´
- **Task 3.10**: ì—ì´ì „íŠ¸ ê°„ ë°ì´í„° ê³µìœ  ì‹œìŠ¤í…œ

#### ì—ì´ì „íŠ¸ í˜‘ì—… ë° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (Tasks 3.11-3.15)
- **Task 3.11**: ì›Œí¬í”Œë¡œìš° ì—”ì§„ êµ¬ì¶•
- **Task 3.12**: ì—ì´ì „íŠ¸ ì²´ì¸ ê´€ë¦¬ ì‹œìŠ¤í…œ
- **Task 3.13**: ë³‘ë ¬ ì²˜ë¦¬ ë° ì¡°ì • ë©”ì»¤ë‹ˆì¦˜
- **Task 3.14**: ì—ì´ì „íŠ¸ ê°„ ì˜ì¡´ì„± ê´€ë¦¬
- **Task 3.15**: í˜‘ì—… íŒ¨í„´ ë¼ì´ë¸ŒëŸ¬ë¦¬

#### ì—ì´ì „íŠ¸ ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§ (Tasks 3.16-3.20)
- **Task 3.16**: ì—ì´ì „íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‹œìŠ¤í…œ
- **Task 3.17**: ì—ì´ì „íŠ¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- **Task 3.18**: ì—ì´ì „íŠ¸ ë¡œê¹… ë° ì¶”ì 
- **Task 3.19**: ì—ì´ì „íŠ¸ ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ
- **Task 3.20**: ì—ì´ì „íŠ¸ ë°°í¬ ë° í™•ì¥ ì‹œìŠ¤í…œ

---

## ğŸ“ Phase 3 ì„¸ë¶€ ì‘ì—…ì§€ì‹œì„œ

### Task 3.1: ì—ì´ì „íŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤ ì„¤ê³„

#### SubTask 3.1.1: ì¶”ìƒ ë² ì´ìŠ¤ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ êµ¬í˜„
**ë‹´ë‹¹ì**: ì‹œë‹ˆì–´ ë°±ì—”ë“œ ì•„í‚¤í…íŠ¸  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/base_agent.py
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List, TypeVar, Generic
import asyncio
from dataclasses import dataclass
from enum import Enum
import uuid
from datetime import datetime

T = TypeVar('T')
R = TypeVar('R')

class AgentStatus(Enum):
    IDLE = "idle"
    INITIALIZING = "initializing"
    READY = "ready"
    PROCESSING = "processing"
    WAITING = "waiting"
    ERROR = "error"
    TERMINATED = "terminated"

@dataclass
class AgentMetadata:
    agent_id: str
    agent_type: str
    version: str
    capabilities: List[str]
    created_at: datetime
    last_active: datetime

@dataclass
class AgentContext:
    project_id: str
    session_id: str
    user_id: str
    parent_agent_id: Optional[str]
    execution_context: Dict[str, Any]
    shared_memory: Dict[str, Any]

class BaseAgent(ABC, Generic[T, R]):
    """Base class for all T-Developer agents"""
    
    def __init__(self, agent_config: Dict[str, Any]):
        self.agent_id = str(uuid.uuid4())
        self.config = agent_config
        self.status = AgentStatus.IDLE
        self.metadata = self._create_metadata()
        self.context: Optional[AgentContext] = None
        self.logger = self._setup_logger()
        
    @abstractmethod
    async def initialize(self) -> None:
        """Initialize agent resources and connections"""
        pass
        
    @abstractmethod
    async def execute(self, input_data: T) -> R:
        """Execute agent's primary function"""
        pass
        
    @abstractmethod
    async def validate_input(self, input_data: T) -> bool:
        """Validate input data before processing"""
        pass
        
    @abstractmethod
    async def cleanup(self) -> None:
        """Clean up agent resources"""
        pass
```

#### SubTask 3.1.2: ì—ì´ì „íŠ¸ ì¸í„°í˜ì´ìŠ¤ ì •ì˜
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/interfaces.ts
export interface IAgent<TInput = any, TOutput = any> {
  // Core properties
  readonly agentId: string;
  readonly agentType: string;
  readonly version: string;
  
  // Status and state
  status: AgentStatus;
  context: AgentContext | null;
  
  // Lifecycle methods
  initialize(): Promise<void>;
  execute(input: TInput): Promise<TOutput>;
  terminate(): Promise<void>;
  
  // Communication methods
  sendMessage(targetAgentId: string, message: AgentMessage): Promise<void>;
  receiveMessage(message: AgentMessage): Promise<void>;
  
  // State management
  saveState(): Promise<void>;
  loadState(): Promise<void>;
  
  // Health and monitoring
  healthCheck(): Promise<HealthCheckResult>;
  getMetrics(): Promise<AgentMetrics>;
}

export interface ICollaborativeAgent extends IAgent {
  // Collaboration methods
  requestCollaboration(agentIds: string[], task: CollaborationTask): Promise<CollaborationResult>;
  joinCollaboration(collaborationId: string): Promise<void>;
  leaveCollaboration(collaborationId: string): Promise<void>;
  
  // Workflow participation
  participateInWorkflow(workflowId: string, role: string): Promise<void>;
  handleWorkflowStep(step: WorkflowStep): Promise<StepResult>;
}
```

#### SubTask 3.1.3: ì—ì´ì „íŠ¸ íŒ©í† ë¦¬ íŒ¨í„´ êµ¬í˜„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 10ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/agent_factory.py
from typing import Type, Dict, Any, Optional
from abc import ABC, abstractmethod
import importlib
import inspect

class AgentFactory:
    """Factory for creating and managing agent instances"""
    
    _registry: Dict[str, Type[BaseAgent]] = {}
    _instances: Dict[str, BaseAgent] = {}
    
    @classmethod
    def register_agent(cls, agent_type: str, agent_class: Type[BaseAgent]) -> None:
        """Register an agent type in the factory"""
        if agent_type in cls._registry:
            raise ValueError(f"Agent type '{agent_type}' is already registered")
        
        if not issubclass(agent_class, BaseAgent):
            raise TypeError(f"{agent_class} must be a subclass of BaseAgent")
            
        cls._registry[agent_type] = agent_class
    
    @classmethod
    def create_agent(
        cls,
        agent_type: str,
        config: Dict[str, Any],
        singleton: bool = False
    ) -> BaseAgent:
        """Create an agent instance"""
        if agent_type not in cls._registry:
            raise ValueError(f"Unknown agent type: {agent_type}")
        
        if singleton and agent_type in cls._instances:
            return cls._instances[agent_type]
        
        agent_class = cls._registry[agent_type]
        agent_instance = agent_class(config)
        
        if singleton:
            cls._instances[agent_type] = agent_instance
            
        return agent_instance
    
    @classmethod
    def discover_agents(cls, module_path: str) -> None:
        """Auto-discover and register agents from a module"""
        module = importlib.import_module(module_path)
        
        for name, obj in inspect.getmembers(module):
            if (inspect.isclass(obj) and 
                issubclass(obj, BaseAgent) and 
                obj is not BaseAgent):
                
                agent_type = getattr(obj, 'AGENT_TYPE', name.lower())
                cls.register_agent(agent_type, obj)
```

#### SubTask 3.1.4: ì—ì´ì „íŠ¸ ëŠ¥ë ¥ ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/capabilities.py
from typing import List, Dict, Any, Callable, Optional
from dataclasses import dataclass
from enum import Enum
import inspect

class CapabilityType(Enum):
    ANALYSIS = "analysis"
    GENERATION = "generation"
    VALIDATION = "validation"
    TRANSFORMATION = "transformation"
    INTEGRATION = "integration"
    MONITORING = "monitoring"

@dataclass
class Capability:
    name: str
    type: CapabilityType
    description: str
    input_schema: Dict[str, Any]
    output_schema: Dict[str, Any]
    required_permissions: List[str]
    estimated_duration: Optional[int]  # milliseconds
    
class CapabilityMixin:
    """Mixin to add capability management to agents"""
    
    def __init__(self):
        self._capabilities: Dict[str, Capability] = {}
        self._capability_handlers: Dict[str, Callable] = {}
        
    def register_capability(
        self,
        capability: Capability,
        handler: Callable
    ) -> None:
        """Register a new capability"""
        self._capabilities[capability.name] = capability
        self._capability_handlers[capability.name] = handler
        
    def has_capability(self, capability_name: str) -> bool:
        """Check if agent has a specific capability"""
        return capability_name in self._capabilities
        
    def get_capabilities(self) -> List[Capability]:
        """Get all agent capabilities"""
        return list(self._capabilities.values())
        
    async def execute_capability(
        self,
        capability_name: str,
        input_data: Dict[str, Any]
    ) -> Any:
        """Execute a specific capability"""
        if not self.has_capability(capability_name):
            raise ValueError(f"Capability '{capability_name}' not found")
            
        capability = self._capabilities[capability_name]
        handler = self._capability_handlers[capability_name]
        
        # Validate input against schema
        self._validate_schema(input_data, capability.input_schema)
        
        # Execute handler
        if inspect.iscoroutinefunction(handler):
            result = await handler(input_data)
        else:
            result = handler(input_data)
            
        # Validate output against schema
        self._validate_schema(result, capability.output_schema)
        
        return result
```

### Task 3.2: ì—ì´ì „íŠ¸ ìƒëª…ì£¼ê¸° ê´€ë¦¬

#### SubTask 3.2.1: ìƒëª…ì£¼ê¸° ìƒíƒœ ë¨¸ì‹  êµ¬í˜„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/lifecycle.py
from typing import Dict, List, Callable, Optional, Set
from enum import Enum
import asyncio
from datetime import datetime

class LifecycleEvent(Enum):
    CREATED = "created"
    INITIALIZING = "initializing"
    INITIALIZED = "initialized"
    STARTING = "starting"
    STARTED = "started"
    EXECUTING = "executing"
    PAUSING = "pausing"
    PAUSED = "paused"
    RESUMING = "resuming"
    STOPPING = "stopping"
    STOPPED = "stopped"
    ERROR = "error"
    TERMINATED = "terminated"

class LifecycleStateMachine:
    """State machine for agent lifecycle management"""
    
    def __init__(self):
        self.current_state = LifecycleEvent.CREATED
        self.state_history: List[tuple[LifecycleEvent, datetime]] = [
            (self.current_state, datetime.utcnow())
        ]
        self.transitions = self._define_transitions()
        self.state_handlers: Dict[LifecycleEvent, List[Callable]] = {}
        
    def _define_transitions(self) -> Dict[LifecycleEvent, Set[LifecycleEvent]]:
        """Define valid state transitions"""
        return {
            LifecycleEvent.CREATED: {LifecycleEvent.INITIALIZING},
            LifecycleEvent.INITIALIZING: {LifecycleEvent.INITIALIZED, LifecycleEvent.ERROR},
            LifecycleEvent.INITIALIZED: {LifecycleEvent.STARTING},
            LifecycleEvent.STARTING: {LifecycleEvent.STARTED, LifecycleEvent.ERROR},
            LifecycleEvent.STARTED: {LifecycleEvent.EXECUTING, LifecycleEvent.STOPPING},
            LifecycleEvent.EXECUTING: {LifecycleEvent.STARTED, LifecycleEvent.PAUSING, LifecycleEvent.ERROR},
            LifecycleEvent.PAUSING: {LifecycleEvent.PAUSED, LifecycleEvent.ERROR},
            LifecycleEvent.PAUSED: {LifecycleEvent.RESUMING, LifecycleEvent.STOPPING},
            LifecycleEvent.RESUMING: {LifecycleEvent.STARTED, LifecycleEvent.ERROR},
            LifecycleEvent.STOPPING: {LifecycleEvent.STOPPED, LifecycleEvent.ERROR},
            LifecycleEvent.STOPPED: {LifecycleEvent.TERMINATED},
            LifecycleEvent.ERROR: {LifecycleEvent.STOPPING, LifecycleEvent.TERMINATED},
            LifecycleEvent.TERMINATED: set()
        }
```

ì´ì œ Phase 3ì˜ ë‚˜ë¨¸ì§€ ì‘ì—…ë“¤ì„ ê³„ì† ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤. ê° Taskë³„ë¡œ ìƒì„¸í•œ SubTask ì‘ì—…ì§€ì‹œì„œë¥¼ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤.

#### SubTask 3.2.2: ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/initialization.py
from typing import Dict, Any, List, Optional
import asyncio
from dataclasses import dataclass
import yaml
import json

@dataclass
class InitializationConfig:
    """Configuration for agent initialization"""
    timeout: int = 30000  # milliseconds
    retry_attempts: int = 3
    retry_delay: int = 1000  # milliseconds
    required_resources: List[str] = None
    environment_variables: Dict[str, str] = None
    dependencies: List[str] = None

class AgentInitializer:
    """Manages agent initialization process"""
    
    def __init__(self):
        self.initialization_steps: List[InitializationStep] = []
        self.resource_validator = ResourceValidator()
        self.dependency_resolver = DependencyResolver()
        
    async def initialize_agent(
        self,
        agent: BaseAgent,
        config: InitializationConfig
    ) -> InitializationResult:
        """Initialize an agent with proper error handling and retries"""
        
        result = InitializationResult(agent_id=agent.agent_id)
        
        try:
            # Step 1: Validate environment
            await self._validate_environment(config)
            
            # Step 2: Check and acquire resources
            resources = await self._acquire_resources(config.required_resources)
            
            # Step 3: Resolve dependencies
            dependencies = await self._resolve_dependencies(config.dependencies)
            
            # Step 4: Configure agent
            await self._configure_agent(agent, config, resources, dependencies)
            
            # Step 5: Run initialization with timeout
            await asyncio.wait_for(
                agent.initialize(),
                timeout=config.timeout / 1000
            )
            
            # Step 6: Verify initialization
            await self._verify_initialization(agent)
            
            result.success = True
            result.initialized_at = datetime.utcnow()
            
        except asyncio.TimeoutError:
            result.error = "Initialization timeout"
            await self._handle_initialization_failure(agent, result)
            
        except Exception as e:
            result.error = str(e)
            await self._handle_initialization_failure(agent, result)
            
        return result
```

#### SubTask 3.2.3: ì—ì´ì „íŠ¸ ì¢…ë£Œ ì²˜ë¦¬
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 10ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/termination.ts
export class AgentTerminator {
  private readonly gracefulShutdownTimeout = 30000; // 30 seconds
  private readonly forceShutdownTimeout = 5000; // 5 seconds
  
  async terminateAgent(
    agent: IAgent,
    options: TerminationOptions = {}
  ): Promise<TerminationResult> {
    const result = new TerminationResult(agent.agentId);
    
    try {
      // Step 1: Signal termination intent
      await agent.onTerminationRequested();
      
      // Step 2: Stop accepting new tasks
      agent.stopAcceptingTasks();
      
      // Step 3: Wait for current tasks to complete
      if (options.waitForCompletion) {
        await this.waitForTaskCompletion(agent, options.completionTimeout);
      }
      
      // Step 4: Save agent state
      if (options.saveState) {
        await agent.saveState();
      }
      
      // Step 5: Cleanup resources
      await this.cleanupResources(agent);
      
      // Step 6: Graceful shutdown
      await this.gracefulShutdown(agent);
      
      result.success = true;
      result.terminatedAt = new Date();
      
    } catch (error) {
      // Force shutdown if graceful fails
      await this.forceShutdown(agent);
      result.forcedShutdown = true;
      result.error = error.message;
    }
    
    return result;
  }
  
  private async cleanupResources(agent: IAgent): Promise<void> {
    // Close connections
    await agent.closeConnections();
    
    // Release locks
    await agent.releaseLocks();
    
    // Clear caches
    await agent.clearCaches();
    
    // Unsubscribe from events
    await agent.unsubscribeFromEvents();
  }
}
```

#### SubTask 3.2.4: ìƒëª…ì£¼ê¸° ì´ë²¤íŠ¸ í•¸ë“¤ë§
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/lifecycle_events.py
from typing import Dict, List, Callable, Any
import asyncio
from dataclasses import dataclass
from datetime import datetime

@dataclass
class LifecycleEventData:
    event_type: LifecycleEvent
    agent_id: str
    timestamp: datetime
    metadata: Dict[str, Any]
    source: str

class LifecycleEventHandler:
    """Handles lifecycle events for agents"""
    
    def __init__(self):
        self.event_handlers: Dict[LifecycleEvent, List[EventHandler]] = {}
        self.event_history: List[LifecycleEventData] = []
        self.event_bus = EventBus()
        
    def register_handler(
        self,
        event_type: LifecycleEvent,
        handler: Callable,
        priority: int = 0
    ) -> None:
        """Register an event handler"""
        if event_type not in self.event_handlers:
            self.event_handlers[event_type] = []
            
        self.event_handlers[event_type].append(
            EventHandler(handler=handler, priority=priority)
        )
        
        # Sort by priority
        self.event_handlers[event_type].sort(
            key=lambda x: x.priority,
            reverse=True
        )
    
    async def emit_event(self, event_data: LifecycleEventData) -> None:
        """Emit a lifecycle event"""
        # Record in history
        self.event_history.append(event_data)
        
        # Publish to event bus
        await self.event_bus.publish(
            topic=f"agent.lifecycle.{event_data.event_type.value}",
            data=event_data
        )
        
        # Execute registered handlers
        if event_data.event_type in self.event_handlers:
            handlers = self.event_handlers[event_data.event_type]
            
            for handler in handlers:
                try:
                    await handler.execute(event_data)
                except Exception as e:
                    await self._handle_handler_error(handler, event_data, e)
```

### Task 3.3: ì—ì´ì „íŠ¸ ìƒíƒœ ê´€ë¦¬ ì‹œìŠ¤í…œ

#### SubTask 3.3.1: ìƒíƒœ ì €ì¥ì†Œ êµ¬í˜„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/state_store.py
from typing import Dict, Any, Optional, List
import json
import asyncio
from abc import ABC, abstractmethod
import aioredis
import aioboto3

class StateStore(ABC):
    """Abstract base class for agent state storage"""
    
    @abstractmethod
    async def save_state(self, agent_id: str, state: Dict[str, Any]) -> None:
        pass
    
    @abstractmethod
    async def load_state(self, agent_id: str) -> Optional[Dict[str, Any]]:
        pass
    
    @abstractmethod
    async def delete_state(self, agent_id: str) -> None:
        pass
    
    @abstractmethod
    async def list_states(self, prefix: str = "") -> List[str]:
        pass

class RedisStateStore(StateStore):
    """Redis-based state store for high-performance access"""
    
    def __init__(self, redis_url: str, ttl: int = 3600):
        self.redis_url = redis_url
        self.ttl = ttl
        self.redis: Optional[aioredis.Redis] = None
        
    async def connect(self) -> None:
        self.redis = await aioredis.from_url(self.redis_url)
        
    async def save_state(self, agent_id: str, state: Dict[str, Any]) -> None:
        key = f"agent:state:{agent_id}"
        value = json.dumps(state)
        
        await self.redis.setex(key, self.ttl, value)
        
        # Also save to sorted set for listing
        await self.redis.zadd(
            "agent:states",
            {agent_id: datetime.utcnow().timestamp()}
        )
    
    async def load_state(self, agent_id: str) -> Optional[Dict[str, Any]]:
        key = f"agent:state:{agent_id}"
        value = await self.redis.get(key)
        
        if value:
            return json.loads(value)
        return None

class DynamoDBStateStore(StateStore):
    """DynamoDB-based state store for persistent storage"""
    
    def __init__(self, table_name: str, region: str):
        self.table_name = table_name
        self.region = region
        self.dynamodb = None
        
    async def connect(self) -> None:
        session = aioboto3.Session()
        self.dynamodb = await session.resource('dynamodb', region_name=self.region).__aenter__()
        self.table = await self.dynamodb.Table(self.table_name)
```

#### SubTask 3.3.2: ìƒíƒœ ë™ê¸°í™” ë©”ì»¤ë‹ˆì¦˜
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/state_sync.ts
export class StateSynchronizer {
  private readonly syncInterval = 5000; // 5 seconds
  private readonly conflictResolver: ConflictResolver;
  private readonly stateValidator: StateValidator;
  
  constructor(
    private readonly localStore: StateStore,
    private readonly remoteStore: StateStore
  ) {
    this.conflictResolver = new ConflictResolver();
    this.stateValidator = new StateValidator();
  }
  
  async startSync(agentId: string): Promise<void> {
    const syncJob = async () => {
      try {
        // Load states from both stores
        const [localState, remoteState] = await Promise.all([
          this.localStore.load(agentId),
          this.remoteStore.load(agentId)
        ]);
        
        // Check for conflicts
        if (localState && remoteState) {
          const hasConflict = await this.detectConflict(localState, remoteState);
          
          if (hasConflict) {
            // Resolve conflict
            const resolvedState = await this.conflictResolver.resolve(
              localState,
              remoteState
            );
            
            // Save resolved state
            await Promise.all([
              this.localStore.save(agentId, resolvedState),
              this.remoteStore.save(agentId, resolvedState)
            ]);
          }
        } else if (localState && !remoteState) {
          // Push local to remote
          await this.remoteStore.save(agentId, localState);
        } else if (!localState && remoteState) {
          // Pull remote to local
          await this.localStore.save(agentId, remoteState);
        }
      } catch (error) {
        console.error(`State sync error for agent ${agentId}:`, error);
      }
    };
    
    // Start periodic sync
    setInterval(syncJob, this.syncInterval);
    
    // Initial sync
    await syncJob();
  }
}
```

#### SubTask 3.3.3: ìƒíƒœ ë²„ì „ ê´€ë¦¬
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/state_versioning.py
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
import hashlib
from datetime import datetime

@dataclass
class StateVersion:
    version_id: str
    agent_id: str
    state_data: Dict[str, Any]
    created_at: datetime
    created_by: str
    parent_version: Optional[str]
    metadata: Dict[str, Any]
    checksum: str

class StateVersionManager:
    """Manages versioning of agent states"""
    
    def __init__(self, storage: VersionStorage):
        self.storage = storage
        self.version_tree = {}
        
    async def save_version(
        self,
        agent_id: str,
        state: Dict[str, Any],
        created_by: str,
        parent_version: Optional[str] = None
    ) -> StateVersion:
        """Save a new version of agent state"""
        
        # Generate version ID
        version_id = self._generate_version_id(agent_id, state)
        
        # Calculate checksum
        checksum = self._calculate_checksum(state)
        
        # Create version object
        version = StateVersion(
            version_id=version_id,
            agent_id=agent_id,
            state_data=state,
            created_at=datetime.utcnow(),
            created_by=created_by,
            parent_version=parent_version,
            metadata={
                "size": len(json.dumps(state)),
                "keys": list(state.keys())
            },
            checksum=checksum
        )
        
        # Store version
        await self.storage.store_version(version)
        
        # Update version tree
        self._update_version_tree(version)
        
        return version
    
    async def get_version(self, version_id: str) -> Optional[StateVersion]:
        """Retrieve a specific version"""
        return await self.storage.get_version(version_id)
    
    async def list_versions(
        self,
        agent_id: str,
        limit: int = 10
    ) -> List[StateVersion]:
        """List versions for an agent"""
        return await self.storage.list_versions(agent_id, limit)
```

#### SubTask 3.3.4: ìƒíƒœ ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/state_migration.py
from typing import Dict, Any, Callable, List
from abc import ABC, abstractmethod
import semantic_version

class StateMigration(ABC):
    """Base class for state migrations"""
    
    @property
    @abstractmethod
    def from_version(self) -> str:
        pass
    
    @property
    @abstractmethod
    def to_version(self) -> str:
        pass
    
    @abstractmethod
    async def migrate_up(self, state: Dict[str, Any]) -> Dict[str, Any]:
        pass
    
    @abstractmethod
    async def migrate_down(self, state: Dict[str, Any]) -> Dict[str, Any]:
        pass

class StateMigrationManager:
    """Manages state migrations between versions"""
    
    def __init__(self):
        self.migrations: List[StateMigration] = []
        self.version_graph = {}
        
    def register_migration(self, migration: StateMigration) -> None:
        """Register a new migration"""
        self.migrations.append(migration)
        self._rebuild_version_graph()
        
    async def migrate_state(
        self,
        state: Dict[str, Any],
        from_version: str,
        to_version: str
    ) -> Dict[str, Any]:
        """Migrate state from one version to another"""
        
        # Find migration path
        path = self._find_migration_path(from_version, to_version)
        
        if not path:
            raise ValueError(
                f"No migration path found from {from_version} to {to_version}"
            )
        
        # Apply migrations in sequence
        current_state = state.copy()
        
        for migration in path:
            if migration.from_version == from_version:
                current_state = await migration.migrate_up(current_state)
            else:
                current_state = await migration.migrate_down(current_state)
        
        return current_state
```

### Task 3.4: ì—ì´ì „íŠ¸ ì„¤ì • ë° ì´ˆê¸°í™” ì‹œìŠ¤í…œ

#### SubTask 3.4.1: ì„¤ì • ìŠ¤í‚¤ë§ˆ ì •ì˜
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 10ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/config_schema.py
from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field, validator
from enum import Enum

class LogLevel(str, Enum):
    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

class ResourceRequirement(BaseModel):
    cpu: Optional[float] = Field(None, ge=0.1, le=16.0)
    memory: Optional[int] = Field(None, ge=128, le=65536)  # MB
    gpu: Optional[bool] = False
    storage: Optional[int] = Field(None, ge=1, le=1000)  # GB

class NetworkConfig(BaseModel):
    enable_http: bool = True
    enable_websocket: bool = False
    enable_grpc: bool = False
    timeout: int = Field(30000, ge=1000, le=300000)  # milliseconds
    retry_policy: Dict[str, Any] = {
        "max_attempts": 3,
        "backoff_multiplier": 2,
        "initial_delay": 1000
    }

class AgentConfig(BaseModel):
    """Configuration schema for agents"""
    
    # Basic settings
    agent_type: str
    version: str = "1.0.0"
    enabled: bool = True
    
    # Runtime settings
    max_concurrent_tasks: int = Field(10, ge=1, le=1000)
    task_timeout: int = Field(60000, ge=1000, le=3600000)  # milliseconds
    
    # Resource requirements
    resources: ResourceRequirement = ResourceRequirement()
    
    # Network configuration
    network: NetworkConfig = NetworkConfig()
    
    # Logging configuration
    logging: Dict[str, Any] = {
        "level": LogLevel.INFO,
        "format": "json",
        "destination": "cloudwatch"
    }
    
    # Model configuration (for AI agents)
    model_config: Optional[Dict[str, Any]] = None
    
    # Custom settings
    custom_settings: Dict[str, Any] = {}
    
    @validator('version')
    def validate_version(cls, v):
        # Validate semantic versioning
        import semantic_version
        semantic_version.Version(v)
        return v
```

#### SubTask 3.4.2: ì„¤ì • ë¡œë” êµ¬í˜„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/config_loader.ts
export class ConfigLoader {
  private readonly configSources: ConfigSource[] = [];
  private readonly validator: ConfigValidator;
  private readonly cache: ConfigCache;
  
  constructor() {
    this.validator = new ConfigValidator();
    this.cache = new ConfigCache();
  }
  
  addSource(source: ConfigSource): void {
    this.configSources.push(source);
  }
  
  async loadConfig(agentType: string): Promise<AgentConfig> {
    // Check cache first
    const cached = await this.cache.get(agentType);
    if (cached) {
      return cached;
    }
    
    // Load from sources in priority order
    let config: Partial<AgentConfig> = {};
    
    for (const source of this.configSources) {
      try {
        const sourceConfig = await source.load(agentType);
        if (sourceConfig) {
          config = this.mergeConfigs(config, sourceConfig);
        }
      } catch (error) {
        console.warn(`Failed to load from source ${source.name}:`, error);
      }
    }
    
    // Apply environment overrides
    config = this.applyEnvironmentOverrides(config);
    
    // Validate final config
    const validatedConfig = await this.validator.validate(config);
    
    // Cache the result
    await this.cache.set(agentType, validatedConfig);
    
    return validatedConfig;
  }
  
  private mergeConfigs(
    base: Partial<AgentConfig>,
    override: Partial<AgentConfig>
  ): Partial<AgentConfig> {
    return deepMerge(base, override, {
      arrayMerge: (target, source) => source,
      customMerge: (key) => {
        if (key === 'custom_settings') {
          return (target, source) => ({ ...target, ...source });
        }
      }
    });
  }
}

// Config sources
export class FileConfigSource implements ConfigSource {
  constructor(private readonly configPath: string) {}
  
  async load(agentType: string): Promise<Partial<AgentConfig>> {
    const filePath = path.join(this.configPath, `${agentType}.json`);
    if (await fs.pathExists(filePath)) {
      return await fs.readJson(filePath);
    }
    return null;
  }
}

export class DynamoDBConfigSource implements ConfigSource {
  constructor(private readonly tableName: string) {}
  
  async load(agentType: string): Promise<Partial<AgentConfig>> {
    const result = await dynamodb.get({
      TableName: this.tableName,
      Key: { agent_type: agentType }
    }).promise();
    
    return result.Item?.config || null;
  }
}
```

#### SubTask 3.4.3: ë™ì  ì„¤ì • ì—…ë°ì´íŠ¸
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/dynamic_config.py
from typing import Dict, Any, Callable, List
import asyncio
from dataclasses import dataclass
from datetime import datetime

@dataclass
class ConfigChange:
    path: str
    old_value: Any
    new_value: Any
    timestamp: datetime
    source: str

class DynamicConfigManager:
    """Manages dynamic configuration updates for agents"""
    
    def __init__(self):
        self.config_watchers: Dict[str, ConfigWatcher] = {}
        self.change_handlers: Dict[str, List[Callable]] = {}
        self.config_store = ConfigStore()
        
    async def watch_config(
        self,
        agent_id: str,
        config_path: str,
        handler: Callable[[ConfigChange], None]
    ) -> None:
        """Watch for configuration changes"""
        
        watcher_key = f"{agent_id}:{config_path}"
        
        if watcher_key not in self.config_watchers:
            watcher = ConfigWatcher(
                agent_id=agent_id,
                config_path=config_path,
                callback=self._on_config_change
            )
            self.config_watchers[watcher_key] = watcher
            await watcher.start()
        
        if watcher_key not in self.change_handlers:
            self.change_handlers[watcher_key] = []
        
        self.change_handlers[watcher_key].append(handler)
    
    async def update_config(
        self,
        agent_id: str,
        updates: Dict[str, Any],
        source: str = "api"
    ) -> None:
        """Update agent configuration"""
        
        current_config = await self.config_store.get(agent_id)
        
        # Apply updates
        changes = []
        for path, new_value in updates.items():
            old_value = self._get_nested_value(current_config, path)
            
            if old_value != new_value:
                self._set_nested_value(current_config, path, new_value)
                
                changes.append(ConfigChange(
                    path=path,
                    old_value=old_value,
                    new_value=new_value,
                    timestamp=datetime.utcnow(),
                    source=source
                ))
        
        # Save updated config
        await self.config_store.save(agent_id, current_config)
        
        # Notify watchers
        for change in changes:
            await self._notify_watchers(agent_id, change)
```

#### SubTask 3.4.4: ì„¤ì • ê²€ì¦ ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/config_validation.py
from typing import Dict, Any, List, Optional
from pydantic import ValidationError
import jsonschema

class ConfigValidator:
    """Validates agent configurations"""
    
    def __init__(self):
        self.schemas: Dict[str, Dict[str, Any]] = {}
        self.custom_validators: Dict[str, List[Callable]] = {}
        
    def register_schema(self, agent_type: str, schema: Dict[str, Any]) -> None:
        """Register a JSON schema for an agent type"""
        self.schemas[agent_type] = schema
        
    def register_validator(
        self,
        agent_type: str,
        validator: Callable[[Dict[str, Any]], Optional[str]]
    ) -> None:
        """Register a custom validator function"""
        if agent_type not in self.custom_validators:
            self.custom_validators[agent_type] = []
        
        self.custom_validators[agent_type].append(validator)
    
    async def validate(
        self,
        agent_type: str,
        config: Dict[str, Any]
    ) -> ValidationResult:
        """Validate configuration against all registered validators"""
        
        result = ValidationResult(valid=True)
        
        # Schema validation
        if agent_type in self.schemas:
            try:
                jsonschema.validate(config, self.schemas[agent_type])
            except jsonschema.ValidationError as e:
                result.valid = False
                result.add_error(f"Schema validation failed: {e.message}")
        
        # Pydantic model validation
        try:
            AgentConfig(**config)
        except ValidationError as e:
            result.valid = False
            for error in e.errors():
                result.add_error(
                    f"{'.'.join(str(x) for x in error['loc'])}: {error['msg']}"
                )
        
        # Custom validators
        if agent_type in self.custom_validators:
            for validator in self.custom_validators[agent_type]:
                error = await validator(config)
                if error:
                    result.valid = False
                    result.add_error(error)
        
        return result
```

### Task 3.5: ì—ì´ì „íŠ¸ ì—ëŸ¬ ì²˜ë¦¬ í”„ë ˆì„ì›Œí¬

#### SubTask 3.5.1: ì—ëŸ¬ ë¶„ë¥˜ ì²´ê³„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 10ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/error_types.py
from typing import Optional, Dict, Any
from enum import Enum
from dataclasses import dataclass

class ErrorSeverity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class ErrorCategory(Enum):
    CONFIGURATION = "configuration"
    INITIALIZATION = "initialization"
    EXECUTION = "execution"
    COMMUNICATION = "communication"
    RESOURCE = "resource"
    DEPENDENCY = "dependency"
    VALIDATION = "validation"
    TIMEOUT = "timeout"
    PERMISSION = "permission"
    EXTERNAL_SERVICE = "external_service"

@dataclass
class AgentError(Exception):
    """Base error class for agent-related errors"""
    
    error_code: str
    message: str
    category: ErrorCategory
    severity: ErrorSeverity
    agent_id: Optional[str] = None
    context: Dict[str, Any] = None
    recoverable: bool = True
    retry_after: Optional[int] = None  # seconds
    
    def __post_init__(self):
        super().__init__(self.message)
        
    def to_dict(self) -> Dict[str, Any]:
        return {
            "error_code": self.error_code,
            "message": self.message,
            "category": self.category.value,
            "severity": self.severity.value,
            "agent_id": self.agent_id,
            "context": self.context,
            "recoverable": self.recoverable,
            "retry_after": self.retry_after
        }

# Specific error types
class ConfigurationError(AgentError):
    def __init__(self, message: str, **kwargs):
        super().__init__(
            error_code="AGENT_CONFIG_ERROR",
            message=message,
            category=ErrorCategory.CONFIGURATION,
            severity=ErrorSeverity.HIGH,
            **kwargs
        )

class InitializationError(AgentError):
    def __init__(self, message: str, **kwargs):
        super().__init__(
            error_code="AGENT_INIT_ERROR",
            message=message,
            category=ErrorCategory.INITIALIZATION,
            severity=ErrorSeverity.CRITICAL,
            **kwargs
        )

class ExecutionError(AgentError):
    def __init__(self, message: str, **kwargs):
        super().__init__(
            error_code="AGENT_EXEC_ERROR",
            message=message,
            category=ErrorCategory.EXECUTION,
            severity=ErrorSeverity.MEDIUM,
            **kwargs
        )
```

#### SubTask 3.5.2: ì—ëŸ¬ í•¸ë“¤ëŸ¬ êµ¬í˜„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/error_handler.ts
export class AgentErrorHandler {
  private readonly handlers: Map<string, ErrorHandlerFunction[]> = new Map();
  private readonly fallbackHandler: ErrorHandlerFunction;
  private readonly errorRecorder: ErrorRecorder;
  
  constructor() {
    this.errorRecorder = new ErrorRecorder();
    this.fallbackHandler = this.defaultFallbackHandler;
  }
  
  registerHandler(
    errorPattern: string | RegExp,
    handler: ErrorHandlerFunction,
    priority: number = 0
  ): void {
    const key = errorPattern.toString();
    
    if (!this.handlers.has(key)) {
      this.handlers.set(key, []);
    }
    
    this.handlers.get(key)!.push({
      handler,
      priority,
      pattern: errorPattern
    });
    
    // Sort by priority
    this.handlers.get(key)!.sort((a, b) => b.priority - a.priority);
  }
  
  async handleError(
    error: AgentError,
    context: ErrorContext
  ): Promise<ErrorHandlingResult> {
    // Record error
    await this.errorRecorder.record(error, context);
    
    // Find matching handlers
    const matchingHandlers = this.findMatchingHandlers(error);
    
    // Execute handlers in priority order
    for (const handlerInfo of matchingHandlers) {
      try {
        const result = await handlerInfo.handler(error, context);
        
        if (result.handled) {
          return result;
        }
      } catch (handlerError) {
        console.error('Error in error handler:', handlerError);
      }
    }
    
    // Use fallback handler
    return await this.fallbackHandler(error, context);
  }
  
  private findMatchingHandlers(error: AgentError): ErrorHandlerInfo[] {
    const handlers: ErrorHandlerInfo[] = [];
    
    for (const [pattern, handlerList] of this.handlers) {
      if (this.matchesPattern(error, pattern)) {
        handlers.push(...handlerList);
      }
    }
    
    return handlers.sort((a, b) => b.priority - a.priority);
  }
  
  private matchesPattern(error: AgentError, pattern: string): boolean {
    // Match by error code
    if (error.error_code === pattern) return true;
    
    // Match by category
    if (error.category === pattern) return true;
    
    // Match by regex
    if (pattern.startsWith('/') && pattern.endsWith('/')) {
      const regex = new RegExp(pattern.slice(1, -1));
      return regex.test(error.error_code) || regex.test(error.message);
    }
    
    return false;
  }
}
```

#### SubTask 3.5.3: ì—ëŸ¬ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/error_recovery.py
from typing import Dict, Any, List, Optional, Callable
import asyncio
from abc import ABC, abstractmethod
from dataclasses import dataclass

@dataclass
class RecoveryStrategy:
    name: str
    description: str
    applicable_errors: List[ErrorCategory]
    max_attempts: int
    backoff_strategy: str  # linear, exponential, fibonacci
    recovery_action: Callable

class RecoveryManager:
    """Manages error recovery strategies for agents"""
    
    def __init__(self):
        self.strategies: Dict[str, RecoveryStrategy] = {}
        self.recovery_history: List[RecoveryAttempt] = []
        self.circuit_breakers: Dict[str, CircuitBreaker] = {}
        
    def register_strategy(self, strategy: RecoveryStrategy) -> None:
        """Register a recovery strategy"""
        self.strategies[strategy.name] = strategy
        
    async def attempt_recovery(
        self,
        error: AgentError,
        agent: BaseAgent
    ) -> RecoveryResult:
        """Attempt to recover from an error"""
        
        # Check circuit breaker
        breaker_key = f"{agent.agent_id}:{error.category.value}"
        if breaker_key in self.circuit_breakers:
            if self.circuit_breakers[breaker_key].is_open():
                return RecoveryResult(
                    success=False,
                    reason="Circuit breaker is open"
                )
        
        # Find applicable strategies
        applicable_strategies = self._find_applicable_strategies(error)
        
        # Try strategies in order
        for strategy in applicable_strategies:
            result = await self._execute_recovery_strategy(
                strategy,
                error,
                agent
            )
            
            if result.success:
                return result
        
        # No recovery possible
        return RecoveryResult(
            success=False,
            reason="No applicable recovery strategy"
        )
    
    async def _execute_recovery_strategy(
        self,
        strategy: RecoveryStrategy,
        error: AgentError,
        agent: BaseAgent
    ) -> RecoveryResult:
        """Execute a specific recovery strategy"""
        
        attempt = RecoveryAttempt(
            strategy_name=strategy.name,
            error=error,
            agent_id=agent.agent_id,
            started_at=datetime.utcnow()
        )
        
        for attempt_num in range(strategy.max_attempts):
            try:
                # Calculate backoff
                delay = self._calculate_backoff(
                    attempt_num,
                    strategy.backoff_strategy
                )
                
                if attempt_num > 0:
                    await asyncio.sleep(delay)
                
                # Execute recovery action
                await strategy.recovery_action(agent, error)
                
                # Verify recovery
                if await self._verify_recovery(agent):
                    attempt.success = True
                    attempt.completed_at = datetime.utcnow()
                    self.recovery_history.append(attempt)
                    
                    return RecoveryResult(
                        success=True,
                        attempts=attempt_num + 1,
                        strategy_used=strategy.name
                    )
                    
            except Exception as e:
                # Log recovery failure
                attempt.errors.append(str(e))
        
        # Recovery failed
        attempt.success = False
        attempt.completed_at = datetime.utcnow()
        self.recovery_history.append(attempt)
        
        # Update circuit breaker
        self._update_circuit_breaker(agent.agent_id, error.category)
        
        return RecoveryResult(
            success=False,
            attempts=strategy.max_attempts,
            reason=f"Recovery failed after {strategy.max_attempts} attempts"
        )
```

#### SubTask 3.5.4: ì—ëŸ¬ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/error_monitoring.py
from typing import Dict, Any, List, Optional
import asyncio
from collections import defaultdict
from datetime import datetime, timedelta

class ErrorMonitor:
    """Monitors agent errors and sends alerts"""
    
    def __init__(self):
        self.error_metrics = ErrorMetrics()
        self.alert_manager = AlertManager()
        self.error_patterns = ErrorPatternDetector()
        
    async def monitor_error(self, error: AgentError, context: Dict[str, Any]) -> None:
        """Monitor and analyze an error"""
        
        # Update metrics
        await self.error_metrics.record(error)
        
        # Detect patterns
        patterns = await self.error_patterns.analyze(error)
        
        # Check alert conditions
        alerts = await self._check_alert_conditions(error, patterns)
        
        # Send alerts if needed
        for alert in alerts:
            await self.alert_manager.send(alert)
    
    async def _check_alert_conditions(
        self,
        error: AgentError,
        patterns: List[ErrorPattern]
    ) -> List[Alert]:
        """Check if error conditions warrant alerts"""
        
        alerts = []
        
        # Critical error alert
        if error.severity == ErrorSeverity.CRITICAL:
            alerts.append(Alert(
                level=AlertLevel.CRITICAL,
                title=f"Critical error in agent {error.agent_id}",
                message=error.message,
                metadata=error.to_dict()
            ))
        
        # Error rate threshold
        error_rate = await self.error_metrics.get_error_rate(
            agent_id=error.agent_id,
            window=timedelta(minutes=5)
        )
        
        if error_rate > 0.1:  # 10% error rate
            alerts.append(Alert(
                level=AlertLevel.WARNING,
                title=f"High error rate for agent {error.agent_id}",
                message=f"Error rate: {error_rate:.2%}",
                metadata={"error_rate": error_rate}
            ))
        
        # Pattern-based alerts
        for pattern in patterns:
            if pattern.severity == "high":
                alerts.append(Alert(
                    level=AlertLevel.WARNING,
                    title=f"Error pattern detected: {pattern.name}",
                    message=pattern.description,
                    metadata=pattern.to_dict()
                ))
        
        return alerts

class ErrorMetrics:
    """Tracks error metrics for monitoring"""
    
    def __init__(self):
        self.error_counts = defaultdict(int)
        self.error_history = defaultdict(list)
        
    async def record(self, error: AgentError) -> None:
        """Record an error occurrence"""
        
        # Count by category
        self.error_counts[error.category.value] += 1
        
        # Count by agent
        self.error_counts[f"agent:{error.agent_id}"] += 1
        
        # Add to history
        self.error_history[error.agent_id].append({
            "timestamp": datetime.utcnow(),
            "error": error.to_dict()
        })
        
        # Trim old history
        cutoff = datetime.utcnow() - timedelta(hours=24)
        self.error_history[error.agent_id] = [
            e for e in self.error_history[error.agent_id]
            if e["timestamp"] > cutoff
        ]
```

ì´ì œ Task 3.6ë¶€í„° Task 3.10ê¹Œì§€ ì—ì´ì „íŠ¸ í†µì‹  ë° ë©”ì‹œì§• ì‹œìŠ¤í…œì— ëŒ€í•œ ì‘ì—…ì§€ì‹œì„œë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

### Task 3.6: ì—ì´ì „íŠ¸ ê°„ í†µì‹  í”„ë¡œí† ì½œ

#### SubTask 3.6.1: ë©”ì‹œì§€ í¬ë§· ì •ì˜
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 10ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/messaging/message_format.py
from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field
from enum import Enum
import uuid
from datetime import datetime

class MessageType(Enum):
    REQUEST = "request"
    RESPONSE = "response"
    EVENT = "event"
    COMMAND = "command"
    QUERY = "query"
    BROADCAST = "broadcast"
    ACKNOWLEDGMENT = "ack"
    ERROR = "error"

class MessagePriority(Enum):
    LOW = 0
    NORMAL = 1
    HIGH = 2
    URGENT = 3

@dataclass
class AgentMessage:
    """Standard message format for inter-agent communication"""
    
    # Message identification
    message_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    correlation_id: Optional[str] = None
    
    # Message metadata
    message_type: MessageType = MessageType.REQUEST
    priority: MessagePriority = MessagePriority.NORMAL
    timestamp: datetime = field(default_factory=datetime.utcnow)
    ttl: Optional[int] = None  # Time to live in seconds
    
    # Sender/Receiver information
    sender_id: str = ""
    sender_type: str = ""
    recipient_id: Optional[str] = None  # None for broadcasts
    recipient_type: Optional[str] = None
    
    # Message content
    action: str = ""
    payload: Dict[str, Any] = field(default_factory=dict)
    
    # Routing information
    routing_key: Optional[str] = None
    reply_to: Optional[str] = None
    
    # Security
    signature: Optional[str] = None
    encrypted: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert message to dictionary for serialization"""
        return {
            "message_id": self.message_id,
            "correlation_id": self.correlation_id,
            "message_type": self.message_type.value,
            "priority": self.priority.value,
            "timestamp": self.timestamp.isoformat(),
            "ttl": self.ttl,
            "sender_id": self.sender_id,
            "sender_type": self.sender_type,
            "recipient_id": self.recipient_id,
            "recipient_type": self.recipient_type,
            "action": self.action,
            "payload": self.payload,
            "routing_key": self.routing_key,
            "reply_to": self.reply_to,
            "signature": self.signature,
            "encrypted": self.encrypted
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'AgentMessage':
        """Create message from dictionary"""
        data = data.copy()
        
        # Convert enums
        if 'message_type' in data:
            data['message_type'] = MessageType(data['message_type'])
        if 'priority' in data:
            data['priority'] = MessagePriority(data['priority'])
        
        # Convert timestamp
        if 'timestamp' in data and isinstance(data['timestamp'], str):
            data['timestamp'] = datetime.fromisoformat(data['timestamp'])
        
        return cls(**data)
```

#### SubTask 3.6.2: í†µì‹  ì±„ë„ êµ¬í˜„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/messaging/communication_channel.ts
export abstract class CommunicationChannel {
  abstract async send(message: AgentMessage): Promise<void>;
  abstract async receive(filter?: MessageFilter): Promise<AgentMessage | null>;
  abstract async subscribe(
    topic: string,
    handler: MessageHandler
  ): Promise<Subscription>;
  abstract async unsubscribe(subscription: Subscription): Promise<void>;
  abstract async close(): Promise<void>;
}

export class InMemoryChannel extends CommunicationChannel {
  private readonly messages: Map<string, AgentMessage[]> = new Map();
  private readonly subscribers: Map<string, Set<MessageHandler>> = new Map();
  private readonly messageQueue: AsyncQueue<AgentMessage>;
  
  constructor() {
    super();
    this.messageQueue = new AsyncQueue<AgentMessage>();
  }
  
  async send(message: AgentMessage): Promise<void> {
    // Store message
    const key = message.recipient_id || 'broadcast';
    if (!this.messages.has(key)) {
      this.messages.set(key, []);
    }
    this.messages.get(key)!.push(message);
    
    // Notify subscribers
    await this.notifySubscribers(message);
    
    // Add to queue
    await this.messageQueue.enqueue(message);
  }
  
  async receive(filter?: MessageFilter): Promise<AgentMessage | null> {
    // Try to get from queue first
    const queued = await this.messageQueue.dequeue(1000); // 1 second timeout
    if (queued && (!filter || this.matchesFilter(queued, filter))) {
      return queued;
    }
    
    // Search in stored messages
    for (const [key, messages] of this.messages) {
      for (let i = messages.length - 1; i >= 0; i--) {
        const message = messages[i];
        if (!filter || this.matchesFilter(message, filter)) {
          messages.splice(i, 1); // Remove message
          return message;
        }
      }
    }
    
    return null;
  }
  
  private async notifySubscribers(message: AgentMessage): Promise<void> {
    const topic = message.routing_key || message.action;
    const handlers = this.subscribers.get(topic) || new Set();
    
    // Also check wildcard subscriptions
    const wildcardHandlers = this.subscribers.get('*') || new Set();
    
    const allHandlers = new Set([...handlers, ...wildcardHandlers]);
    
    // Execute handlers in parallel
    await Promise.all(
      Array.from(allHandlers).map(handler => 
        handler(message).catch(error => 
          console.error('Handler error:', error)
        )
      )
    );
  }
}

export class RedisChannel extends CommunicationChannel {
  private readonly redis: Redis;
  private readonly pubsub: Redis;
  private readonly subscriptions: Map<string, Subscription> = new Map();
  
  constructor(redisUrl: string) {
    super();
    this.redis = new Redis(redisUrl);
    this.pubsub = new Redis(redisUrl);
  }
  
  async send(message: AgentMessage): Promise<void> {
    const channel = message.routing_key || message.recipient_id || 'broadcast';
    const serialized = JSON.stringify(message.to_dict());
    
    // Publish to channel
    await this.redis.publish(channel, serialized);
    
    // Store in list for persistence
    if (message.ttl) {
      await this.redis.setex(
        `message:${message.message_id}`,
        message.ttl,
        serialized
      );
    }
  }
}
```

#### SubTask 3.6.3: í”„ë¡œí† ì½œ í•¸ë“¤ëŸ¬ êµ¬í˜„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/messaging/protocol_handler.py
from typing import Dict, Any, Callable, Optional, List
import asyncio
from abc import ABC, abstractmethod

class ProtocolHandler(ABC):
    """Base class for protocol handlers"""
    
    @abstractmethod
    async def handle_request(
        self,
        message: AgentMessage,
        agent: BaseAgent
    ) -> AgentMessage:
        pass
    
    @abstractmethod
    async def handle_response(
        self,
        message: AgentMessage,
        agent: BaseAgent
    ) -> None:
        pass

class RequestResponseProtocol(ProtocolHandler):
    """Handles request-response pattern"""
    
    def __init__(self):
        self.pending_requests: Dict[str, PendingRequest] = {}
        self.request_handlers: Dict[str, RequestHandler] = {}
        
    def register_handler(
        self,
        action: str,
        handler: Callable[[AgentMessage, BaseAgent], Any]
    ) -> None:
        """Register a handler for a specific action"""
        self.request_handlers[action] = handler
    
    async def send_request(
        self,
        agent: BaseAgent,
        target_agent_id: str,
        action: str,
        payload: Dict[str, Any],
        timeout: int = 30000  # milliseconds
    ) -> Any:
        """Send a request and wait for response"""
        
        # Create request message
        request = AgentMessage(
            message_type=MessageType.REQUEST,
            sender_id=agent.agent_id,
            sender_type=agent.agent_type,
            recipient_id=target_agent_id,
            action=action,
            payload=payload,
            reply_to=agent.agent_id
        )
        
        # Create pending request
        future = asyncio.Future()
        pending = PendingRequest(
            request=request,
            future=future,
            timeout=timeout,
            created_at=datetime.utcnow()
        )
        
        self.pending_requests[request.message_id] = pending
        
        # Send request
        await agent.send_message(request)
        
        # Wait for response with timeout
        try:
            response = await asyncio.wait_for(
                future,
                timeout=timeout / 1000
            )
            return response
        except asyncio.TimeoutError:
            del self.pending_requests[request.message_id]
            raise TimeoutError(f"Request timeout after {timeout}ms")
    
    async def handle_request(
        self,
        message: AgentMessage,
        agent: BaseAgent
    ) -> AgentMessage:
        """Handle incoming request"""
        
        if message.action not in self.request_handlers:
            return self._create_error_response(
                message,
                f"Unknown action: {message.action}"
            )
        
        try:
            # Execute handler
            handler = self.request_handlers[message.action]
            result = await handler(message, agent)
            
            # Create response
            return AgentMessage(
                message_type=MessageType.RESPONSE,
                correlation_id=message.message_id,
                sender_id=agent.agent_id,
                sender_type=agent.agent_type,
                recipient_id=message.sender_id,
                action=f"{message.action}_response",
                payload={"result": result}
            )
            
        except Exception as e:
            return self._create_error_response(message, str(e))
```

#### SubTask 3.6.4: ë³´ì•ˆ í†µì‹  ë ˆì´ì–´
**ë‹´ë‹¹ì**: ë³´ì•ˆ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/messaging/secure_communication.py
from typing import Dict, Any, Tuple
import hashlib
import hmac
import base64
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.asymmetric import rsa, padding

class SecureCommunicationLayer:
    """Provides encryption and authentication for agent messages"""
    
    def __init__(self):
        self.key_manager = KeyManager()
        self.certificate_store = CertificateStore()
        
    async def encrypt_message(
        self,
        message: AgentMessage,
        recipient_public_key: bytes
    ) -> AgentMessage:
        """Encrypt message payload"""
        
        # Generate symmetric key for this message
        symmetric_key = Fernet.generate_key()
        cipher = Fernet(symmetric_key)
        
        # Encrypt payload
        payload_bytes = json.dumps(message.payload).encode()
        encrypted_payload = cipher.encrypt(payload_bytes)
        
        # Encrypt symmetric key with recipient's public key
        public_key = serialization.load_pem_public_key(recipient_public_key)
        encrypted_key = public_key.encrypt(
            symmetric_key,
            padding.OAEP(
                mgf=padding.MGF1(algorithm=hashes.SHA256()),
                algorithm=hashes.SHA256(),
                label=None
            )
        )
        
        # Update message
        message.payload = {
            "encrypted_data": base64.b64encode(encrypted_payload).decode(),
            "encrypted_key": base64.b64encode(encrypted_key).decode()
        }
        message.encrypted = True
        
        return message
    
    async def decrypt_message(
        self,
        message: AgentMessage,
        recipient_private_key: bytes
    ) -> AgentMessage:
        """Decrypt message payload"""
        
        if not message.encrypted:
            return message
        
        # Extract encrypted data
        encrypted_data = base64.b64decode(message.payload["encrypted_data"])
        encrypted_key = base64.b64decode(message.payload["encrypted_key"])
        
        # Decrypt symmetric key
        private_key = serialization.load_pem_private_key(
            recipient_private_key,
            password=None
        )
        symmetric_key = private_key.decrypt(
            encrypted_key,
            padding.OAEP(
                mgf=padding.MGF1(algorithm=hashes.SHA256()),
                algorithm=hashes.SHA256(),
                label=None
            )
        )
        
        # Decrypt payload
        cipher = Fernet(symmetric_key)
        decrypted_payload = cipher.decrypt(encrypted_data)
        
        # Update message
        message.payload = json.loads(decrypted_payload.decode())
        message.encrypted = False
        
        return message
    
    async def sign_message(
        self,
        message: AgentMessage,
        sender_private_key: bytes
    ) -> AgentMessage:
        """Sign message for authentication"""
        
        # Create message digest
        message_data = json.dumps({
            "message_id": message.message_id,
            "sender_id": message.sender_id,
            "recipient_id": message.recipient_id,
            "action": message.action,
            "payload": message.payload,
            "timestamp": message.timestamp.isoformat()
        }, sort_keys=True)
        
        # Sign digest
        private_key = serialization.load_pem_private_key(
            sender_private_key,
            password=None
        )
        
        signature = private_key.sign(
            message_data.encode(),
            padding.PSS(
                mgf=padding.MGF1(hashes.SHA256()),
                salt_length=padding.PSS.MAX_LENGTH
            ),
            hashes.SHA256()
        )
        
        message.signature = base64.b64encode(signature).decode()
        return message
```

### Task 3.7: ë©”ì‹œì§€ í ì‹œìŠ¤í…œ

#### SubTask 3.7.1: í ì¸í„°í˜ì´ìŠ¤ ì •ì˜
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 10ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/messaging/queue_interface.py
from typing import Generic, TypeVar, Optional, List, Callable
from abc import ABC, abstractmethod
import asyncio

T = TypeVar('T')

class QueueInterface(ABC, Generic[T]):
    """Abstract interface for message queues"""
    
    @abstractmethod
    async def enqueue(self, item: T, priority: int = 0) -> None:
        """Add item to queue"""
        pass
    
    @abstractmethod
    async def dequeue(self, timeout: Optional[int] = None) -> Optional[T]:
        """Remove and return item from queue"""
        pass
    
    @abstractmethod
    async def peek(self) -> Optional[T]:
        """View next item without removing"""
        pass
    
    @abstractmethod
    async def size(self) -> int:
        """Get current queue size"""
        pass
    
    @abstractmethod
    async def clear(self) -> None:
        """Clear all items from queue"""
        pass
    
    @abstractmethod
    async def subscribe(
        self,
        callback: Callable[[T], None],
        filter_fn: Optional[Callable[[T], bool]] = None
    ) -> str:
        """Subscribe to queue events"""
        pass
    
    @abstractmethod
    async def unsubscribe(self, subscription_id: str) -> None:
        """Unsubscribe from queue events"""
        pass

class PriorityQueue(QueueInterface[T]):
    """Priority queue implementation"""
    
    def __init__(self, max_size: Optional[int] = None):
        self._queue = asyncio.PriorityQueue(maxsize=max_size or 0)
        self._subscribers: Dict[str, QueueSubscriber] = {}
        self._counter = 0  # For FIFO ordering of same priority
        
    async def enqueue(self, item: T, priority: int = 0) -> None:
        """Add item with priority (lower number = higher priority)"""
        self._counter += 1
        await self._queue.put((priority, self._counter, item))
        
        # Notify subscribers
        await self._notify_subscribers(item)
    
    async def dequeue(self, timeout: Optional[int] = None) -> Optional[T]:
        """Remove and return highest priority item"""
        try:
            if timeout:
                priority, counter, item = await asyncio.wait_for(
                    self._queue.get(),
                    timeout=timeout / 1000
                )
            else:
                priority, counter, item = await self._queue.get()
            
            return item
        except asyncio.TimeoutError:
            return None
```

#### SubTask 3.7.2: ë¶„ì‚° í êµ¬í˜„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/messaging/distributed_queue.ts
export class DistributedQueue<T> implements QueueInterface<T> {
  private readonly sqs: AWS.SQS;
  private readonly queueUrl: string;
  private readonly serializer: MessageSerializer<T>;
  
  constructor(
    queueName: string,
    options: DistributedQueueOptions = {}
  ) {
    this.sqs = new AWS.SQS({
      region: options.region || process.env.AWS_REGION
    });
    
    this.queueUrl = options.queueUrl || 
      `https://sqs.${options.region}.amazonaws.com/${options.accountId}/${queueName}`;
    
    this.serializer = options.serializer || new JSONSerializer<T>();
  }
  
  async enqueue(item: T, options: EnqueueOptions = {}): Promise<void> {
    const message: AWS.SQS.SendMessageRequest = {
      QueueUrl: this.queueUrl,
      MessageBody: await this.serializer.serialize(item),
      MessageAttributes: {
        priority: {
          StringValue: String(options.priority || 0),
          DataType: 'Number'
        },
        timestamp: {
          StringValue: new Date().toISOString(),
          DataType: 'String'
        }
      }
    };
    
    if (options.delaySeconds) {
      message.DelaySeconds = options.delaySeconds;
    }
    
    if (options.deduplicationId) {
      message.MessageDeduplicationId = options.deduplicationId;
    }
    
    await this.sqs.sendMessage(message).promise();
  }
  
  async enqueueBatch(items: T[], options: EnqueueOptions = {}): Promise<void> {
    const chunks = this.chunkArray(items, 10); // SQS limit
    
    for (const chunk of chunks) {
      const entries = await Promise.all(
        chunk.map(async (item, index) => ({
          Id: String(index),
          MessageBody: await this.serializer.serialize(item),
          MessageAttributes: {
            priority: {
              StringValue: String(options.priority || 0),
              DataType: 'Number'
            }
          }
        }))
      );
      
      await this.sqs.sendMessageBatch({
        QueueUrl: this.queueUrl,
        Entries: entries
      }).promise();
    }
  }
  
  async dequeue(
    count: number = 1,
    timeout: number = 30000
  ): Promise<T[]> {
    const params: AWS.SQS.ReceiveMessageRequest = {
      QueueUrl: this.queueUrl,
      MaxNumberOfMessages: Math.min(count, 10), // SQS limit
      WaitTimeSeconds: Math.floor(timeout / 1000),
      MessageAttributeNames: ['All'],
      AttributeNames: ['All']
    };
    
    const result = await this.sqs.receiveMessage(params).promise();
    
    if (!result.Messages || result.Messages.length === 0) {
      return [];
    }
    
    const items: T[] = [];
    
    for (const message of result.Messages) {
      try {
        const item = await this.serializer.deserialize(message.Body!);
        items.push(item);
        
        // Delete message after successful processing
        await this.sqs.deleteMessage({
          QueueUrl: this.queueUrl,
          ReceiptHandle: message.ReceiptHandle!
        }).promise();
      } catch (error) {
        console.error('Failed to process message:', error);
        // Message will become visible again after visibility timeout
      }
    }
    
    return items;
  }
  
  async size(): Promise<number> {
    const attributes = await this.sqs.getQueueAttributes({
      QueueUrl: this.queueUrl,
      AttributeNames: ['ApproximateNumberOfMessages']
    }).promise();
    
    return parseInt(
      attributes.Attributes?.ApproximateNumberOfMessages || '0'
    );
  }
}
```

#### SubTask 3.7.3: í ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/messaging/queue_monitor.py
from typing import Dict, Any, List, Optional
import asyncio
from dataclasses import dataclass
from datetime import datetime, timedelta

@dataclass
class QueueMetrics:
    queue_name: str
    current_size: int
    enqueue_rate: float  # messages per second
    dequeue_rate: float
    average_message_age: float  # seconds
    dead_letter_count: int
    error_rate: float
    timestamp: datetime

class QueueMonitor:
    """Monitors queue health and performance"""
    
    def __init__(self):
        self.metrics_store = MetricsStore()
        self.alert_rules: List[AlertRule] = []
        self.monitoring_tasks: Dict[str, asyncio.Task] = {}
        
    async def start_monitoring(
        self,
        queue_name: str,
        queue: QueueInterface,
        interval: int = 60  # seconds
    ) -> None:
        """Start monitoring a queue"""
        
        if queue_name in self.monitoring_tasks:
            raise ValueError(f"Already monitoring queue: {queue_name}")
        
        task = asyncio.create_task(
            self._monitor_queue(queue_name, queue, interval)
        )
        self.monitoring_tasks[queue_name] = task
    
    async def _monitor_queue(
        self,
        queue_name: str,
        queue: QueueInterface,
        interval: int
    ) -> None:
        """Monitor queue continuously"""
        
        previous_metrics = None
        
        while True:
            try:
                # Collect metrics
                metrics = await self._collect_metrics(queue_name, queue)
                
                # Calculate rates if we have previous metrics
                if previous_metrics:
                    metrics.enqueue_rate = self._calculate_rate(
                        previous_metrics.current_size,
                        metrics.current_size,
                        interval
                    )
                
                # Store metrics
                await self.metrics_store.store(metrics)
                
                # Check alert rules
                await self._check_alerts(metrics)
                
                # Update CloudWatch
                await self._update_cloudwatch(metrics)
                
                previous_metrics = metrics
                
            except Exception as e:
                print(f"Error monitoring queue {queue_name}: {e}")
            
            await asyncio.sleep(interval)
    
    async def _collect_metrics(
        self,
        queue_name: str,
        queue: QueueInterface
    ) -> QueueMetrics:
        """Collect current queue metrics"""
        
        # Get basic metrics
        current_size = await queue.size()
        
        # Get additional metrics from queue attributes
        attributes = await queue.get_attributes()
        
        return QueueMetrics(
            queue_name=queue_name,
            current_size=current_size,
            enqueue_rate=0.0,  # Will be calculated
            dequeue_rate=0.0,  # Will be calculated
            average_message_age=attributes.get('average_age', 0.0),
            dead_letter_count=attributes.get('dead_letter_count', 0),
            error_rate=attributes.get('error_rate', 0.0),
            timestamp=datetime.utcnow()
        )
```

#### SubTask 3.7.4: ë°ë“œë ˆí„° í ì²˜ë¦¬
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/messaging/dead_letter_queue.py
from typing import Dict, Any, Optional, List, Callable
import asyncio
from dataclasses import dataclass

@dataclass
class DeadLetterMessage:
    original_message: AgentMessage
    error_count: int
    last_error: str
    first_failure: datetime
    last_failure: datetime
    processing_agent: str

class DeadLetterQueueHandler:
    """Handles messages that fail processing"""
    
    def __init__(self, max_retries: int = 3):
        self.max_retries = max_retries
        self.dlq = PriorityQueue[DeadLetterMessage]()
        self.retry_strategies: Dict[str, RetryStrategy] = {}
        self.recovery_handlers: Dict[str, RecoveryHandler] = {}
        
    def register_retry_strategy(
        self,
        error_pattern: str,
        strategy: RetryStrategy
    ) -> None:
        """Register a retry strategy for specific error patterns"""
        self.retry_strategies[error_pattern] = strategy
    
    async def handle_failed_message(
        self,
        message: AgentMessage,
        error: Exception,
        processing_agent: str
    ) -> None:
        """Handle a message that failed processing"""
        
        # Check if message is already a dead letter
        existing = await self._find_existing_dead_letter(message.message_id)
        
        if existing:
            # Update existing dead letter
            existing.error_count += 1
            existing.last_error = str(error)
            existing.last_failure = datetime.utcnow()
            
            if existing.error_count > self.max_retries:
                # Move to permanent dead letter storage
                await self._archive_dead_letter(existing)
                return
        else:
            # Create new dead letter
            existing = DeadLetterMessage(
                original_message=message,
                error_count=1,
                last_error=str(error),
                first_failure=datetime.utcnow(),
                last_failure=datetime.utcnow(),
                processing_agent=processing_agent
            )
        
        # Determine retry strategy
        strategy = self._select_retry_strategy(existing)
        
        if strategy:
            # Schedule retry
            delay = strategy.calculate_delay(existing.error_count)
            await self._schedule_retry(existing, delay)
        else:
            # No retry strategy, archive immediately
            await self._archive_dead_letter(existing)
    
    async def process_dead_letters(self) -> None:
        """Process messages in the dead letter queue"""
        
        while True:
            try:
                # Get next dead letter
                dead_letter = await self.dlq.dequeue(timeout=5000)
                
                if not dead_letter:
                    continue
                
                # Try to recover
                recovery_handler = self._select_recovery_handler(dead_letter)
                
                if recovery_handler:
                    success = await recovery_handler.attempt_recovery(dead_letter)
                    
                    if success:
                        # Successfully recovered
                        await self._notify_recovery(dead_letter)
                        continue
                
                # Recovery failed or no handler, requeue for retry
                await self.handle_failed_message(
                    dead_letter.original_message,
                    Exception(dead_letter.last_error),
                    dead_letter.processing_agent
                )
                
            except Exception as e:
                print(f"Error processing dead letter: {e}")
                await asyncio.sleep(1)
```

### Task 3.8: ì´ë²¤íŠ¸ ë²„ìŠ¤ êµ¬í˜„

#### SubTask 3.8.1: ì´ë²¤íŠ¸ ë²„ìŠ¤ ì•„í‚¤í…ì²˜
**ë‹´ë‹¹ì**: ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/events/event_bus.py
from typing import Dict, List, Callable, Any, Optional, Set
import asyncio
from dataclasses import dataclass
from enum import Enum
import re

class EventType(Enum):
    AGENT_STARTED = "agent.started"
    AGENT_STOPPED = "agent.stopped"
    AGENT_ERROR = "agent.error"
    TASK_CREATED = "task.created"
    TASK_COMPLETED = "task.completed"
    WORKFLOW_STARTED = "workflow.started"
    WORKFLOW_COMPLETED = "workflow.completed"
    CUSTOM = "custom"

@dataclass
class Event:
    event_type: EventType
    event_name: str
    source_agent_id: str
    payload: Dict[str, Any]
    timestamp: datetime
    correlation_id: Optional[str] = None
    metadata: Dict[str, Any] = None

class EventBus:
    """Central event bus for agent system"""
    
    def __init__(self):
        self.subscribers: Dict[str, List[EventSubscriber]] = {}
        self.event_history: List[Event] = []
        self.event_filters: Dict[str, EventFilter] = {}
        self.middleware: List[EventMiddleware] = []
        
    async def publish(
        self,
        event_type: EventType,
        event_name: str,
        source_agent_id: str,
        payload: Dict[str, Any],
        correlation_id: Optional[str] = None
    ) -> None:
        """Publish an event to the bus"""
        
        # Create event
        event = Event(
            event_type=event_type,
            event_name=event_name,
            source_agent_id=source_agent_id,
            payload=payload,
            timestamp=datetime.utcnow(),
            correlation_id=correlation_id,
            metadata={}
        )
        
        # Apply middleware
        for middleware in self.middleware:
            event = await middleware.process(event)
            if event is None:
                return  # Event filtered out
        
        # Store in history
        self.event_history.append(event)
        
        # Find matching subscribers
        subscribers = self._find_subscribers(event)
        
        # Notify subscribers asynchronously
        tasks = []
        for subscriber in subscribers:
            task = asyncio.create_task(
                self._notify_subscriber(subscriber, event)
            )
            tasks.append(task)
        
        # Wait for all notifications to complete
        await asyncio.gather(*tasks, return_exceptions=True)
    
    def subscribe(
        self,
        pattern: str,
        handler: Callable[[Event], None],
        filter_fn: Optional[Callable[[Event], bool]] = None
    ) -> str:
        """Subscribe to events matching pattern"""
        
        subscriber = EventSubscriber(
            id=str(uuid.uuid4()),
            pattern=pattern,
            handler=handler,
            filter_fn=filter_fn
        )
        
        if pattern not in self.subscribers:
            self.subscribers[pattern] = []
        
        self.subscribers[pattern].append(subscriber)
        
        return subscriber.id
    
    def unsubscribe(self, subscription_id: str) -> None:
        """Unsubscribe from events"""
        
        for pattern, subscribers in self.subscribers.items():
            self.subscribers[pattern] = [
                s for s in subscribers if s.id != subscription_id
            ]
```

#### SubTask 3.8.2: ì´ë²¤íŠ¸ ë¼ìš°íŒ… ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/events/event_router.ts
export class EventRouter {
  private readonly routes: Map<string, RouteHandler[]> = new Map();
  private readonly wildcardRoutes: RouteHandler[] = [];
  private readonly topicRoutes: Map<string, TopicRoute> = new Map();
  
  constructor(private readonly eventBus: EventBus) {}
  
  addRoute(
    pattern: string,
    handler: RouteHandler,
    options: RouteOptions = {}
  ): void {
    // Parse pattern
    const route = this.parsePattern(pattern);
    
    // Create route handler
    const routeHandler: RouteHandler = {
      pattern,
      regex: route.regex,
      handler,
      priority: options.priority || 0,
      filter: options.filter,
      transform: options.transform
    };
    
    // Add to appropriate collection
    if (pattern === '*') {
      this.wildcardRoutes.push(routeHandler);
    } else if (pattern.includes('*') || pattern.includes('+')) {
      this.addTopicRoute(pattern, routeHandler);
    } else {
      if (!this.routes.has(pattern)) {
        this.routes.set(pattern, []);
      }
      this.routes.get(pattern)!.push(routeHandler);
    }
    
    // Sort by priority
    this.sortRoutes();
  }
  
  async routeEvent(event: Event): Promise<void> {
    const matchingRoutes = this.findMatchingRoutes(event);
    
    // Execute routes in priority order
    for (const route of matchingRoutes) {
      try {
        // Apply filter if present
        if (route.filter && !await route.filter(event)) {
          continue;
        }
        
        // Transform event if needed
        let processedEvent = event;
        if (route.transform) {
          processedEvent = await route.transform(event);
        }
        
        // Execute handler
        await route.handler(processedEvent);
        
      } catch (error) {
        console.error(`Route handler error: ${error}`);
        // Emit error event
        await this.eventBus.publish({
          event_type: EventType.ROUTING_ERROR,
          event_name: 'event.routing.error',
          payload: {
            original_event: event,
            route_pattern: route.pattern,
            error: error.message
          }
        });
      }
    }
  }
  
  private findMatchingRoutes(event: Event): RouteHandler[] {
    const routes: RouteHandler[] = [];
    const eventKey = `${event.event_type}.${event.event_name}`;
    
    // Exact matches
    if (this.routes.has(eventKey)) {
      routes.push(...this.routes.get(eventKey)!);
    }
    
    // Topic matches
    for (const [pattern, topicRoute] of this.topicRoutes) {
      if (topicRoute.matches(eventKey)) {
        routes.push(...topicRoute.handlers);
      }
    }
    
    // Wildcard routes
    routes.push(...this.wildcardRoutes);
    
    // Sort by priority
    return routes.sort((a, b) => b.priority - a.priority);
  }
}
```

#### SubTask 3.8.3: ì´ë²¤íŠ¸ ì˜ì†ì„± ë° ì¬ìƒ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/events/event_persistence.py
from typing import List, Optional, Dict, Any
import asyncio
from datetime import datetime, timedelta

class EventStore:
    """Persists and retrieves events"""
    
    def __init__(self, storage_backend: StorageBackend):
        self.storage = storage_backend
        self.event_streams: Dict[str, EventStream] = {}
        
    async def append_event(self, event: Event) -> None:
        """Append event to store"""
        
        # Serialize event
        serialized = self._serialize_event(event)
        
        # Store in backend
        await self.storage.store(
            key=f"event:{event.timestamp.timestamp()}:{event.event_name}",
            value=serialized,
            ttl=30 * 24 * 3600  # 30 days
        )
        
        # Update event stream
        stream_key = event.correlation_id or "global"
        if stream_key not in self.event_streams:
            self.event_streams[stream_key] = EventStream(stream_key)
        
        self.event_streams[stream_key].append(event)
    
    async def get_events(
        self,
        start_time: datetime,
        end_time: datetime,
        event_types: Optional[List[EventType]] = None,
        source_agents: Optional[List[str]] = None
    ) -> List[Event]:
        """Retrieve events within time range"""
        
        # Query storage backend
        keys = await self.storage.list_keys(
            prefix="event:",
            start_key=f"event:{start_time.timestamp()}",
            end_key=f"event:{end_time.timestamp()}"
        )
        
        events = []
        for key in keys:
            serialized = await self.storage.get(key)
            if serialized:
                event = self._deserialize_event(serialized)
                
                # Apply filters
                if event_types and event.event_type not in event_types:
                    continue
                if source_agents and event.source_agent_id not in source_agents:
                    continue
                
                events.append(event)
        
        return sorted(events, key=lambda e: e.timestamp)

class EventReplayer:
    """Replays historical events"""
    
    def __init__(self, event_store: EventStore, event_bus: EventBus):
        self.event_store = event_store
        self.event_bus = event_bus
        
    async def replay_events(
        self,
        start_time: datetime,
        end_time: datetime,
        speed_multiplier: float = 1.0,
        filter_fn: Optional[Callable[[Event], bool]] = None
    ) -> None:
        """Replay events from history"""
        
        # Get events
        events = await self.event_store.get_events(start_time, end_time)
        
        if not events:
            return
        
        # Apply filter
        if filter_fn:
            events = [e for e in events if filter_fn(e)]
        
        # Calculate time offsets
        base_time = events[0].timestamp
        current_base = datetime.utcnow()
        
        # Replay events
        for event in events:
            # Calculate delay
            time_offset = (event.timestamp - base_time).total_seconds()
            adjusted_delay = time_offset / speed_multiplier
            
            # Wait for appropriate time
            if adjusted_delay > 0:
                await asyncio.sleep(adjusted_delay)
            
            # Republish event
            await self.event_bus.publish(
                event_type=event.event_type,
                event_name=f"replay.{event.event_name}",
                source_agent_id=event.source_agent_id,
                payload={
                    **event.payload,
                    "_replay": True,
                    "_original_timestamp": event.timestamp.isoformat()
                },
                correlation_id=event.correlation_id
            )
```

#### SubTask 3.8.4: ì´ë²¤íŠ¸ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/events/event_workflow.py
from typing import Dict, List, Optional, Any, Callable
import asyncio
from dataclasses import dataclass
from enum import Enum

class WorkflowState(Enum):
    PENDING = "pending"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class WorkflowStep:
    id: str
    name: str
    trigger_event: str
    action: Callable
    success_event: str
    failure_event: str
    timeout: Optional[int] = None
    retry_count: int = 0
    conditions: List[Callable] = None

class EventDrivenWorkflow:
    """Workflow driven by events"""
    
    def __init__(self, workflow_id: str, event_bus: EventBus):
        self.workflow_id = workflow_id
        self.event_bus = event_bus
        self.steps: Dict[str, WorkflowStep] = {}
        self.state = WorkflowState.PENDING
        self.current_step: Optional[str] = None
        self.context: Dict[str, Any] = {}
        self.step_results: Dict[str, Any] = {}
        
    def add_step(self, step: WorkflowStep) -> None:
        """Add a step to the workflow"""
        self.steps[step.id] = step
        
        # Subscribe to trigger event
        self.event_bus.subscribe(
            step.trigger_event,
            lambda event: asyncio.create_task(
                self._handle_step_trigger(step, event)
            )
        )
    
    async def start(self, initial_context: Dict[str, Any] = None) -> None:
        """Start the workflow"""
        
        self.state = WorkflowState.RUNNING
        self.context = initial_context or {}
        
        # Publish workflow started event
        await self.event_bus.publish(
            event_type=EventType.WORKFLOW_STARTED,
            event_name="workflow.started",
            source_agent_id=self.workflow_id,
            payload={
                "workflow_id": self.workflow_id,
                "context": self.context
            }
        )
        
        # Trigger first step if defined
        if self.steps:
            first_step = next(iter(self.steps.values()))
            await self._execute_step(first_step, {})
    
    async def _handle_step_trigger(
        self,
        step: WorkflowStep,
        event: Event
    ) -> None:
        """Handle step trigger event"""
        
        if self.state != WorkflowState.RUNNING:
            return
        
        # Check conditions
        if step.conditions:
            for condition in step.conditions:
                if not await condition(event, self.context):
                    return
        
        # Execute step
        await self._execute_step(step, event.payload)
    
    async def _execute_step(
        self,
        step: WorkflowStep,
        input_data: Dict[str, Any]
    ) -> None:
        """Execute a workflow step"""
        
        self.current_step = step.id
        attempt = 0
        
        while attempt <= step.retry_count:
            try:
                # Execute with timeout
                if step.timeout:
                    result = await asyncio.wait_for(
                        step.action(input_data, self.context),
                        timeout=step.timeout / 1000
                    )
                else:
                    result = await step.action(input_data, self.context)
                
                # Store result
                self.step_results[step.id] = result
                
                # Update context
                if isinstance(result, dict):
                    self.context.update(result)
                
                # Publish success event
                await self.event_bus.publish(
                    event_type=EventType.CUSTOM,
                    event_name=step.success_event,
                    source_agent_id=self.workflow_id,
                    payload={
                        "step_id": step.id,
                        "result": result,
                        "context": self.context
                    },
                    correlation_id=self.workflow_id
                )
                
                break
                
            except Exception as e:
                attempt += 1
                
                if attempt > step.retry_count:
                    # Publish failure event
                    await self.event_bus.publish(
                        event_type=EventType.CUSTOM,
                        event_name=step.failure_event,
                        source_agent_id=self.workflow_id,
                        payload={
                            "step_id": step.id,
                            "error": str(e),
                            "attempts": attempt
                        },
                        correlation_id=self.workflow_id
                    )
                    
                    # Fail workflow
                    self.state = WorkflowState.FAILED
                    break
                
                # Wait before retry
                await asyncio.sleep(2 ** attempt)
```

### Task 3.9: ë™ê¸°/ë¹„ë™ê¸° í†µì‹  ë ˆì´ì–´

#### SubTask 3.9.1: ë™ê¸° í†µì‹  êµ¬í˜„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/communication/sync_communication.py
from typing import Dict, Any, Optional, TypeVar, Generic
import asyncio
from abc import ABC, abstractmethod
import aiohttp
import grpc

T = TypeVar('T')
R = TypeVar('R')

class SyncCommunicationClient(ABC, Generic[T, R]):
    """Base class for synchronous communication"""
    
    @abstractmethod
    async def call(
        self,
        target: str,
        method: str,
        data: T,
        timeout: int = 30000
    ) -> R:
        pass

class HTTPCommunicationClient(SyncCommunicationClient[Dict, Dict]):
    """HTTP-based synchronous communication"""
    
    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None
        
    async def initialize(self) -> None:
        """Initialize HTTP client"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(limit=100)
        )
    
    async def call(
        self,
        target: str,
        method: str,
        data: Dict[str, Any],
        timeout: int = 30000
    ) -> Dict[str, Any]:
        """Make synchronous HTTP call"""
        
        if not self.session:
            await self.initialize()
        
        url = f"{target}/{method}"
        
        try:
            async with self.session.post(
                url,
                json=data,
                timeout=aiohttp.ClientTimeout(total=timeout/1000)
            ) as response:
                response.raise_for_status()
                return await response.json()
                
        except aiohttp.ClientError as e:
            raise CommunicationError(f"HTTP call failed: {e}")
        except asyncio.TimeoutError:
            raise TimeoutError(f"Call to {url} timed out after {timeout}ms")

class GRPCCommunicationClient(SyncCommunicationClient):
    """gRPC-based synchronous communication"""
    
    def __init__(self):
        self.channels: Dict[str, grpc.aio.Channel] = {}
        self.stubs: Dict[str, Any] = {}
        
    async def call(
        self,
        target: str,
        method: str,
        data: Any,
        timeout: int = 30000
    ) -> Any:
        """Make synchronous gRPC call"""
        
        # Get or create channel
        if target not in self.channels:
            self.channels[target] = grpc.aio.insecure_channel(
                target,
                options=[
                    ('grpc.max_receive_message_length', 100 * 1024 * 1024),
                    ('grpc.max_send_message_length', 100 * 1024 * 1024),
                ]
            )
        
        # Get stub
        stub = self._get_stub(target, method)
        
        # Make call
        try:
            response = await stub.__call__(
                data,
                timeout=timeout/1000
            )
            return response
        except grpc.RpcError as e:
            raise CommunicationError(f"gRPC call failed: {e.details()}")
```

#### SubTask 3.9.2: ë¹„ë™ê¸° í†µì‹  êµ¬í˜„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/communication/async_communication.ts
export class AsyncCommunicationClient {
  private readonly messageQueue: QueueInterface<AsyncMessage>;
  private readonly responseHandlers: Map<string, ResponseHandler> = new Map();
  private readonly websocketClients: Map<string, WebSocket> = new Map();
  
  constructor(
    private readonly agentId: string,
    private readonly queueConfig: QueueConfig
  ) {
    this.messageQueue = this.createQueue(queueConfig);
    this.startResponseProcessor();
  }
  
  async sendAsync(
    target: string,
    action: string,
    data: any,
    options: AsyncOptions = {}
  ): Promise<string> {
    // Create async message
    const message = new AsyncMessage({
      messageId: uuid(),
      senderId: this.agentId,
      targetId: target,
      action,
      data,
      replyTo: options.replyTo || this.agentId,
      correlation: options.correlationId,
      priority: options.priority || MessagePriority.NORMAL,
      ttl: options.ttl || 300000 // 5 minutes
    });
    
    // Register callback if provided
    if (options.onResponse) {
      this.responseHandlers.set(message.messageId, {
        callback: options.onResponse,
        timeout: options.responseTimeout || 60000,
        timeoutCallback: options.onTimeout
      });
      
      // Set timeout
      setTimeout(() => {
        this.handleTimeout(message.messageId);
      }, options.responseTimeout || 60000);
    }
    
    // Send message
    await this.messageQueue.enqueue(message);
    
    // Track message
    await this.trackMessage(message);
    
    return message.messageId;
  }
  
  async broadcast(
    action: string,
    data: any,
    options: BroadcastOptions = {}
  ): Promise<void> {
    // Create broadcast message
    const message = new BroadcastMessage({
      messageId: uuid(),
      senderId: this.agentId,
      action,
      data,
      topic: options.topic || action,
      excludeSelf: options.excludeSelf || false
    });
    
    // Send via appropriate channel
    if (options.channel === 'websocket') {
      await this.broadcastWebSocket(message);
    } else {
      await this.broadcastQueue(message);
    }
  }
  
  private async broadcastWebSocket(message: BroadcastMessage): Promise<void> {
    const payload = JSON.stringify(message);
    
    // Send to all connected websocket clients
    for (const [clientId, ws] of this.websocketClients) {
      if (message.excludeSelf && clientId === this.agentId) {
        continue;
      }
      
      if (ws.readyState === WebSocket.OPEN) {
        ws.send(payload);
      }
    }
  }
  
  subscribe(
    topic: string,
    handler: MessageHandler,
    options: SubscriptionOptions = {}
  ): Subscription {
    // Create subscription
    const subscription = new Subscription({
      id: uuid(),
      topic,
      handler,
      filter: options.filter,
      transform: options.transform
    });
    
    // Register with appropriate channel
    if (options.channel === 'websocket') {
      this.subscribeWebSocket(subscription);
    } else {
      this.subscribeQueue(subscription);
    }
    
    return subscription;
  }
}
```

#### SubTask 3.9.3: í†µì‹  ì¶”ìƒí™” ë ˆì´ì–´
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/communication/communication_layer.py
from typing import Dict, Any, Optional, Union, List
from abc import ABC, abstractmethod
import asyncio

class CommunicationLayer:
    """Unified communication layer for agents"""
    
    def __init__(self):
        self.sync_clients: Dict[str, SyncCommunicationClient] = {
            'http': HTTPCommunicationClient(),
            'grpc': GRPCCommunicationClient()
        }
        self.async_client = AsyncCommunicationClient()
        self.event_bus = EventBus()
        self.message_router = MessageRouter()
        
    async def request(
        self,
        target: str,
        action: str,
        data: Any,
        options: RequestOptions = None
    ) -> Any:
        """Make a request (sync or async based on options)"""
        
        options = options or RequestOptions()
        
        if options.async_mode:
            # Async request
            return await self._async_request(target, action, data, options)
        else:
            # Sync request
            return await self._sync_request(target, action, data, options)
    
    async def _sync_request(
        self,
        target: str,
        action: str,
        data: Any,
        options: RequestOptions
    ) -> Any:
        """Make synchronous request"""
        
        # Select protocol
        protocol = options.protocol or 'http'
        client = self.sync_clients.get(protocol)
        
        if not client:
            raise ValueError(f"Unknown protocol: {protocol}")
        
        # Add retry logic
        last_error = None
        for attempt in range(options.retry_count + 1):
            try:
                result = await client.call(
                    target=target,
                    method=action,
                    data=data,
                    timeout=options.timeout
                )
                
                # Log success
                await self._log_communication(
                    'sync_request',
                    target,
                    action,
                    success=True,
                    attempt=attempt
                )
                
                return result
                
            except Exception as e:
                last_error = e
                
                if attempt < options.retry_count:
                    # Calculate backoff
                    delay = options.retry_delay * (2 ** attempt)
                    await asyncio.sleep(delay / 1000)
                
        # All retries failed
        await self._log_communication(
            'sync_request',
            target,
            action,
            success=False,
            error=str(last_error)
        )
        
        raise last_error
    
    async def _async_request(
        self,
        target: str,
        action: str,
        data: Any,
        options: RequestOptions
    ) -> str:
        """Make asynchronous request"""
        
        # Create future for response
        future = asyncio.Future()
        
        # Send async message
        message_id = await self.async_client.sendAsync(
            target=target,
            action=action,
            data=data,
            options={
                'priority': options.priority,
                'ttl': options.ttl,
                'onResponse': lambda resp: future.set_result(resp),
                'onTimeout': lambda: future.set_exception(
                    TimeoutError(f"Async request timeout")
                )
            }
        )
        
        # Return message ID if fire-and-forget
        if options.fire_and_forget:
            return message_id
        
        # Wait for response
        try:
            response = await asyncio.wait_for(
                future,
                timeout=options.timeout / 1000
            )
            return response
        except asyncio.TimeoutError:
            raise TimeoutError(f"Async request timeout after {options.timeout}ms")
```

#### SubTask 3.9.4: í†µì‹  ëª¨ë‹ˆí„°ë§ ë° ì¶”ì 
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/communication/communication_monitor.py
from typing import Dict, Any, List, Optional
import asyncio
from dataclasses import dataclass
from datetime import datetime, timedelta

@dataclass
class CommunicationMetrics:
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    average_latency: float = 0.0
    p95_latency: float = 0.0
    p99_latency: float = 0.0
    requests_per_second: float = 0.0
    error_rate: float = 0.0
    
class CommunicationMonitor:
    """Monitors communication between agents"""
    
    def __init__(self):
        self.metrics: Dict[str, CommunicationMetrics] = {}
        self.latency_samples: Dict[str, List[float]] = {}
        self.trace_store = TraceStore()
        self.alert_manager = AlertManager()
        
    async def record_communication(
        self,
        source_agent: str,
        target_agent: str,
        action: str,
        success: bool,
        latency: float,
        metadata: Dict[str, Any] = None
    ) -> None:
        """Record a communication event"""
        
        # Create trace
        trace = CommunicationTrace(
            trace_id=str(uuid.uuid4()),
            source_agent=source_agent,
            target_agent=target_agent,
            action=action,
            success=success,
            latency=latency,
            timestamp=datetime.utcnow(),
            metadata=metadata or {}
        )
        
        # Store trace
        await self.trace_store.store(trace)
        
        # Update metrics
        key = f"{source_agent}->{target_agent}"
        if key not in self.metrics:
            self.metrics[key] = CommunicationMetrics()
            self.latency_samples[key] = []
        
        metrics = self.metrics[key]
        metrics.total_requests += 1
        
        if success:
            metrics.successful_requests += 1
            self.latency_samples[key].append(latency)
            
            # Keep only recent samples
            if len(self.latency_samples[key]) > 1000:
                self.latency_samples[key] = self.latency_samples[key][-1000:]
        else:
            metrics.failed_requests += 1
        
        # Update calculated metrics
        self._update_calculated_metrics(key)
        
        # Check for alerts
        await self._check_alerts(key, metrics)
    
    def _update_calculated_metrics(self, key: str) -> None:
        """Update calculated metrics"""
        
        metrics = self.metrics[key]
        samples = self.latency_samples[key]
        
        if samples:
            # Calculate latency percentiles
            sorted_samples = sorted(samples)
            metrics.average_latency = sum(samples) / len(samples)
            metrics.p95_latency = sorted_samples[int(len(samples) * 0.95)]
            metrics.p99_latency = sorted_samples[int(len(samples) * 0.99)]
        
        # Calculate error rate
        if metrics.total_requests > 0:
            metrics.error_rate = metrics.failed_requests / metrics.total_requests
    
    async def get_communication_graph(
        self,
        time_window: timedelta = timedelta(hours=1)
    ) -> Dict[str, Any]:
        """Get communication graph for visualization"""
        
        # Get recent traces
        end_time = datetime.utcnow()
        start_time = end_time - time_window
        
        traces = await self.trace_store.query(
            start_time=start_time,
            end_time=end_time
        )
        
        # Build graph
        nodes = set()
        edges = []
        
        for trace in traces:
            nodes.add(trace.source_agent)
            nodes.add(trace.target_agent)
            
            edges.append({
                'source': trace.source_agent,
                'target': trace.target_agent,
                'weight': 1,
                'latency': trace.latency,
                'success': trace.success
            })
        
        # Aggregate edges
        edge_map = {}
        for edge in edges:
            key = f"{edge['source']}->{edge['target']}"
            
            if key not in edge_map:
                edge_map[key] = {
                    'source': edge['source'],
                    'target': edge['target'],
                    'count': 0,
                    'total_latency': 0,
                    'success_count': 0
                }
            
            edge_map[key]['count'] += 1
            edge_map[key]['total_latency'] += edge['latency']
            if edge['success']:
                edge_map[key]['success_count'] += 1
        
        # Calculate averages
        aggregated_edges = []
        for edge in edge_map.values():
            aggregated_edges.append({
                'source': edge['source'],
                'target': edge['target'],
                'weight': edge['count'],
                'average_latency': edge['total_latency'] / edge['count'],
                'success_rate': edge['success_count'] / edge['count']
            })
        
        return {
            'nodes': list(nodes),
            'edges': aggregated_edges,
            'time_window': time_window.total_seconds(),
            'generated_at': datetime.utcnow().isoformat()
        }
```

### Task 3.10: ì—ì´ì „íŠ¸ ê°„ ë°ì´í„° ê³µìœ  ì‹œìŠ¤í…œ

#### SubTask 3.10.1: ê³µìœ  ë©”ëª¨ë¦¬ êµ¬í˜„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/data_sharing/shared_memory.py
from typing import Dict, Any, Optional, List, Set
import asyncio
from dataclasses import dataclass
import mmap
import pickle

@dataclass
class SharedMemorySegment:
    segment_id: str
    size: int
    owner_agent: str
    permissions: Dict[str, Set[str]]  # agent_id -> {read, write}
    created_at: datetime
    last_modified: datetime

class SharedMemoryManager:
    """Manages shared memory between agents"""
    
    def __init__(self):
        self.segments: Dict[str, SharedMemorySegment] = {}
        self.memory_maps: Dict[str, mmap.mmap] = {}
        self.locks: Dict[str, asyncio.Lock] = {}
        self.access_log = AccessLog()
        
    async def create_segment(
        self,
        segment_id: str,
        size: int,
        owner_agent: str,
        initial_data: Optional[Any] = None
    ) -> SharedMemorySegment:
        """Create a new shared memory segment"""
        
        if segment_id in self.segments:
            raise ValueError(f"Segment {segment_id} already exists")
        
        # Create memory segment
        segment = SharedMemorySegment(
            segment_id=segment_id,
            size=size,
            owner_agent=owner_agent,
            permissions={owner_agent: {'read', 'write'}},
            created_at=datetime.utcnow(),
            last_modified=datetime.utcnow()
        )
        
        # Create memory map
        self.memory_maps[segment_id] = mmap.mmap(-1, size)
        
        # Initialize with data if provided
        if initial_data:
            await self.write_data(segment_id, owner_agent, initial_data)
        
        # Create lock for this segment
        self.locks[segment_id] = asyncio.Lock()
        
        self.segments[segment_id] = segment
        
        # Log creation
        await self.access_log.log_action(
            'create',
            segment_id,
            owner_agent,
            metadata={'size': size}
        )
        
        return segment
    
    async def read_data(
        self,
        segment_id: str,
        agent_id: str,
        offset: int = 0,
        length: Optional[int] = None
    ) -> Any:
        """Read data from shared memory"""
        
        # Check permissions
        if not self._has_permission(segment_id, agent_id, 'read'):
            raise PermissionError(
                f"Agent {agent_id} does not have read permission for segment {segment_id}"
            )
        
        # Acquire lock
        async with self.locks[segment_id]:
            memory_map = self.memory_maps[segment_id]
            memory_map.seek(offset)
            
            if length:
                data_bytes = memory_map.read(length)
            else:
                data_bytes = memory_map.read()
            
            # Deserialize
            data = pickle.loads(data_bytes)
            
            # Log access
            await self.access_log.log_action(
                'read',
                segment_id,
                agent_id,
                metadata={'offset': offset, 'length': length}
            )
            
            return data
    
    async def write_data(
        self,
        segment_id: str,
        agent_id: str,
        data: Any,
        offset: int = 0
    ) -> None:
        """Write data to shared memory"""
        
        # Check permissions
        if not self._has_permission(segment_id, agent_id, 'write'):
            raise PermissionError(
                f"Agent {agent_id} does not have write permission for segment {segment_id}"
            )
        
        # Serialize data
        data_bytes = pickle.dumps(data)
        
        if len(data_bytes) + offset > self.segments[segment_id].size:
            raise ValueError("Data too large for segment")
        
        # Acquire lock
        async with self.locks[segment_id]:
            memory_map = self.memory_maps[segment_id]
            memory_map.seek(offset)
            memory_map.write(data_bytes)
            
            # Update metadata
            self.segments[segment_id].last_modified = datetime.utcnow()
            
            # Log access
            await self.access_log.log_action(
                'write',
                segment_id,
                agent_id,
                metadata={'offset': offset, 'data_size': len(data_bytes)}
            )
```

#### SubTask 3.10.2: ë¶„ì‚° ìºì‹œ ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/data_sharing/distributed_cache.ts
export class DistributedCache {
  private readonly localCache: LRUCache<string, CacheEntry>;
  private readonly redisClient: Redis;
  private readonly invalidationBus: EventBus;
  private readonly cacheMetrics: CacheMetrics;
  
  constructor(options: DistributedCacheOptions) {
    this.localCache = new LRUCache({
      max: options.localCacheSize || 1000,
      ttl: options.localCacheTTL || 60000,
      updateAgeOnGet: true
    });
    
    this.redisClient = new Redis(options.redisUrl);
    this.invalidationBus = new EventBus('cache-invalidation');
    this.cacheMetrics = new CacheMetrics();
    
    // Subscribe to invalidation events
    this.setupInvalidationListener();
  }
  
  async get<T>(
    key: string,
    options: CacheGetOptions = {}
  ): Promise<T | null> {
    const startTime = Date.now();
    
    // Check local cache first
    const localEntry = this.localCache.get(key);
    if (localEntry && !this.isExpired(localEntry)) {
      this.cacheMetrics.recordHit('local');
      return localEntry.value as T;
    }
    
    // Check distributed cache
    try {
      const redisValue = await this.redisClient.get(key);
      if (redisValue) {
        const entry = JSON.parse(redisValue) as CacheEntry;
        
        if (!this.isExpired(entry)) {
          // Update local cache
          this.localCache.set(key, entry);
          this.cacheMetrics.recordHit('distributed');
          return entry.value as T;
        }
      }
    } catch (error) {
      console.error(`Redis get error for key ${key}:`, error);
    }
    
    // Cache miss
    this.cacheMetrics.recordMiss();
    
    // Load data if loader provided
    if (options.loader) {
      const value = await options.loader();
      if (value !== null && value !== undefined) {
        await this.set(key, value, options.ttl);
        return value;
      }
    }
    
    return null;
  }
  
  async set<T>(
    key: string,
    value: T,
    ttl?: number
  ): Promise<void> {
    const entry: CacheEntry = {
      key,
      value,
      timestamp: Date.now(),
      ttl: ttl || this.defaultTTL,
      version: uuid()
    };
    
    // Set in local cache
    this.localCache.set(key, entry);
    
    // Set in distributed cache
    try {
      await this.redisClient.setex(
        key,
        Math.ceil(entry.ttl / 1000),
        JSON.stringify(entry)
      );
      
      // Broadcast invalidation to other instances
      await this.invalidationBus.publish({
        type: 'cache.updated',
        key,
        version: entry.version,
        source: this.instanceId
      });
    } catch (error) {
      console.error(`Redis set error for key ${key}:`, error);
    }
  }
  
  async invalidate(
    pattern: string,
    options: InvalidateOptions = {}
  ): Promise<number> {
    let invalidatedCount = 0;
    
    // Invalidate local cache
    if (options.local !== false) {
      for (const [key, entry] of this.localCache.entries()) {
        if (this.matchesPattern(key, pattern)) {
          this.localCache.delete(key);
          invalidatedCount++;
        }
      }
    }
    
    // Invalidate distributed cache
    if (options.distributed !== false) {
      try {
        const keys = await this.redisClient.keys(pattern);
        if (keys.length > 0) {
          await this.redisClient.del(...keys);
          invalidatedCount += keys.length;
        }
      } catch (error) {
        console.error(`Redis invalidation error for pattern ${pattern}:`, error);
      }
    }
    
    // Broadcast invalidation
    if (options.broadcast !== false) {
      await this.invalidationBus.publish({
        type: 'cache.invalidated',
        pattern,
        source: this.instanceId
      });
    }
    
    return invalidatedCount;
  }
  
  private setupInvalidationListener(): void {
    this.invalidationBus.subscribe('cache.*', async (event) => {
      // Ignore our own events
      if (event.source === this.instanceId) return;
      
      switch (event.type) {
        case 'cache.updated':
          // Remove from local cache to force refresh
          this.localCache.delete(event.key);
          break;
          
        case 'cache.invalidated':
          // Invalidate matching keys in local cache
          for (const key of this.localCache.keys()) {
            if (this.matchesPattern(key, event.pattern)) {
              this.localCache.delete(key);
            }
          }
          break;
      }
    });
  }
}
```

#### SubTask 3.10.3: ë°ì´í„° ë™ê¸°í™” í”„ë¡œí† ì½œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/data_sharing/data_sync_protocol.py
from typing import Dict, Any, List, Optional, Set
import asyncio
from dataclasses import dataclass
from enum import Enum

class SyncStrategy(Enum):
    EVENTUAL = "eventual"
    STRONG = "strong"
    CAUSAL = "causal"
    READ_YOUR_WRITES = "read_your_writes"

@dataclass
class SyncOperation:
    operation_id: str
    operation_type: str  # create, update, delete
    data_key: str
    data_value: Any
    version: int
    timestamp: datetime
    source_agent: str
    vector_clock: Dict[str, int]

class DataSyncProtocol:
    """Implements data synchronization between agents"""
    
    def __init__(self, agent_id: str, sync_strategy: SyncStrategy):
        self.agent_id = agent_id
        self.sync_strategy = sync_strategy
        self.vector_clock = VectorClock(agent_id)
        self.sync_log: List[SyncOperation] = []
        self.peers: Set[str] = set()
        self.conflict_resolver = ConflictResolver()
        
    async def write_data(
        self,
        key: str,
        value: Any,
        options: WriteOptions = None
    ) -> WriteResult:
        """Write data with synchronization"""
        
        options = options or WriteOptions()
        
        # Update vector clock
        self.vector_clock.increment()
        
        # Create sync operation
        operation = SyncOperation(
            operation_id=str(uuid.uuid4()),
            operation_type="update",
            data_key=key,
            data_value=value,
            version=self.vector_clock.get_version(self.agent_id),
            timestamp=datetime.utcnow(),
            source_agent=self.agent_id,
            vector_clock=self.vector_clock.to_dict()
        )
        
        # Apply locally
        await self._apply_operation(operation)
        
        # Sync based on strategy
        if self.sync_strategy == SyncStrategy.STRONG:
            # Wait for acknowledgment from all peers
            await self._sync_strong(operation, options.quorum)
        elif self.sync_strategy == SyncStrategy.EVENTUAL:
            # Fire and forget
            asyncio.create_task(self._sync_eventual(operation))
        elif self.sync_strategy == SyncStrategy.CAUSAL:
            # Ensure causal consistency
            await self._sync_causal(operation)
        
        return WriteResult(
            success=True,
            version=operation.version,
            timestamp=operation.timestamp
        )
    
    async def read_data(
        self,
        key: str,
        options: ReadOptions = None
    ) -> ReadResult:
        """Read data with consistency guarantees"""
        
        options = options or ReadOptions()
        
        if self.sync_strategy == SyncStrategy.STRONG:
            # Read from quorum
            return await self._read_quorum(key, options.quorum)
        elif self.sync_strategy == SyncStrategy.READ_YOUR_WRITES:
            # Ensure we see our own writes
            return await self._read_with_session_guarantee(key)
        else:
            # Read locally
            return await self._read_local(key)
    
    async def _sync_strong(
        self,
        operation: SyncOperation,
        quorum: int
    ) -> None:
        """Strong consistency sync"""
        
        # Send to all peers
        tasks = []
        for peer in self.peers:
            task = self._send_operation_to_peer(peer, operation)
            tasks.append(task)
        
        # Wait for quorum
        results = await asyncio.gather(*tasks, return_exceptions=True)
        successful = sum(1 for r in results if not isinstance(r, Exception))
        
        if successful < quorum:
            raise ConsistencyError(
                f"Failed to achieve quorum: {successful}/{quorum}"
            )
    
    async def handle_sync_request(
        self,
        operation: SyncOperation
    ) -> SyncResponse:
        """Handle incoming sync request from peer"""
        
        # Check for conflicts
        conflicts = await self._detect_conflicts(operation)
        
        if conflicts:
            # Resolve conflicts
            resolution = await self.conflict_resolver.resolve(
                operation,
                conflicts,
                self.vector_clock
            )
            
            if resolution.action == "reject":
                return SyncResponse(
                    accepted=False,
                    reason="Conflict detected",
                    conflict_info=resolution.info
                )
            elif resolution.action == "merge":
                operation = resolution.merged_operation
        
        # Update vector clock
        self.vector_clock.merge(operation.vector_clock)
        
        # Apply operation
        await self._apply_operation(operation)
        
        return SyncResponse(
            accepted=True,
            version=self.vector_clock.get_version(self.agent_id)
        )
```

#### SubTask 3.10.4: ë°ì´í„° ì ‘ê·¼ ì œì–´
**ë‹´ë‹¹ì**: ë³´ì•ˆ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/data_sharing/access_control.py
from typing import Dict, Set, List, Optional
from dataclasses import dataclass
from enum import Enum

class Permission(Enum):
    READ = "read"
    WRITE = "write"
    DELETE = "delete"
    SHARE = "share"
    ADMIN = "admin"

@dataclass
class AccessPolicy:
    policy_id: str
    resource_pattern: str
    principal_pattern: str
    permissions: Set[Permission]
    conditions: List[AccessCondition]
    effect: str  # allow or deny

class DataAccessControl:
    """Controls access to shared data between agents"""
    
    def __init__(self):
        self.policies: List[AccessPolicy] = []
        self.role_assignments: Dict[str, Set[str]] = {}  # agent_id -> roles
        self.resource_ownership: Dict[str, str] = {}  # resource -> owner
        self.audit_logger = AuditLogger()
        
    def add_policy(self, policy: AccessPolicy) -> None:
        """Add an access control policy"""
        self.policies.append(policy)
        self.policies.sort(key=lambda p: p.effect == "deny", reverse=True)
    
    async def check_permission(
        self,
        agent_id: str,
        resource: str,
        permission: Permission,
        context: Dict[str, Any] = None
    ) -> bool:
        """Check if agent has permission for resource"""
        
        # Log access attempt
        await self.audit_logger.log_access_attempt(
            agent_id,
            resource,
            permission,
            context
        )
        
        # Check ownership
        if self._is_owner(agent_id, resource):
            return True
        
        # Get agent roles
        agent_roles = self.role_assignments.get(agent_id, set())
        
        # Evaluate policies
        for policy in self.policies:
            if self._matches_policy(
                policy,
                agent_id,
                agent_roles,
                resource,
                permission,
                context
            ):
                decision = policy.effect == "allow"
                
                # Log decision
                await self.audit_logger.log_access_decision(
                    agent_id,
                    resource,
                    permission,
                    decision,
                    policy.policy_id
                )
                
                return decision
        
        # Default deny
        await self.audit_logger.log_access_decision(
            agent_id,
            resource,
            permission,
            False,
            "default_deny"
        )
        
        return False
    
    async def grant_permission(
        self,
        grantor_id: str,
        grantee_id: str,
        resource: str,
        permissions: Set[Permission]
    ) -> None:
        """Grant permissions to another agent"""
        
        # Check if grantor has share permission
        if not await self.check_permission(
            grantor_id,
            resource,
            Permission.SHARE
        ):
            raise PermissionError(
                f"Agent {grantor_id} cannot share resource {resource}"
            )
        
        # Create grant policy
        policy = AccessPolicy(
            policy_id=f"grant-{uuid.uuid4()}",
            resource_pattern=resource,
            principal_pattern=grantee_id,
            permissions=permissions,
            conditions=[],
            effect="allow"
        )
        
        self.add_policy(policy)
        
        # Log grant
        await self.audit_logger.log_permission_grant(
            grantor_id,
            grantee_id,
            resource,
            permissions
        )
    
    def _matches_policy(
        self,
        policy: AccessPolicy,
        agent_id: str,
        agent_roles: Set[str],
        resource: str,
        permission: Permission,
        context: Dict[str, Any]
    ) -> bool:
        """Check if policy matches request"""
        
        # Check resource pattern
        if not self._matches_pattern(resource, policy.resource_pattern):
            return False
        
        # Check principal pattern
        principals = {agent_id} | agent_roles
        if not any(
            self._matches_pattern(p, policy.principal_pattern)
            for p in principals
        ):
            return False
        
        # Check permission
        if permission not in policy.permissions:
            return False
        
        # Check conditions
        for condition in policy.conditions:
            if not condition.evaluate(context):
                return False
        
        return True
```

ì´ì œ Task 3.11ë¶€í„° Task 3.15ê¹Œì§€ ì—ì´ì „íŠ¸ í˜‘ì—… ë° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì— ëŒ€í•œ ì‘ì—…ì§€ì‹œì„œë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

### Task 3.11: ì›Œí¬í”Œë¡œìš° ì—”ì§„ êµ¬ì¶•

#### SubTask 3.11.1: ì›Œí¬í”Œë¡œìš° ì •ì˜ ì–¸ì–´
**ë‹´ë‹¹ì**: ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/workflow/workflow_definition.py
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
from enum import Enum
import yaml

class WorkflowNodeType(Enum):
    START = "start"
    END = "end"
    TASK = "task"
    DECISION = "decision"
    PARALLEL = "parallel"
    LOOP = "loop"
    SUBWORKFLOW = "subworkflow"

@dataclass
class WorkflowNode:
    id: str
    name: str
    type: WorkflowNodeType
    agent_type: Optional[str] = None
    action: Optional[str] = None
    inputs: Dict[str, Any] = None
    outputs: Dict[str, str] = None
    conditions: List[Dict[str, Any]] = None
    next_nodes: List[str] = None
    retry_policy: Optional[Dict[str, Any]] = None
    timeout: Optional[int] = None

@dataclass
class WorkflowDefinition:
    id: str
    name: str
    version: str
    description: str
    nodes: Dict[str, WorkflowNode]
    variables: Dict[str, Any]
    triggers: List[Dict[str, Any]]
    metadata: Dict[str, Any]

class WorkflowDefinitionParser:
    """Parses workflow definitions from various formats"""
    
    def parse_yaml(self, yaml_content: str) -> WorkflowDefinition:
        """Parse YAML workflow definition"""
        
        data = yaml.safe_load(yaml_content)
        
        # Parse nodes
        nodes = {}
        for node_data in data.get('nodes', []):
            node = WorkflowNode(
                id=node_data['id'],
                name=node_data['name'],
                type=WorkflowNodeType(node_data['type']),
                agent_type=node_data.get('agent'),
                action=node_data.get('action'),
                inputs=node_data.get('inputs', {}),
                outputs=node_data.get('outputs', {}),
                conditions=node_data.get('conditions', []),
                next_nodes=node_data.get('next', []),
                retry_policy=node_data.get('retry'),
                timeout=node_data.get('timeout')
            )
            nodes[node.id] = node
        
        # Create workflow definition
        return WorkflowDefinition(
            id=data['id'],
            name=data['name'],
            version=data['version'],
            description=data.get('description', ''),
            nodes=nodes,
            variables=data.get('variables', {}),
            triggers=data.get('triggers', []),
            metadata=data.get('metadata', {})
        )
    
    def parse_dsl(self, dsl_content: str) -> WorkflowDefinition:
        """Parse custom DSL workflow definition"""
        
        # Example DSL:
        # workflow "order-processing" v1.0:
        #   trigger: event("order.created")
        #   
        #   validate_order -> agent:validator action:"validate"
        #   process_payment -> agent:payment action:"charge" 
        #   ship_order -> agent:shipping action:"ship"
        #   
        #   validate_order => process_payment when: status == "valid"
        #   process_payment => ship_order when: payment == "success"
        
        lexer = WorkflowDSLLexer(dsl_content)
        parser = WorkflowDSLParser(lexer)
        return parser.parse()
```

#### SubTask 3.11.2: ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì—”ì§„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 18ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/workflow/workflow_engine.ts
export class WorkflowEngine {
  private readonly executionStore: WorkflowExecutionStore;
  private readonly agentRegistry: AgentRegistry;
  private readonly eventBus: EventBus;
  private readonly stateManager: WorkflowStateManager;
  
  constructor(
    private readonly engineId: string,
    private readonly config: WorkflowEngineConfig
  ) {
    this.executionStore = new WorkflowExecutionStore();
    this.agentRegistry = new AgentRegistry();
    this.eventBus = new EventBus();
    this.stateManager = new WorkflowStateManager();
  }
  
  async startWorkflow(
    definition: WorkflowDefinition,
    initialContext: WorkflowContext
  ): Promise<WorkflowExecution> {
    // Create execution instance
    const execution = new WorkflowExecution({
      id: uuid(),
      workflowId: definition.id,
      status: WorkflowStatus.RUNNING,
      startTime: new Date(),
      context: initialContext,
      currentNodes: [],
      completedNodes: new Set<string>(),
      nodeResults: {}
    });
    
    // Save execution
    await this.executionStore.save(execution);
    
    // Emit start event
    await this.eventBus.emit({
      type: 'workflow.started',
      workflowId: definition.id,
      executionId: execution.id,
      context: initialContext
    });
    
    // Start from START nodes
    const startNodes = this.findStartNodes(definition);
    for (const node of startNodes) {
      await this.executeNode(execution, definition, node);
    }
    
    return execution;
  }
  
  private async executeNode(
    execution: WorkflowExecution,
    definition: WorkflowDefinition,
    node: WorkflowNode
  ): Promise<void> {
    // Check if already executed
    if (execution.completedNodes.has(node.id)) {
      return;
    }
    
    // Update current nodes
    execution.currentNodes.push(node.id);
    await this.executionStore.save(execution);
    
    try {
      let result: any;
      
      switch (node.type) {
        case WorkflowNodeType.TASK:
          result = await this.executeTaskNode(execution, node);
          break;
          
        case WorkflowNodeType.DECISION:
          result = await this.executeDecisionNode(execution, node);
          break;
          
        case WorkflowNodeType.PARALLEL:
          result = await this.executeParallelNode(execution, definition, node);
          break;
          
        case WorkflowNodeType.LOOP:
          result = await this.executeLoopNode(execution, definition, node);
          break;
          
        case WorkflowNodeType.SUBWORKFLOW:
          result = await this.executeSubworkflowNode(execution, node);
          break;
          
        case WorkflowNodeType.END:
          await this.completeWorkflow(execution);
          return;
      }
      
      // Store result
      execution.nodeResults[node.id] = result;
      
      // Mark as completed
      execution.completedNodes.add(node.id);
      execution.currentNodes = execution.currentNodes.filter(
        id => id !== node.id
      );
      
      // Update context with outputs
      if (node.outputs) {
        this.updateContext(execution.context, node.outputs, result);
      }
      
      // Save state
      await this.executionStore.save(execution);
      
      // Emit node completed event
      await this.eventBus.emit({
        type: 'workflow.node.completed',
        workflowId: definition.id,
        executionId: execution.id,
        nodeId: node.id,
        result
      });
      
      // Execute next nodes
      const nextNodes = await this.determineNextNodes(
        execution,
        definition,
        node,
        result
      );
      
      for (const nextNode of nextNodes) {
        await this.executeNode(execution, definition, nextNode);
      }
      
    } catch (error) {
      await this.handleNodeError(execution, node, error);
    }
  }
  
  private async executeTaskNode(
    execution: WorkflowExecution,
    node: WorkflowNode
  ): Promise<any> {
    // Get agent
    const agent = await this.agentRegistry.getAgent(node.agent_type!);
    
    // Prepare inputs
    const inputs = this.resolveInputs(node.inputs, execution.context);
    
    // Execute with timeout
    const timeoutMs = node.timeout || this.config.defaultNodeTimeout;
    
    try {
      const result = await Promise.race([
        agent.execute(node.action!, inputs),
        this.timeout(timeoutMs)
      ]);
      
      return result;
    } catch (error) {
      if (node.retry_policy) {
        return await this.retryNode(node, agent, inputs, node.retry_policy);
      }
      throw error;
    }
  }
}
```

#### SubTask 3.11.3: ì›Œí¬í”Œë¡œìš° ìƒíƒœ ê´€ë¦¬
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/workflow/workflow_state.py
from typing import Dict, Any, List, Optional, Set
import asyncio
from dataclasses import dataclass
from datetime import datetime

@dataclass
class WorkflowState:
    execution_id: str
    workflow_id: str
    status: WorkflowStatus
    current_nodes: List[str]
    completed_nodes: Set[str]
    failed_nodes: Set[str]
    node_states: Dict[str, NodeState]
    context: Dict[str, Any]
    checkpoints: List[WorkflowCheckpoint]
    created_at: datetime
    updated_at: datetime

class WorkflowStateManager:
    """Manages workflow execution state"""
    
    def __init__(self, storage: StateStorage):
        self.storage = storage
        self.state_cache: Dict[str, WorkflowState] = {}
        self.checkpoint_interval = 5  # nodes
        
    async def create_state(
        self,
        execution_id: str,
        workflow_id: str,
        initial_context: Dict[str, Any]
    ) -> WorkflowState:
        """Create new workflow state"""
        
        state = WorkflowState(
            execution_id=execution_id,
            workflow_id=workflow_id,
            status=WorkflowStatus.INITIALIZING,
            current_nodes=[],
            completed_nodes=set(),
            failed_nodes=set(),
            node_states={},
            context=initial_context.copy(),
            checkpoints=[],
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
        
        await self.save_state(state)
        return state
    
    async def update_node_state(
        self,
        execution_id: str,
        node_id: str,
        status: NodeStatus,
        result: Optional[Any] = None,
        error: Optional[str] = None
    ) -> None:
        """Update state of a specific node"""
        
        state = await self.get_state(execution_id)
        
        # Update node state
        node_state = NodeState(
            node_id=node_id,
            status=status,
            start_time=datetime.utcnow(),
            end_time=None,
            result=result,
            error=error,
            retry_count=0
        )
        
        state.node_states[node_id] = node_state
        
        # Update node lists
        if status == NodeStatus.RUNNING:
            if node_id not in state.current_nodes:
                state.current_nodes.append(node_id)
        elif status == NodeStatus.COMPLETED:
            state.completed_nodes.add(node_id)
            state.current_nodes = [
                n for n in state.current_nodes if n != node_id
            ]
        elif status == NodeStatus.FAILED:
            state.failed_nodes.add(node_id)
            state.current_nodes = [
                n for n in state.current_nodes if n != node_id
            ]
        
        # Check if checkpoint needed
        if len(state.completed_nodes) % self.checkpoint_interval == 0:
            await self.create_checkpoint(state)
        
        await self.save_state(state)
    
    async def create_checkpoint(
        self,
        state: WorkflowState
    ) -> WorkflowCheckpoint:
        """Create a checkpoint of current state"""
        
        checkpoint = WorkflowCheckpoint(
            checkpoint_id=str(uuid.uuid4()),
            execution_id=state.execution_id,
            timestamp=datetime.utcnow(),
            completed_nodes=state.completed_nodes.copy(),
            context_snapshot=state.context.copy(),
            node_results={
                node_id: node_state.result
                for node_id, node_state in state.node_states.items()
                if node_state.status == NodeStatus.COMPLETED
            }
        )
        
        # Store checkpoint
        await self.storage.save_checkpoint(checkpoint)
        
        # Add to state
        state.checkpoints.append(checkpoint)
        
        return checkpoint
    
    async def restore_from_checkpoint(
        self,
        execution_id: str,
        checkpoint_id: str
    ) -> WorkflowState:
        """Restore workflow state from checkpoint"""
        
        checkpoint = await self.storage.get_checkpoint(checkpoint_id)
        
        if not checkpoint:
            raise ValueError(f"Checkpoint {checkpoint_id} not found")
        
        # Get original state
        state = await self.get_state(execution_id)
        
        # Restore from checkpoint
        state.completed_nodes = checkpoint.completed_nodes
        state.context = checkpoint.context_snapshot
        state.current_nodes = []
        state.failed_nodes = set()
        
        # Reset node states
        for node_id in state.node_states:
            if node_id in checkpoint.completed_nodes:
                state.node_states[node_id].status = NodeStatus.COMPLETED
                state.node_states[node_id].result = checkpoint.node_results.get(
                    node_id
                )
            else:
                # Mark as pending
                state.node_states[node_id].status = NodeStatus.PENDING
        
        state.status = WorkflowStatus.RESUMING
        
        await self.save_state(state)
        return state
```

#### SubTask 3.11.4: ì›Œí¬í”Œë¡œìš° ëª¨ë‹ˆí„°ë§
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/workflow/workflow_monitor.ts
export class WorkflowMonitor {
  private readonly metrics: WorkflowMetrics;
  private readonly alertManager: AlertManager;
  private readonly dashboardService: DashboardService;
  
  constructor() {
    this.metrics = new WorkflowMetrics();
    this.alertManager = new AlertManager();
    this.dashboardService = new DashboardService();
  }
  
  async monitorExecution(
    execution: WorkflowExecution
  ): Promise<MonitoringHandle> {
    const handle = new MonitoringHandle(execution.id);
    
    // Start monitoring tasks
    const tasks = [
      this.monitorProgress(execution, handle),
      this.monitorPerformance(execution, handle),
      this.monitorErrors(execution, handle),
      this.monitorResources(execution, handle)
    ];
    
    handle.tasks = tasks.map(t => 
      t.catch(error => 
        console.error(`Monitoring error: ${error}`)
      )
    );
    
    return handle;
  }
  
  private async monitorProgress(
    execution: WorkflowExecution,
    handle: MonitoringHandle
  ): Promise<void> {
    while (!handle.stopped && execution.status === WorkflowStatus.RUNNING) {
      // Calculate progress
      const totalNodes = execution.definition.nodes.length;
      const completedNodes = execution.completedNodes.size;
      const progress = (completedNodes / totalNodes) * 100;
      
      // Update metrics
      await this.metrics.updateProgress(execution.id, {
        progress,
        completedNodes,
        totalNodes,
        currentNodes: execution.currentNodes,
        estimatedTimeRemaining: this.estimateTimeRemaining(execution)
      });
      
      // Check for stuck workflows
      if (await this.isWorkflowStuck(execution)) {
        await this.alertManager.sendAlert({
          level: AlertLevel.WARNING,
          type: 'workflow.stuck',
          message: `Workflow ${execution.id} appears to be stuck`,
          metadata: {
            currentNodes: execution.currentNodes,
            lastUpdate: execution.lastUpdateTime
          }
        });
      }
      
      await sleep(5000); // Check every 5 seconds
    }
  }
  
  private async monitorPerformance(
    execution: WorkflowExecution,
    handle: MonitoringHandle
  ): Promise<void> {
    const performanceData: PerformanceData[] = [];
    
    while (!handle.stopped && execution.status === WorkflowStatus.RUNNING) {
      // Collect performance metrics
      const metrics = {
        timestamp: Date.now(),
        executionTime: Date.now() - execution.startTime.getTime(),
        nodeExecutionTimes: await this.getNodeExecutionTimes(execution),
        memoryUsage: await this.getMemoryUsage(execution),
        cpuUsage: await this.getCPUUsage(execution)
      };
      
      performanceData.push(metrics);
      
      // Analyze performance
      const analysis = this.analyzePerformance(performanceData);
      
      if (analysis.hasBottleneck) {
        await this.alertManager.sendAlert({
          level: AlertLevel.INFO,
          type: 'workflow.bottleneck',
          message: `Performance bottleneck detected in workflow ${execution.id}`,
          metadata: {
            bottleneckNode: analysis.bottleneckNode,
            avgExecutionTime: analysis.avgExecutionTime
          }
        });
      }
      
      // Update dashboard
      await this.dashboardService.updatePerformanceData(
        execution.id,
        metrics
      );
      
      await sleep(10000); // Check every 10 seconds
    }
  }
  
  async generateExecutionReport(
    executionId: string
  ): Promise<WorkflowExecutionReport> {
    const execution = await this.getExecution(executionId);
    const metrics = await this.metrics.getExecutionMetrics(executionId);
    
    return {
      executionId,
      workflowId: execution.workflowId,
      status: execution.status,
      startTime: execution.startTime,
      endTime: execution.endTime,
      duration: execution.endTime 
        ? execution.endTime.getTime() - execution.startTime.getTime()
        : null,
      nodeMetrics: this.calculateNodeMetrics(execution),
      performanceMetrics: metrics.performance,
      resourceUsage: metrics.resources,
      errors: execution.errors,
      timeline: this.generateTimeline(execution)
    };
  }
}
```

### Task 3.12: ì—ì´ì „íŠ¸ ì²´ì¸ ê´€ë¦¬ ì‹œìŠ¤í…œ

#### SubTask 3.12.1: ì²´ì¸ ì •ì˜ ë° êµ¬ì„±
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/chain/agent_chain.py
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass
from abc import ABC, abstractmethod
import asyncio

@dataclass
class ChainLink:
    agent_type: str
    action: str
    input_mapper: Optional[Callable[[Any], Any]] = None
    output_mapper: Optional[Callable[[Any], Any]] = None
    error_handler: Optional[Callable[[Exception], Any]] = None
    retry_policy: Optional[RetryPolicy] = None
    timeout: Optional[int] = None

class AgentChain:
    """Manages sequential execution of agents"""
    
    def __init__(self, chain_id: str, links: List[ChainLink]):
        self.chain_id = chain_id
        self.links = links
        self.execution_history: List[ChainExecution] = []
        self.middleware: List[ChainMiddleware] = []
        
    def add_middleware(self, middleware: ChainMiddleware) -> None:
        """Add middleware to the chain"""
        self.middleware.append(middleware)
    
    async def execute(
        self,
        initial_input: Any,
        context: ChainContext = None
    ) -> ChainResult:
        """Execute the agent chain"""
        
        context = context or ChainContext()
        execution = ChainExecution(
            chain_id=self.chain_id,
            start_time=datetime.utcnow(),
            context=context
        )
        
        current_input = initial_input
        
        try:
            # Apply pre-execution middleware
            for mw in self.middleware:
                current_input = await mw.before_chain(current_input, context)
            
            # Execute each link
            for i, link in enumerate(self.links):
                try:
                    # Apply pre-link middleware
                    for mw in self.middleware:
                        current_input = await mw.before_link(
                            i,
                            link,
                            current_input,
                            context
                        )
                    
                    # Execute link
                    result = await self._execute_link(
                        link,
                        current_input,
                        context
                    )
                    
                    # Apply post-link middleware
                    for mw in self.middleware:
                        result = await mw.after_link(
                            i,
                            link,
                            result,
                            context
                        )
                    
                    # Store result
                    execution.link_results.append(LinkResult(
                        link_index=i,
                        agent_type=link.agent_type,
                        action=link.action,
                        input=current_input,
                        output=result,
                        success=True
                    ))
                    
                    # Prepare input for next link
                    current_input = result
                    
                except Exception as e:
                    # Handle link error
                    handled = await self._handle_link_error(
                        link,
                        e,
                        current_input,
                        context
                    )
                    
                    if not handled:
                        raise
                    
                    current_input = handled
            
            # Apply post-execution middleware
            final_result = current_input
            for mw in self.middleware:
                final_result = await mw.after_chain(final_result, context)
            
            execution.end_time = datetime.utcnow()
            execution.success = True
            execution.final_result = final_result
            
        except Exception as e:
            execution.end_time = datetime.utcnow()
            execution.success = False
            execution.error = str(e)
            raise
            
        finally:
            self.execution_history.append(execution)
        
        return ChainResult(
            success=execution.success,
            result=execution.final_result,
            execution_time=(
                execution.end_time - execution.start_time
            ).total_seconds(),
            link_results=execution.link_results
        )
```

#### SubTask 3.12.2: ì²´ì¸ ì‹¤í–‰ ì „ëµ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/chain/chain_execution_strategy.ts
export abstract class ChainExecutionStrategy {
  abstract async execute(
    chain: AgentChain,
    input: any,
    context: ChainContext
  ): Promise<ChainResult>;
}

export class SequentialExecutionStrategy extends ChainExecutionStrategy {
  async execute(
    chain: AgentChain,
    input: any,
    context: ChainContext
  ): Promise<ChainResult> {
    let currentInput = input;
    const results: LinkResult[] = [];
    
    for (const link of chain.links) {
      const result = await this.executeLink(link, currentInput, context);
      results.push(result);
      currentInput = result.output;
    }
    
    return {
      success: true,
      result: currentInput,
      linkResults: results
    };
  }
}

export class ParallelExecutionStrategy extends ChainExecutionStrategy {
  constructor(
    private readonly mergeStrategy: MergeStrategy
  ) {
    super();
  }
  
  async execute(
    chain: AgentChain,
    input: any,
    context: ChainContext
  ): Promise<ChainResult> {
    // Execute all links in parallel
    const promises = chain.links.map(link =>
      this.executeLink(link, input, context)
    );
    
    const results = await Promise.all(promises);
    
    // Merge results
    const mergedResult = await this.mergeStrategy.merge(
      results.map(r => r.output)
    );
    
    return {
      success: true,
      result: mergedResult,
      linkResults: results
    };
  }
}

export class ConditionalExecutionStrategy extends ChainExecutionStrategy {
  constructor(
    private readonly conditions: ChainConditions
  ) {
    super();
  }
  
  async execute(
    chain: AgentChain,
    input: any,
    context: ChainContext
  ): Promise<ChainResult> {
    let currentInput = input;
    const results: LinkResult[] = [];
    
    for (let i = 0; i < chain.links.length; i++) {
      const link = chain.links[i];
      
      // Check if link should be executed
      if (await this.shouldExecuteLink(i, currentInput, context)) {
        const result = await this.executeLink(link, currentInput, context);
        results.push(result);
        currentInput = result.output;
      } else {
        // Skip link
        results.push({
          linkIndex: i,
          skipped: true,
          reason: 'Condition not met'
        });
      }
    }
    
    return {
      success: true,
      result: currentInput,
      linkResults: results
    };
  }
  
  private async shouldExecuteLink(
    linkIndex: number,
    input: any,
    context: ChainContext
  ): Promise<boolean> {
    const condition = this.conditions.getCondition(linkIndex);
    if (!condition) return true;
    
    return await condition.evaluate(input, context);
  }
}

export class PipelineExecutionStrategy extends ChainExecutionStrategy {
  constructor(
    private readonly bufferSize: number = 3
  ) {
    super();
  }
  
  async execute(
    chain: AgentChain,
    input: any,
    context: ChainContext
  ): Promise<ChainResult> {
    const pipeline = new ExecutionPipeline(this.bufferSize);
    const results: LinkResult[] = [];
    
    // Setup pipeline stages
    for (let i = 0; i < chain.links.length; i++) {
      const link = chain.links[i];
      
      pipeline.addStage(async (stageInput: any) => {
        const result = await this.executeLink(link, stageInput, context);
        results[i] = result;
        return result.output;
      });
    }
    
    // Execute pipeline
    const finalResult = await pipeline.execute(input);
    
    return {
      success: true,
      result: finalResult,
      linkResults: results
    };
  }
}
```

#### SubTask 3.12.3: ì²´ì¸ ìµœì í™” ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/chain/chain_optimizer.py
from typing import List, Dict, Any, Tuple
import asyncio
from dataclasses import dataclass

@dataclass
class ChainOptimizationResult:
    original_chain: AgentChain
    optimized_chain: AgentChain
    improvements: List[str]
    estimated_speedup: float
    recommendations: List[str]

class ChainOptimizer:
    """Optimizes agent chain execution"""
    
    def __init__(self):
        self.optimization_rules: List[OptimizationRule] = [
            ParallelizableLinksRule(),
            RedundantLinkRemovalRule(),
            CachingOpportunityRule(),
            LinkReorderingRule(),
            BatchingRule()
        ]
        self.performance_analyzer = ChainPerformanceAnalyzer()
        
    async def optimize_chain(
        self,
        chain: AgentChain,
        execution_history: List[ChainExecution] = None
    ) -> ChainOptimizationResult:
        """Optimize an agent chain"""
        
        # Analyze current performance
        if execution_history:
            performance = await self.performance_analyzer.analyze(
                chain,
                execution_history
            )
        else:
            performance = None
        
        # Apply optimization rules
        optimized_chain = chain.copy()
        improvements = []
        
        for rule in self.optimization_rules:
            if await rule.is_applicable(optimized_chain, performance):
                result = await rule.apply(optimized_chain, performance)
                optimized_chain = result.chain
                improvements.extend(result.improvements)
        
        # Estimate speedup
        speedup = await self._estimate_speedup(
            chain,
            optimized_chain,
            performance
        )
        
        # Generate recommendations
        recommendations = await self._generate_recommendations(
            chain,
            optimized_chain,
            performance
        )
        
        return ChainOptimizationResult(
            original_chain=chain,
            optimized_chain=optimized_chain,
            improvements=improvements,
            estimated_speedup=speedup,
            recommendations=recommendations
        )
    
    async def _estimate_speedup(
        self,
        original: AgentChain,
        optimized: AgentChain,
        performance: ChainPerformance
    ) -> float:
        """Estimate performance improvement"""
        
        if not performance:
            return 1.0
        
        # Simulate execution times
        original_time = await self._simulate_execution_time(
            original,
            performance
        )
        optimized_time = await self._simulate_execution_time(
            optimized,
            performance
        )
        
        return original_time / optimized_time if optimized_time > 0 else 1.0

class ParallelizableLinksRule(OptimizationRule):
    """Identifies links that can be executed in parallel"""
    
    async def is_applicable(
        self,
        chain: AgentChain,
        performance: ChainPerformance
    ) -> bool:
        # Check for independent links
        dependencies = self._analyze_dependencies(chain)
        
        for i in range(len(chain.links) - 1):
            for j in range(i + 1, len(chain.links)):
                if not self._has_dependency(dependencies, i, j):
                    return True
        
        return False
    
    async def apply(
        self,
        chain: AgentChain,
        performance: ChainPerformance
    ) -> OptimizationResult:
        """Convert sequential links to parallel execution"""
        
        dependencies = self._analyze_dependencies(chain)
        parallel_groups = self._identify_parallel_groups(
            chain.links,
            dependencies
        )
        
        # Create optimized chain with parallel execution
        optimized_links = []
        improvements = []
        
        for group in parallel_groups:
            if len(group) > 1:
                # Create parallel link
                parallel_link = ParallelLink(
                    links=[chain.links[i] for i in group],
                    merge_strategy=AutoMergeStrategy()
                )
                optimized_links.append(parallel_link)
                
                improvements.append(
                    f"Parallelized {len(group)} independent links: "
                    f"{[chain.links[i].agent_type for i in group]}"
                )
            else:
                # Keep sequential
                optimized_links.append(chain.links[group[0]])
        
        optimized_chain = AgentChain(
            chain_id=f"{chain.chain_id}_optimized",
            links=optimized_links
        )
        
        return OptimizationResult(
            chain=optimized_chain,
            improvements=improvements
        )
```

#### SubTask 3.12.4: ì²´ì¸ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/chain/chain_recovery.py
from typing import Dict, Any, List, Optional
import asyncio
from dataclasses import dataclass

@dataclass
class ChainRecoveryPoint:
    link_index: int
    input_data: Any
    context_snapshot: Dict[str, Any]
    timestamp: datetime

class ChainRecoveryManager:
    """Manages chain execution recovery"""
    
    def __init__(self):
        self.recovery_points: Dict[str, List[ChainRecoveryPoint]] = {}
        self.recovery_strategies: Dict[str, RecoveryStrategy] = {
            'retry': RetryRecoveryStrategy(),
            'skip': SkipLinkRecoveryStrategy(),
            'compensate': CompensationRecoveryStrategy(),
            'restart': RestartChainRecoveryStrategy()
        }
        
    async def create_recovery_point(
        self,
        chain_id: str,
        link_index: int,
        input_data: Any,
        context: ChainContext
    ) -> None:
        """Create a recovery point"""
        
        recovery_point = ChainRecoveryPoint(
            link_index=link_index,
            input_data=self._deep_copy(input_data),
            context_snapshot=self._snapshot_context(context),
            timestamp=datetime.utcnow()
        )
        
        if chain_id not in self.recovery_points:
            self.recovery_points[chain_id] = []
        
        self.recovery_points[chain_id].append(recovery_point)
        
        # Keep only recent recovery points
        self._cleanup_old_recovery_points(chain_id)
    
    async def recover_chain_execution(
        self,
        chain: AgentChain,
        failure_info: ChainFailureInfo,
        strategy_name: str = 'retry'
    ) -> ChainResult:
        """Recover from chain execution failure"""
        
        strategy = self.recovery_strategies.get(strategy_name)
        if not strategy:
            raise ValueError(f"Unknown recovery strategy: {strategy_name}")
        
        # Find appropriate recovery point
        recovery_point = self._find_recovery_point(
            chain.chain_id,
            failure_info.failed_link_index
        )
        
        if not recovery_point:
            # No recovery point, restart from beginning
            recovery_point = ChainRecoveryPoint(
                link_index=0,
                input_data=failure_info.initial_input,
                context_snapshot={},
                timestamp=datetime.utcnow()
            )
        
        # Apply recovery strategy
        recovery_result = await strategy.recover(
            chain,
            recovery_point,
            failure_info
        )
        
        return recovery_result

class CompensationRecoveryStrategy(RecoveryStrategy):
    """Compensates for failed operations"""
    
    async def recover(
        self,
        chain: AgentChain,
        recovery_point: ChainRecoveryPoint,
        failure_info: ChainFailureInfo
    ) -> ChainResult:
        """Recover by compensating previous operations"""
        
        # Create compensation chain
        compensation_links = []
        
        # Add compensation for completed links in reverse order
        for i in range(failure_info.failed_link_index - 1, -1, -1):
            original_link = chain.links[i]
            
            if self._has_compensation(original_link):
                compensation_link = self._create_compensation_link(
                    original_link,
                    failure_info.link_results[i]
                )
                compensation_links.append(compensation_link)
        
        # Execute compensation chain
        if compensation_links:
            compensation_chain = AgentChain(
                chain_id=f"{chain.chain_id}_compensation",
                links=compensation_links
            )
            
            await compensation_chain.execute(
                failure_info.failure_context,
                ChainContext(is_compensation=True)
            )
        
        # Restart from recovery point with modified strategy
        modified_chain = self._modify_chain_for_recovery(
            chain,
            recovery_point.link_index
        )
        
        return await modified_chain.execute(
            recovery_point.input_data,
            ChainContext(
                is_recovery=True,
                recovery_metadata=failure_info.to_dict()
            )
        )
```

### Task 3.13: ë³‘ë ¬ ì²˜ë¦¬ ë° ì¡°ì • ë©”ì»¤ë‹ˆì¦˜

#### SubTask 3.13.1: ë³‘ë ¬ ì‹¤í–‰ í”„ë ˆì„ì›Œí¬
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/parallel/parallel_executor.py
from typing import List, Dict, Any, Optional, Callable
import asyncio
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

@dataclass
class ParallelTask:
    task_id: str
    agent_type: str
    action: str
    input_data: Any
    dependencies: List[str] = None
    priority: int = 0

class ParallelExecutor:
    """Executes agent tasks in parallel"""
    
    def __init__(self, max_concurrency: int = 10):
        self.max_concurrency = max_concurrency
        self.semaphore = asyncio.Semaphore(max_concurrency)
        self.thread_pool = ThreadPoolExecutor(max_workers=max_concurrency)
        self.task_queue = asyncio.PriorityQueue()
        self.running_tasks: Dict[str, asyncio.Task] = {}
        self.completed_tasks: Dict[str, Any] = {}
        
    async def execute_parallel(
        self,
        tasks: List[ParallelTask],
        timeout: Optional[int] = None
    ) -> Dict[str, Any]:
        """Execute tasks in parallel with dependency management"""
        
        # Build dependency graph
        dependency_graph = self._build_dependency_graph(tasks)
        
        # Start execution
        start_time = datetime.utcnow()
        
        try:
            # Schedule initial tasks (no dependencies)
            for task in tasks:
                if not task.dependencies:
                    await self._schedule_task(task)
            
            # Process tasks
            while self.running_tasks or not self.task_queue.empty():
                # Check timeout
                if timeout:
                    elapsed = (datetime.utcnow() - start_time).total_seconds()
                    if elapsed > timeout / 1000:
                        raise TimeoutError("Parallel execution timeout")
                
                # Get next task
                try:
                    priority, task = await asyncio.wait_for(
                        self.task_queue.get(),
                        timeout=1.0
                    )
                except asyncio.TimeoutError:
                    # No new tasks, check running tasks
                    await self._check_completed_tasks(dependency_graph)
                    continue
                
                # Execute task
                await self._execute_task(task)
                
                # Check for newly available tasks
                await self._check_completed_tasks(dependency_graph)
            
            return self.completed_tasks
            
        finally:
            # Cleanup
            await self._cleanup()
    
    async def _execute_task(self, task: ParallelTask) -> None:
        """Execute a single task"""
        
        async with self.semaphore:
            # Get agent
            agent = await self._get_agent(task.agent_type)
            
            # Create task coroutine
            task_coro = self._run_agent_task(agent, task)
            
            # Create and track async task
            async_task = asyncio.create_task(task_coro)
            self.running_tasks[task.task_id] = async_task
    
    async def _run_agent_task(
        self,
        agent: BaseAgent,
        task: ParallelTask
    ) -> None:
        """Run agent task and handle results"""
        
        try:
            # Execute agent action
            result = await agent.execute(task.action, task.input_data)
            
            # Store result
            self.completed_tasks[task.task_id] = result
            
            # Remove from running
            del self.running_tasks[task.task_id]
            
        except Exception as e:
            # Handle error
            self.completed_tasks[task.task_id] = ParallelTaskError(
                task_id=task.task_id,
                error=e
            )
            del self.running_tasks[task.task_id]
```

#### SubTask 3.13.2: ì‘ì—… ìŠ¤ì¼€ì¤„ë§ ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/parallel/task_scheduler.ts
export class TaskScheduler {
  private readonly readyQueue: PriorityQueue<ScheduledTask>;
  private readonly waitingTasks: Map<string, ScheduledTask>;
  private readonly resourceManager: ResourceManager;
  private readonly loadBalancer: LoadBalancer;
  
  constructor(config: SchedulerConfig) {
    this.readyQueue = new PriorityQueue(
      (a, b) => this.compareTasks(a, b)
    );
    this.waitingTasks = new Map();
    this.resourceManager = new ResourceManager(config.resources);
    this.loadBalancer = new LoadBalancer(config.agents);
  }
  
  async scheduleTask(task: Task): Promise<void> {
    const scheduledTask = new ScheduledTask({
      ...task,
      scheduledAt: new Date(),
      estimatedResources: await this.estimateResources(task)
    });
    
    // Check if dependencies are satisfied
    if (await this.areDependenciesSatisfied(task)) {
      // Add to ready queue
      this.readyQueue.enqueue(scheduledTask);
    } else {
      // Add to waiting tasks
      this.waitingTasks.set(task.id, scheduledTask);
    }
  }
  
  async getNextTask(): Promise<ScheduledTask | null> {
    while (!this.readyQueue.isEmpty()) {
      const task = this.readyQueue.peek();
      
      // Check resource availability
      if (await this.resourceManager.canAllocate(task.estimatedResources)) {
        // Check agent availability
        const agent = await this.loadBalancer.selectAgent(task.agentType);
        
        if (agent && agent.isAvailable()) {
          // Allocate resources
          await this.resourceManager.allocate(
            task.id,
            task.estimatedResources
          );
          
          // Assign agent
          task.assignedAgent = agent;
          
          return this.readyQueue.dequeue();
        }
      }
      
      // Resources or agent not available, try next task
      // This implements work-stealing
      const deferredTask = this.readyQueue.dequeue();
      this.readyQueue.enqueue(deferredTask); // Re-add with lower priority
      
      // Prevent infinite loop
      await new Promise(resolve => setTimeout(resolve, 100));
    }
    
    return null;
  }
  
  async onTaskCompleted(taskId: string, result: any): Promise<void> {
    // Release resources
    await this.resourceManager.release(taskId);
    
    // Update load balancer
    const task = await this.getTask(taskId);
    if (task?.assignedAgent) {
      this.loadBalancer.updateAgentLoad(
        task.assignedAgent.id,
        -1
      );
    }
    
    // Check waiting tasks for newly satisfied dependencies
    for (const [id, waitingTask] of this.waitingTasks) {
      if (waitingTask.dependencies.includes(taskId)) {
        if (await this.areDependenciesSatisfied(waitingTask)) {
          this.waitingTasks.delete(id);
          this.readyQueue.enqueue(waitingTask);
        }
      }
    }
  }
  
  private compareTasks(a: ScheduledTask, b: ScheduledTask): number {
    // Priority-based comparison
    if (a.priority !== b.priority) {
      return b.priority - a.priority; // Higher priority first
    }
    
    // FIFO for same priority
    return a.scheduledAt.getTime() - b.scheduledAt.getTime();
  }
}

export class LoadBalancer {
  private readonly agentLoads: Map<string, number> = new Map();
  private readonly agentCapabilities: Map<string, Set<string>> = new Map();
  
  constructor(private readonly agents: AgentInfo[]) {
    this.initializeAgentInfo();
  }
  
  async selectAgent(agentType: string): Promise<Agent | null> {
    // Get capable agents
    const capableAgents = this.agents.filter(
      agent => this.agentCapabilities.get(agent.id)?.has(agentType)
    );
    
    if (capableAgents.length === 0) {
      return null;
    }
    
    // Select agent with lowest load
    let selectedAgent = capableAgents[0];
    let minLoad = this.agentLoads.get(selectedAgent.id) || 0;
    
    for (const agent of capableAgents) {
      const load = this.agentLoads.get(agent.id) || 0;
      if (load < minLoad) {
        selectedAgent = agent;
        minLoad = load;
      }
    }
    
    // Update load
    this.agentLoads.set(selectedAgent.id, minLoad + 1);
    
    return selectedAgent;
  }
}
```

#### SubTask 3.13.3: ì¡°ì • ë° ë™ê¸°í™” ë©”ì»¤ë‹ˆì¦˜
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/parallel/coordination.py
from typing import Dict, List, Any, Optional, Set
import asyncio
from dataclasses import dataclass
from enum import Enum

class CoordinationStrategy(Enum):
    BARRIER = "barrier"
    CONSENSUS = "consensus"
    LEADER_ELECTION = "leader_election"
    TOKEN_RING = "token_ring"

class ParallelCoordinator:
    """Coordinates parallel agent execution"""
    
    def __init__(self):
        self.barriers: Dict[str, asyncio.Barrier] = {}
        self.consensus_managers: Dict[str, ConsensusManager] = {}
        self.leader_elections: Dict[str, LeaderElection] = {}
        self.synchronization_points: Dict[str, SyncPoint] = {}
        
    async def create_barrier(
        self,
        barrier_id: str,
        participant_count: int
    ) -> asyncio.Barrier:
        """Create a synchronization barrier"""
        
        if barrier_id in self.barriers:
            raise ValueError(f"Barrier {barrier_id} already exists")
        
        barrier = asyncio.Barrier(participant_count)
        self.barriers[barrier_id] = barrier
        
        return barrier
    
    async def wait_at_barrier(
        self,
        barrier_id: str,
        agent_id: str,
        timeout: Optional[int] = None
    ) -> int:
        """Wait at a synchronization barrier"""
        
        barrier = self.barriers.get(barrier_id)
        if not barrier:
            raise ValueError(f"Barrier {barrier_id} not found")
        
        try:
            # Wait with timeout
            if timeout:
                index = await asyncio.wait_for(
                    barrier.wait(),
                    timeout=timeout / 1000
                )
            else:
                index = await barrier.wait()
            
            # Log synchronization
            await self._log_synchronization(
                'barrier',
                barrier_id,
                agent_id,
                {'index': index}
            )
            
            return index
            
        except asyncio.BrokenBarrierError:
            # Barrier was broken, handle error
            await self._handle_broken_barrier(barrier_id, agent_id)
            raise
    
    async def achieve_consensus(
        self,
        consensus_id: str,
        agent_id: str,
        proposal: Any,
        required_votes: int
    ) -> ConsensusResult:
        """Achieve consensus among agents"""
        
        if consensus_id not in self.consensus_managers:
            self.consensus_managers[consensus_id] = ConsensusManager(
                required_votes=required_votes
            )
        
        manager = self.consensus_managers[consensus_id]
        
        # Submit proposal
        await manager.submit_proposal(agent_id, proposal)
        
        # Wait for consensus
        result = await manager.wait_for_consensus()
        
        return result
    
    async def elect_leader(
        self,
        election_id: str,
        candidate_id: str,
        priority: int = 0
    ) -> LeaderElectionResult:
        """Participate in leader election"""
        
        if election_id not in self.leader_elections:
            self.leader_elections[election_id] = LeaderElection()
        
        election = self.leader_elections[election_id]
        
        # Register candidate
        await election.register_candidate(candidate_id, priority)
        
        # Run election
        result = await election.run_election()
        
        return result

class ConsensusManager:
    """Manages consensus among parallel agents"""
    
    def __init__(self, required_votes: int):
        self.required_votes = required_votes
        self.proposals: Dict[str, Any] = {}
        self.votes: Dict[str, Dict[str, bool]] = {}
        self.consensus_reached = asyncio.Event()
        self.result: Optional[ConsensusResult] = None
        
    async def submit_proposal(
        self,
        agent_id: str,
        proposal: Any
    ) -> None:
        """Submit a proposal for consensus"""
        
        self.proposals[agent_id] = proposal
        
        # Auto-vote for own proposal
        if agent_id not in self.votes:
            self.votes[agent_id] = {}
        self.votes[agent_id][agent_id] = True
        
        # Check if consensus reached
        await self._check_consensus()
    
    async def vote(
        self,
        voter_id: str,
        proposal_id: str,
        vote: bool
    ) -> None:
        """Vote on a proposal"""
        
        if proposal_id not in self.proposals:
            raise ValueError(f"Proposal {proposal_id} not found")
        
        if voter_id not in self.votes:
            self.votes[voter_id] = {}
        
        self.votes[voter_id][proposal_id] = vote
        
        # Check if consensus reached
        await self._check_consensus()
    
    async def _check_consensus(self) -> None:
        """Check if consensus has been reached"""
        
        for proposal_id, proposal in self.proposals.items():
            votes_for = sum(
                1 for voter_votes in self.votes.values()
                if voter_votes.get(proposal_id, False)
            )
            
            if votes_for >= self.required_votes:
                self.result = ConsensusResult(
                    agreed=True,
                    proposal_id=proposal_id,
                    proposal=proposal,
                    votes_for=votes_for,
                    total_votes=len(self.votes)
                )
                self.consensus_reached.set()
                return
```

#### SubTask 3.13.4: ë³‘ë ¬ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/parallel/parallel_monitor.ts
export class ParallelExecutionMonitor {
  private readonly metrics: ParallelMetrics;
  private readonly tracer: DistributedTracer;
  private readonly analyzer: PerformanceAnalyzer;
  
  constructor() {
    this.metrics = new ParallelMetrics();
    this.tracer = new DistributedTracer();
    this.analyzer = new PerformanceAnalyzer();
  }
  
  async monitorParallelExecution(
    executionId: string,
    tasks: ParallelTask[]
  ): Promise<MonitoringSession> {
    const session = new MonitoringSession(executionId);
    
    // Start monitoring
    session.startTime = Date.now();
    session.totalTasks = tasks.length;
    
    // Create spans for distributed tracing
    const rootSpan = this.tracer.createSpan({
      name: 'parallel_execution',
      executionId,
      attributes: {
        taskCount: tasks.length
      }
    });
    
    // Monitor each task
    for (const task of tasks) {
      const taskSpan = this.tracer.createSpan({
        name: `task_${task.id}`,
        parent: rootSpan,
        attributes: {
          agentType: task.agentType,
          action: task.action
        }
      });
      
      session.taskSpans.set(task.id, taskSpan);
    }
    
    // Start real-time monitoring
    this.startRealtimeMonitoring(session);
    
    return session;
  }
  
  private async startRealtimeMonitoring(
    session: MonitoringSession
  ): Promise<void> {
    const monitoringInterval = setInterval(async () => {
      if (session.isCompleted) {
        clearInterval(monitoringInterval);
        return;
      }
      
      // Collect current metrics
      const snapshot = await this.collectMetricsSnapshot(session);
      
      // Analyze performance
      const analysis = this.analyzer.analyzeSnapshot(snapshot);
      
      // Detect issues
      if (analysis.hasBottleneck) {
        await this.handleBottleneck(session, analysis.bottleneck);
      }
      
      if (analysis.hasDeadlock) {
        await this.handleDeadlock(session, analysis.deadlock);
      }
      
      // Update metrics
      await this.metrics.update(session.executionId, snapshot);
      
    }, 1000); // Monitor every second
  }
  
  async collectMetricsSnapshot(
    session: MonitoringSession
  ): Promise<MetricsSnapshot> {
    return {
      timestamp: Date.now(),
      runningTasks: session.getRunningTasks().length,
      completedTasks: session.getCompletedTasks().length,
      failedTasks: session.getFailedTasks().length,
      averageTaskDuration: this.calculateAverageTaskDuration(session),
      resourceUtilization: await this.getResourceUtilization(),
      taskThroughput: this.calculateThroughput(session),
      concurrencyLevel: session.getRunningTasks().length,
      queueDepth: await this.getQueueDepth()
    };
  }
  
  async generateExecutionReport(
    session: MonitoringSession
  ): Promise<ParallelExecutionReport> {
    const endTime = Date.now();
    const duration = endTime - session.startTime;
    
    // Close all spans
    for (const [taskId, span] of session.taskSpans) {
      span.end();
    }
    
    // Collect final metrics
    const finalMetrics = await this.metrics.getExecutionMetrics(
      session.executionId
    );
    
    // Generate Gantt chart data
    const ganttData = this.generateGanttChart(session);
    
    // Calculate efficiency metrics
    const efficiency = this.calculateEfficiencyMetrics(session);
    
    return {
      executionId: session.executionId,
      duration,
      totalTasks: session.totalTasks,
      successfulTasks: session.getCompletedTasks().length,
      failedTasks: session.getFailedTasks().length,
      averageTaskDuration: finalMetrics.averageTaskDuration,
      parallelEfficiency: efficiency.parallelEfficiency,
      speedup: efficiency.speedup,
      resourceEfficiency: efficiency.resourceEfficiency,
      ganttChart: ganttData,
      bottlenecks: session.detectedBottlenecks,
      recommendations: this.generateRecommendations(session, finalMetrics)
    };
  }
  
  private calculateEfficiencyMetrics(
    session: MonitoringSession
  ): EfficiencyMetrics {
    const tasks = session.getAllTasks();
    
    // Calculate theoretical sequential time
    const sequentialTime = tasks.reduce(
      (sum, task) => sum + task.duration,
      0
    );
    
    // Actual parallel time
    const parallelTime = session.endTime - session.startTime;
    
    // Calculate metrics
    const speedup = sequentialTime / parallelTime;
    const parallelEfficiency = speedup / session.maxConcurrency;
    
    return {
      speedup,
      parallelEfficiency,
      resourceEfficiency: this.calculateResourceEfficiency(session)
    };
  }
}
```

### Task 3.14: ì—ì´ì „íŠ¸ ê°„ ì˜ì¡´ì„± ê´€ë¦¬

#### SubTask 3.14.1: ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬í˜„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/dependency/dependency_graph.py
from typing import Dict, List, Set, Optional, Any
from dataclasses import dataclass
from collections import deque
import networkx as nx

@dataclass
class DependencyNode:
    node_id: str
    agent_type: str
    data: Dict[str, Any]
    dependencies: Set[str]
    dependents: Set[str]

class DependencyGraph:
    """Manages dependencies between agents and tasks"""
    
    def __init__(self):
        self.graph = nx.DiGraph()
        self.nodes: Dict[str, DependencyNode] = {}
        
    def add_node(
        self,
        node_id: str,
        agent_type: str,
        data: Dict[str, Any] = None
    ) -> DependencyNode:
        """Add a node to the dependency graph"""
        
        node = DependencyNode(
            node_id=node_id,
            agent_type=agent_type,
            data=data or {},
            dependencies=set(),
            dependents=set()
        )
        
        self.nodes[node_id] = node
        self.graph.add_node(node_id, **node.data)
        
        return node
    
    def add_dependency(
        self,
        from_node: str,
        to_node: str,
        dependency_type: str = "requires"
    ) -> None:
        """Add a dependency relationship"""
        
        if from_node not in self.nodes:
            raise ValueError(f"Node {from_node} not found")
        if to_node not in self.nodes:
            raise ValueError(f"Node {to_node} not found")
        
        # Update node relationships
        self.nodes[from_node].dependencies.add(to_node)
        self.nodes[to_node].dependents.add(from_node)
        
        # Add edge to graph
        self.graph.add_edge(
            to_node,
            from_node,
            dependency_type=dependency_type
        )
    
    def get_execution_order(self) -> List[str]:
        """Get topological order for execution"""
        
        try:
            return list(nx.topological_sort(self.graph))
        except nx.NetworkXUnfeasible:
            raise ValueError("Circular dependency detected")
    
    def get_parallel_groups(self) -> List[Set[str]]:
        """Get groups of nodes that can be executed in parallel"""
        
        groups = []
        remaining = set(self.nodes.keys())
        completed = set()
        
        while remaining:
            # Find nodes with all dependencies satisfied
            ready = set()
            for node_id in remaining:
                dependencies = self.nodes[node_id].dependencies
                if dependencies.issubset(completed):
                    ready.add(node_id)
            
            if not ready:
                # Circular dependency or error
                raise ValueError(
                    f"Cannot resolve dependencies for nodes: {remaining}"
                )
            
            groups.append(ready)
            completed.update(ready)
            remaining.difference_update(ready)
        
        return groups
    
    def detect_cycles(self) -> List[List[str]]:
        """Detect circular dependencies"""
        
        try:
            cycles = list(nx.simple_cycles(self.graph))
            return cycles
        except:
            return []
    
    def get_critical_path(self) -> List[str]:
        """Find the critical path in the dependency graph"""
        
        # Add weight to edges based on estimated execution time
        for edge in self.graph.edges():
            from_node, to_node = edge
            weight = self.nodes[to_node].data.get('estimated_time', 1)
            self.graph[from_node][to_node]['weight'] = weight
        
        # Find longest path (critical path)
        try:
            return nx.dag_longest_path(self.graph, weight='weight')
        except nx.NetworkXError:
            return []
```

#### SubTask 3.14.2: ì˜ì¡´ì„± í•´ê²° ì—”ì§„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/dependency/dependency_resolver.ts
export class DependencyResolver {
  private readonly graph: DependencyGraph;
  private readonly resolutionStrategies: Map<string, ResolutionStrategy>;
  private readonly cache: DependencyCache;
  
  constructor() {
    this.graph = new DependencyGraph();
    this.resolutionStrategies = new Map([
      ['eager', new EagerResolutionStrategy()],
      ['lazy', new LazyResolutionStrategy()],
      ['optimistic', new OptimisticResolutionStrategy()]
    ]);
    this.cache = new DependencyCache();
  }
  
  async resolveDependencies(
    rootNode: string,
    strategy: string = 'eager'
  ): Promise<DependencyResolution> {
    const resolution = new DependencyResolution(rootNode);
    
    // Check cache
    const cached = await this.cache.get(rootNode);
    if (cached && !cached.isExpired()) {
      return cached;
    }
    
    // Get resolution strategy
    const resolver = this.resolutionStrategies.get(strategy);
    if (!resolver) {
      throw new Error(`Unknown resolution strategy: ${strategy}`);
    }
    
    // Resolve dependencies
    await this.resolveNode(rootNode, resolution, resolver);
    
    // Cache result
    await this.cache.set(rootNode, resolution);
    
    return resolution;
  }
  
  private async resolveNode(
    nodeId: string,
    resolution: DependencyResolution,
    strategy: ResolutionStrategy
  ): Promise<void> {
    // Check if already resolved
    if (resolution.isResolved(nodeId)) {
      return;
    }
    
    // Mark as resolving to detect cycles
    resolution.markResolving(nodeId);
    
    // Get node dependencies
    const node = this.graph.getNode(nodeId);
    if (!node) {
      throw new Error(`Node ${nodeId} not found`);
    }
    
    // Resolve dependencies based on strategy
    const dependencies = await strategy.selectDependencies(
      node,
      this.graph
    );
    
    for (const depId of dependencies) {
      // Check for circular dependency
      if (resolution.isResolving(depId)) {
        throw new CircularDependencyError(
          `Circular dependency detected: ${nodeId} -> ${depId}`
        );
      }
      
      // Recursively resolve
      await this.resolveNode(depId, resolution, strategy);
    }
    
    // All dependencies resolved, mark this node as resolved
    resolution.markResolved(nodeId, dependencies);
  }
  
  async validateDependencies(
    nodes: DependencyNode[]
  ): Promise<ValidationResult> {
    const result = new ValidationResult();
    
    // Check for missing dependencies
    for (const node of nodes) {
      for (const depId of node.dependencies) {
        if (!this.graph.hasNode(depId)) {
          result.addError({
            type: 'missing_dependency',
            node: node.id,
            dependency: depId
          });
        }
      }
    }
    
    // Check for circular dependencies
    const cycles = this.graph.detectCycles();
    for (const cycle of cycles) {
      result.addError({
        type: 'circular_dependency',
        cycle
      });
    }
    
    // Check for version conflicts
    const conflicts = await this.detectVersionConflicts(nodes);
    for (const conflict of conflicts) {
      result.addError({
        type: 'version_conflict',
        ...conflict
      });
    }
    
    return result;
  }
}

export class OptimisticResolutionStrategy implements ResolutionStrategy {
  async selectDependencies(
    node: DependencyNode,
    graph: DependencyGraph
  ): Promise<string[]> {
    // Optimistically assume optional dependencies will be available
    const required = node.dependencies.filter(d => !d.optional);
    const optional = node.dependencies.filter(d => d.optional);
    
    // Try to include optional dependencies
    const available = [];
    for (const dep of optional) {
      if (await this.isDependencyAvailable(dep)) {
        available.push(dep.id);
      }
    }
    
    return [...required.map(d => d.id), ...available];
  }
  
  private async isDependencyAvailable(
    dependency: Dependency
  ): Promise<boolean> {
    // Check if dependency can be resolved quickly
    try {
      const timeout = 1000; // 1 second timeout for optional deps
      const result = await Promise.race([
        this.checkAvailability(dependency),
        this.timeout(timeout)
      ]);
      return result === true;
    } catch {
      return false;
    }
  }
}
```

#### SubTask 3.14.3: ì˜ì¡´ì„± ì£¼ì… ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/dependency/dependency_injection.py
from typing import Dict, Any, Type, Optional, List, Callable
import inspect
from dataclasses import dataclass

@dataclass
class Injectable:
    interface: Type
    implementation: Type
    scope: str  # singleton, transient, scoped
    factory: Optional[Callable] = None
    dependencies: List[Type] = None

class DependencyContainer:
    """Dependency injection container for agents"""
    
    def __init__(self):
        self.bindings: Dict[Type, Injectable] = {}
        self.singletons: Dict[Type, Any] = {}
        self.scoped_instances: Dict[str, Dict[Type, Any]] = {}
        
    def bind(
        self,
        interface: Type,
        implementation: Type,
        scope: str = "transient",
        factory: Optional[Callable] = None
    ) -> None:
        """Bind an interface to an implementation"""
        
        # Analyze dependencies
        dependencies = self._analyze_dependencies(implementation)
        
        injectable = Injectable(
            interface=interface,
            implementation=implementation,
            scope=scope,
            factory=factory,
            dependencies=dependencies
        )
        
        self.bindings[interface] = injectable
    
    def resolve(
        self,
        interface: Type,
        scope_id: Optional[str] = None
    ) -> Any:
        """Resolve a dependency"""
        
        if interface not in self.bindings:
            raise ValueError(f"No binding found for {interface}")
        
        injectable = self.bindings[interface]
        
        # Handle different scopes
        if injectable.scope == "singleton":
            return self._resolve_singleton(injectable)
        elif injectable.scope == "scoped":
            return self._resolve_scoped(injectable, scope_id)
        else:  # transient
            return self._resolve_transient(injectable)
    
    def _resolve_singleton(self, injectable: Injectable) -> Any:
        """Resolve singleton instance"""
        
        if injectable.interface in self.singletons:
            return self.singletons[injectable.interface]
        
        instance = self._create_instance(injectable)
        self.singletons[injectable.interface] = instance
        
        return instance
    
    def _resolve_scoped(
        self,
        injectable: Injectable,
        scope_id: str
    ) -> Any:
        """Resolve scoped instance"""
        
        if not scope_id:
            raise ValueError("Scope ID required for scoped dependencies")
        
        if scope_id not in self.scoped_instances:
            self.scoped_instances[scope_id] = {}
        
        scope = self.scoped_instances[scope_id]
        
        if injectable.interface in scope:
            return scope[injectable.interface]
        
        instance = self._create_instance(injectable)
        scope[injectable.interface] = instance
        
        return instance
    
    def _create_instance(self, injectable: Injectable) -> Any:
        """Create an instance with dependency injection"""
        
        if injectable.factory:
            # Use factory method
            return injectable.factory()
        
        # Resolve dependencies
        dependencies = {}
        for dep_type in injectable.dependencies:
            dependencies[dep_type.__name__.lower()] = self.resolve(dep_type)
        
        # Create instance
        return injectable.implementation(**dependencies)
    
    def _analyze_dependencies(self, cls: Type) -> List[Type]:
        """Analyze class dependencies from constructor"""
        
        signature = inspect.signature(cls.__init__)
        dependencies = []
        
        for param_name, param in signature.parameters.items():
            if param_name == 'self':
                continue
            
            if param.annotation != inspect.Parameter.empty:
                dependencies.append(param.annotation)
        
        return dependencies

class AgentDependencyInjector:
    """Dependency injector for agent system"""
    
    def __init__(self, container: DependencyContainer):
        self.container = container
        
    async def inject_agent_dependencies(
        self,
        agent: BaseAgent
    ) -> None:
        """Inject dependencies into an agent"""
        
        # Get agent class
        agent_class = type(agent)
        
        # Find injectable properties
        for attr_name in dir(agent_class):
            if attr_name.startswith('_'):
                continue
            
            attr = getattr(agent_class, attr_name)
            
            # Check for @inject decorator
            if hasattr(attr, '_inject'):
                dependency_type = attr._inject['type']
                required = attr._inject.get('required', True)
                
                try:
                    # Resolve dependency
                    instance = self.container.resolve(
                        dependency_type,
                        scope_id=agent.agent_id
                    )
                    
                    # Inject into agent
                    setattr(agent, attr_name, instance)
                    
                except Exception as e:
                    if required:
                        raise
                    # Optional dependency, log and continue
                    print(f"Failed to inject {attr_name}: {e}")
    
    def inject(
        self,
        dependency_type: Type,
        required: bool = True
    ) -> property:
        """Decorator for marking injectable properties"""
        
        def decorator(func):
            func._inject = {
                'type': dependency_type,
                'required': required
            }
            return func
        
        return decorator
```

#### SubTask 3.14.4: ì˜ì¡´ì„± ë²„ì „ ê´€ë¦¬
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/dependency/version_manager.py
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import semver

@dataclass
class DependencyVersion:
    name: str
    version: str
    constraints: List[str]  # e.g., [">=1.0.0", "<2.0.0"]
    resolved_version: Optional[str] = None

class DependencyVersionManager:
    """Manages dependency versions for agents"""
    
    def __init__(self):
        self.available_versions: Dict[str, List[str]] = {}
        self.version_constraints: Dict[str, List[str]] = {}
        self.resolved_versions: Dict[str, str] = {}
        
    def register_version(
        self,
        dependency_name: str,
        version: str
    ) -> None:
        """Register an available version"""
        
        if dependency_name not in self.available_versions:
            self.available_versions[dependency_name] = []
        
        # Validate version format
        try:
            semver.VersionInfo.parse(version)
        except ValueError:
            raise ValueError(f"Invalid version format: {version}")
        
        self.available_versions[dependency_name].append(version)
        
        # Sort versions
        self.available_versions[dependency_name].sort(
            key=lambda v: semver.VersionInfo.parse(v),
            reverse=True
        )
    
    def add_constraint(
        self,
        dependency_name: str,
        constraint: str
    ) -> None:
        """Add a version constraint"""
        
        if dependency_name not in self.version_constraints:
            self.version_constraints[dependency_name] = []
        
        self.version_constraints[dependency_name].append(constraint)
    
    async def resolve_versions(self) -> Dict[str, str]:
        """Resolve all dependency versions"""
        
        resolution_order = self._get_resolution_order()
        
        for dep_name in resolution_order:
            version = await self._resolve_single_dependency(dep_name)
            
            if not version:
                raise VersionConflictError(
                    f"Cannot resolve version for {dep_name}"
                )
            
            self.resolved_versions[dep_name] = version
        
        return self.resolved_versions
    
    async def _resolve_single_dependency(
        self,
        dependency_name: str
    ) -> Optional[str]:
        """Resolve version for a single dependency"""
        
        available = self.available_versions.get(dependency_name, [])
        constraints = self.version_constraints.get(dependency_name, [])
        
        # Find compatible version
        for version in available:
            if self._satisfies_constraints(version, constraints):
                return version
        
        return None
    
    def _satisfies_constraints(
        self,
        version: str,
        constraints: List[str]
    ) -> bool:
        """Check if version satisfies all constraints"""
        
        ver = semver.VersionInfo.parse(version)
        
        for constraint in constraints:
            if constraint.startswith('>='):
                min_ver = semver.VersionInfo.parse(constraint[2:])
                if ver < min_ver:
                    return False
            elif constraint.startswith('<='):
                max_ver = semver.VersionInfo.parse(constraint[2:])
                if ver > max_ver:
                    return False
            elif constraint.startswith('>'):
                min_ver = semver.VersionInfo.parse(constraint[1:])
                if ver <= min_ver:
                    return False
            elif constraint.startswith('<'):
                max_ver = semver.VersionInfo.parse(constraint[1:])
                if ver >= max_ver:
                    return False
            elif constraint.startswith('=='):
                exact_ver = semver.VersionInfo.parse(constraint[2:])
                if ver != exact_ver:
                    return False
            elif constraint.startswith('~'):
                # Compatible version (same major.minor)
                base_ver = semver.VersionInfo.parse(constraint[1:])
                if ver.major != base_ver.major or ver.minor != base_ver.minor:
                    return False
            elif constraint.startswith('^'):
                # Compatible version (same major)
                base_ver = semver.VersionInfo.parse(constraint[1:])
                if ver.major != base_ver.major or ver < base_ver:
                    return False
        
        return True
```

### Task 3.15: í˜‘ì—… íŒ¨í„´ ë¼ì´ë¸ŒëŸ¬ë¦¬

#### SubTask 3.15.1: í˜‘ì—… íŒ¨í„´ ì •ì˜
**ë‹´ë‹¹ì**: ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/patterns/collaboration_patterns.py
from typing import List, Dict, Any, Optional, Callable
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum

class PatternType(Enum):
    PIPELINE = "pipeline"
    SCATTER_GATHER = "scatter_gather"
    AGGREGATOR = "aggregator"
    ROUTER = "router"
    ORCHESTRATOR = "orchestrator"
    CHOREOGRAPHY = "choreography"
    SAGA = "saga"
    REQUEST_REPLY = "request_reply"

@dataclass
class CollaborationPattern(ABC):
    """Base class for collaboration patterns"""
    
    pattern_id: str
    pattern_type: PatternType
    name: str
    description: str
    participants: List[str]
    
    @abstractmethod
    async def execute(
        self,
        context: PatternContext,
        input_data: Any
    ) -> PatternResult:
        pass

class PipelinePattern(CollaborationPattern):
    """Sequential processing pattern"""
    
    def __init__(self, stages: List[PipelineStage]):
        super().__init__(
            pattern_id=f"pipeline_{uuid.uuid4()}",
            pattern_type=PatternType.PIPELINE,
            name="Pipeline Pattern",
            description="Sequential processing through stages",
            participants=[stage.agent_type for stage in stages]
        )
        self.stages = stages
    
    async def execute(
        self,
        context: PatternContext,
        input_data: Any
    ) -> PatternResult:
        """Execute pipeline pattern"""
        
        current_data = input_data
        stage_results = []
        
        for stage in self.stages:
            try:
                # Execute stage
                result = await stage.execute(current_data, context)
                
                # Transform output for next stage
                if stage.output_transformer:
                    current_data = stage.output_transformer(result)
                else:
                    current_data = result
                
                stage_results.append(StageResult(
                    stage_name=stage.name,
                    success=True,
                    output=result
                ))
                
            except Exception as e:
                # Handle stage failure
                if stage.error_handler:
                    recovery_data = await stage.error_handler(e, current_data)
                    if recovery_data:
                        current_data = recovery_data
                        continue
                
                # Pattern failed
                return PatternResult(
                    pattern_id=self.pattern_id,
                    success=False,
                    error=str(e),
                    stage_results=stage_results
                )
        
        return PatternResult(
            pattern_id=self.pattern_id,
            success=True,
            output=current_data,
            stage_results=stage_results
        )

class ScatterGatherPattern(CollaborationPattern):
    """Parallel processing with result aggregation"""
    
    def __init__(
        self,
        scatter_func: Callable,
        workers: List[str],
        gather_func: Callable
    ):
        super().__init__(
            pattern_id=f"scatter_gather_{uuid.uuid4()}",
            pattern_type=PatternType.SCATTER_GATHER,
            name="Scatter-Gather Pattern",
            description="Distribute work and aggregate results",
            participants=workers
        )
        self.scatter_func = scatter_func
        self.workers = workers
        self.gather_func = gather_func
    
    async def execute(
        self,
        context: PatternContext,
        input_data: Any
    ) -> PatternResult:
        """Execute scatter-gather pattern"""
        
        # Scatter phase
        work_items = self.scatter_func(input_data, len(self.workers))
        
        # Parallel execution
        tasks = []
        for worker, work_item in zip(self.workers, work_items):
            agent = await context.get_agent(worker)
            task = agent.execute(work_item)
            tasks.append(task)
        
        # Wait for all results
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter out errors
        successful_results = []
        errors = []
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                errors.append({
                    'worker': self.workers[i],
                    'error': str(result)
                })
            else:
                successful_results.append(result)
        
        # Gather phase
        if successful_results:
            final_result = self.gather_func(successful_results)
            
            return PatternResult(
                pattern_id=self.pattern_id,
                success=True,
                output=final_result,
                metadata={
                    'total_workers': len(self.workers),
                    'successful': len(successful_results),
                    'errors': errors
                }
            )
        else:
            return PatternResult(
                pattern_id=self.pattern_id,
                success=False,
                error="All workers failed",
                metadata={'errors': errors}
            )
```

#### SubTask 3.15.2: íŒ¨í„´ ì‹¤í–‰ ì—”ì§„
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/patterns/pattern_executor.ts
export class PatternExecutor {
  private readonly patternRegistry: PatternRegistry;
  private readonly agentRegistry: AgentRegistry;
  private readonly contextManager: ContextManager;
  private readonly monitor: PatternMonitor;
  
  constructor() {
    this.patternRegistry = new PatternRegistry();
    this.agentRegistry = AgentRegistry.getInstance();
    this.contextManager = new ContextManager();
    this.monitor = new PatternMonitor();
  }
  
  async executePattern(
    patternName: string,
    input: any,
    options: PatternExecutionOptions = {}
  ): Promise<PatternExecutionResult> {
    // Get pattern definition
    const pattern = this.patternRegistry.getPattern(patternName);
    if (!pattern) {
      throw new Error(`Pattern ${patternName} not found`);
    }
    
    // Create execution context
    const context = await this.contextManager.createContext({
      patternId: pattern.id,
      sessionId: options.sessionId || uuid(),
      metadata: options.metadata || {}
    });
    
    // Start monitoring
    const monitoringSession = await this.monitor.startMonitoring(
      pattern,
      context
    );
    
    try {
      // Validate input
      if (pattern.inputValidator) {
        const validation = await pattern.inputValidator(input);
        if (!validation.valid) {
          throw new ValidationError(validation.errors);
        }
      }
      
      // Execute pre-processors
      let processedInput = input;
      for (const processor of pattern.preProcessors || []) {
        processedInput = await processor(processedInput, context);
      }
      
      // Execute pattern
      const result = await this.executePatternCore(
        pattern,
        processedInput,
        context
      );
      
      // Execute post-processors
      let processedResult = result;
      for (const processor of pattern.postProcessors || []) {
        processedResult = await processor(processedResult, context);
      }
      
      // Complete monitoring
      await this.monitor.completeMonitoring(
        monitoringSession,
        'success'
      );
      
      return {
        success: true,
        patternId: pattern.id,
        result: processedResult,
        executionTime: context.getExecutionTime(),
        metrics: await this.monitor.getMetrics(monitoringSession)
      };
      
    } catch (error) {
      // Handle pattern execution error
      await this.monitor.completeMonitoring(
        monitoringSession,
        'error',
        error
      );
      
      // Execute error handlers
      if (pattern.errorHandlers) {
        for (const handler of pattern.errorHandlers) {
          const handled = await handler(error, context);
          if (handled) {
            return {
              success: false,
              patternId: pattern.id,
              error: error.message,
              handled: true,
              handlerResult: handled
            };
          }
        }
      }
      
      throw error;
      
    } finally {
      // Cleanup
      await this.contextManager.cleanupContext(context);
    }
  }
  
  private async executePatternCore(
    pattern: CollaborationPattern,
    input: any,
    context: PatternContext
  ): Promise<any> {
    switch (pattern.type) {
      case PatternType.ORCHESTRATOR:
        return await this.executeOrchestratorPattern(pattern, input, context);
        
      case PatternType.CHOREOGRAPHY:
        return await this.executeChoreographyPattern(pattern, input, context);
        
      case PatternType.SAGA:
        return await this.executeSagaPattern(pattern, input, context);
        
      default:
        // Use pattern's own execute method
        return await pattern.execute(context, input);
    }
  }
  
  private async executeOrchestratorPattern(
    pattern: OrchestratorPattern,
    input: any,
    context: PatternContext
  ): Promise<any> {
    // Create orchestrator agent
    const orchestrator = await this.agentRegistry.getAgent(
      pattern.orchestratorType
    );
    
    // Prepare worker pool
    const workers = new Map<string, Agent>();
    for (const workerType of pattern.workerTypes) {
      const worker = await this.agentRegistry.getAgent(workerType);
      workers.set(workerType, worker);
    }
    
    // Execute orchestration
    return await orchestrator.orchestrate({
      input,
      workers,
      workflow: pattern.workflow,
      context
    });
  }
  
  private async executeSagaPattern(
    pattern: SagaPattern,
    input: any,
    context: PatternContext
  ): Promise<any> {
    const saga = new SagaExecution(pattern, context);
    const completedSteps: CompletedStep[] = [];
    
    try {
      // Execute saga steps
      for (const step of pattern.steps) {
        const agent = await this.agentRegistry.getAgent(step.agentType);
        
        // Execute step
        const result = await agent.execute(step.action, input);
        
        completedSteps.push({
          step,
          result,
          agent
        });
        
        // Update input for next step
        if (step.outputMapper) {
          input = step.outputMapper(result, input);
        } else {
          input = result;
        }
      }
      
      // All steps completed successfully
      return input;
      
    } catch (error) {
      // Compensate in reverse order
      await this.compensateSaga(completedSteps.reverse(), error);
      throw error;
    }
  }
  
  private async compensateSaga(
    completedSteps: CompletedStep[],
    originalError: Error
  ): Promise<void> {
    for (const completed of completedSteps) {
      if (completed.step.compensate) {
        try {
          await completed.agent.execute(
            completed.step.compensate,
            completed.result
          );
        } catch (compensationError) {
          // Log but continue compensation
          console.error(
            `Compensation failed for step ${completed.step.name}:`,
            compensationError
          );
        }
      }
    }
  }
}
```

#### SubTask 3.15.3: íŒ¨í„´ ì¡°í•© ë° í™•ì¥
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/patterns/pattern_composition.py
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

class CompositePattern(CollaborationPattern):
    """Combines multiple patterns into a complex pattern"""
    
    def __init__(
        self,
        name: str,
        sub_patterns: List[CollaborationPattern],
        composition_strategy: CompositionStrategy
    ):
        super().__init__(
            pattern_id=f"composite_{uuid.uuid4()}",
            pattern_type=PatternType.COMPOSITE,
            name=name,
            description=f"Composite of {len(sub_patterns)} patterns",
            participants=self._extract_participants(sub_patterns)
        )
        self.sub_patterns = sub_patterns
        self.composition_strategy = composition_strategy
    
    async def execute(
        self,
        context: PatternContext,
        input_data: Any
    ) -> PatternResult:
        """Execute composite pattern"""
        
        return await self.composition_strategy.compose(
            self.sub_patterns,
            context,
            input_data
        )
    
    def _extract_participants(
        self,
        patterns: List[CollaborationPattern]
    ) -> List[str]:
        """Extract unique participants from sub-patterns"""
        
        participants = set()
        for pattern in patterns:
            participants.update(pattern.participants)
        
        return list(participants)

class SequentialComposition(CompositionStrategy):
    """Execute patterns sequentially"""
    
    async def compose(
        self,
        patterns: List[CollaborationPattern],
        context: PatternContext,
        input_data: Any
    ) -> PatternResult:
        """Execute patterns in sequence"""
        
        current_data = input_data
        pattern_results = []
        
        for pattern in patterns:
            result = await pattern.execute(context, current_data)
            pattern_results.append(result)
            
            if not result.success:
                return PatternResult(
                    pattern_id="sequential_composition",
                    success=False,
                    error=f"Pattern {pattern.name} failed: {result.error}",
                    sub_results=pattern_results
                )
            
            current_data = result.output
        
        return PatternResult(
            pattern_id="sequential_composition",
            success=True,
            output=current_data,
            sub_results=pattern_results
        )

class ConditionalComposition(CompositionStrategy):
    """Execute patterns based on conditions"""
    
    def __init__(self, conditions: Dict[str, Callable]):
        self.conditions = conditions
    
    async def compose(
        self,
        patterns: List[CollaborationPattern],
        context: PatternContext,
        input_data: Any
    ) -> PatternResult:
        """Execute patterns conditionally"""
        
        executed_patterns = []
        
        for pattern in patterns:
            # Check condition
            condition = self.conditions.get(pattern.pattern_id)
            
            if condition and not await condition(input_data, context):
                continue
            
            # Execute pattern
            result = await pattern.execute(context, input_data)
            executed_patterns.append((pattern, result))
            
            # Check if we should continue
            if result.metadata.get('stop_on_success') and result.success:
                break
            if result.metadata.get('stop_on_failure') and not result.success:
                break
        
        # Aggregate results
        if executed_patterns:
            final_output = executed_patterns[-1][1].output
            
            return PatternResult(
                pattern_id="conditional_composition",
                success=all(r.success for _, r in executed_patterns),
                output=final_output,
                sub_results=[r for _, r in executed_patterns]
            )
        else:
            return PatternResult(
                pattern_id="conditional_composition",
                success=False,
                error="No patterns were executed"
            )

class PatternExtender:
    """Extends existing patterns with additional functionality"""
    
    def extend_with_retry(
        self,
        pattern: CollaborationPattern,
        retry_config: RetryConfig
    ) -> CollaborationPattern:
        """Add retry capability to a pattern"""
        
        class RetryPattern(CollaborationPattern):
            async def execute(
                self,
                context: PatternContext,
                input_data: Any
            ) -> PatternResult:
                last_error = None
                
                for attempt in range(retry_config.max_attempts):
                    try:
                        result = await pattern.execute(context, input_data)
                        
                        if result.success or not retry_config.retry_on_failure:
                            return result
                        
                        last_error = result.error
                        
                    except Exception as e:
                        last_error = str(e)
                    
                    # Wait before retry
                    if attempt < retry_config.max_attempts - 1:
                        delay = retry_config.calculate_delay(attempt)
                        await asyncio.sleep(delay)
                
                # All retries exhausted
                return PatternResult(
                    pattern_id=pattern.pattern_id,
                    success=False,
                    error=f"Failed after {retry_config.max_attempts} attempts: {last_error}"
                )
        
        return RetryPattern(
            pattern_id=f"{pattern.pattern_id}_retry",
            pattern_type=pattern.pattern_type,
            name=f"{pattern.name} (with retry)",
            description=pattern.description,
            participants=pattern.participants
        )
    
    def extend_with_timeout(
        self,
        pattern: CollaborationPattern,
        timeout_ms: int
    ) -> CollaborationPattern:
        """Add timeout to a pattern"""
        
        class TimeoutPattern(CollaborationPattern):
            async def execute(
                self,
                context: PatternContext,
                input_data: Any
            ) -> PatternResult:
                try:
                    result = await asyncio.wait_for(
                        pattern.execute(context, input_data),
                        timeout=timeout_ms / 1000
                    )
                    return result
                    
                except asyncio.TimeoutError:
                    return PatternResult(
                        pattern_id=pattern.pattern_id,
                        success=False,
                        error=f"Pattern timed out after {timeout_ms}ms"
                    )
        
        return TimeoutPattern(
            pattern_id=f"{pattern.pattern_id}_timeout",
            pattern_type=pattern.pattern_type,
            name=f"{pattern.name} (with timeout)",
            description=pattern.description,
            participants=pattern.participants
        )
```

#### SubTask 3.15.4: íŒ¨í„´ ì¹´íƒˆë¡œê·¸ ë° ë¬¸ì„œí™”
**ë‹´ë‹¹ì**: ê¸°ìˆ  ë¬¸ì„œ ì‘ì„±ì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/patterns/pattern_catalog.ts
export class PatternCatalog {
  private readonly patterns: Map<string, PatternDefinition> = new Map();
  private readonly categories: Map<string, Set<string>> = new Map();
  private readonly examples: Map<string, PatternExample[]> = new Map();
  
  constructor() {
    this.initializeBuiltInPatterns();
  }
  
  private initializeBuiltInPatterns(): void {
    // Pipeline Pattern
    this.registerPattern({
      id: 'pipeline',
      name: 'Pipeline Pattern',
      category: 'Sequential',
      description: 'Processes data through a series of stages',
      use_cases: [
        'Data transformation pipelines',
        'Multi-step validation',
        'ETL processes'
      ],
      advantages: [
        'Simple to understand and implement',
        'Clear data flow',
        'Easy to add or remove stages'
      ],
      disadvantages: [
        'No parallelism',
        'Failure in one stage affects entire pipeline',
        'Can become a bottleneck'
      ],
      implementation_notes: `
        The Pipeline pattern is ideal when you have a clear sequence
        of transformations or validations to apply to data. Each stage
        should be independent and focused on a single responsibility.
      `,
      example_code: `
        const pipeline = new PipelinePattern([
          new ValidationStage('validator'),
          new TransformStage('transformer'),
          new PersistenceStage('persister')
        ]);
        
        const result = await pipeline.execute(context, inputData);
      `
    });
    
    // Scatter-Gather Pattern
    this.registerPattern({
      id: 'scatter_gather',
      name: 'Scatter-Gather Pattern',
      category: 'Parallel',
      description: 'Distributes work to multiple agents and aggregates results',
      use_cases: [
        'Parallel data processing',
        'Multi-source data aggregation',
        'Distributed calculations'
      ],
      advantages: [
        'High performance through parallelism',
        'Fault tolerance (partial results)',
        'Scalable'
      ],
      disadvantages: [
        'Complex error handling',
        'Result aggregation overhead',
        'Requires homogeneous tasks'
      ],
      implementation_notes: `
        Use Scatter-Gather when you can divide work into independent
        chunks. The scatter function should create balanced work items,
        and the gather function should handle partial results gracefully.
      `,
      example_code: `
        const scatterGather = new ScatterGatherPattern(
          (data) => data.chunk(workerCount), // scatter
          ['worker1', 'worker2', 'worker3'],  // workers
          (results) => results.flat()         // gather
        );
      `
    });
    
    // Saga Pattern
    this.registerPattern({
      id: 'saga',
      name: 'Saga Pattern',
      category: 'Transaction',
      description: 'Manages distributed transactions with compensation',
      use_cases: [
        'Multi-service transactions',
        'Order processing workflows',
        'Booking systems'
      ],
      advantages: [
        'Handles distributed transactions',
        'Built-in compensation logic',
        'Maintains consistency'
      ],
      disadvantages: [
        'Complex to implement',
        'Compensation may not always be possible',
        'Eventual consistency'
      ],
      implementation_notes: `
        Each step in a Saga must have a corresponding compensation
        action. Design your compensation logic to be idempotent.
        Consider using event sourcing for better auditability.
      `,
      example_code: `
        const orderSaga = new SagaPattern([
          {
            name: 'reserve_inventory',
            action: 'reserveItems',
            compensate: 'releaseItems'
          },
          {
            name: 'charge_payment',
            action: 'chargeCard',
            compensate: 'refundPayment'
          },
          {
            name: 'ship_order',
            action: 'createShipment',
            compensate: 'cancelShipment'
          }
        ]);
      `
    });
  }
  
  registerPattern(definition: PatternDefinition): void {
    this.patterns.set(definition.id, definition);
    
    // Update category index
    if (!this.categories.has(definition.category)) {
      this.categories.set(definition.category, new Set());
    }
    this.categories.get(definition.category)!.add(definition.id);
  }
  
  addExample(
    patternId: string,
    example: PatternExample
  ): void {
    if (!this.examples.has(patternId)) {
      this.examples.set(patternId, []);
    }
    this.examples.get(patternId)!.push(example);
  }
  
  generateDocumentation(): PatternDocumentation {
    const docs: PatternDocumentation = {
      overview: this.generateOverview(),
      patterns: {},
      examples: {},
      bestPractices: this.generateBestPractices(),
      decisionMatrix: this.generateDecisionMatrix()
    };
    
    // Generate documentation for each pattern
    for (const [id, pattern] of this.patterns) {
      docs.patterns[id] = {
        ...pattern,
        examples: this.examples.get(id) || [],
        relatedPatterns: this.findRelatedPatterns(id),
        implementationGuide: this.generateImplementationGuide(pattern)
      };
    }
    
    return docs;
  }
  
  private generateDecisionMatrix(): DecisionMatrix {
    return {
      headers: ['Pattern', 'Use When', 'Avoid When', 'Complexity'],
      rows: Array.from(this.patterns.values()).map(pattern => ({
        pattern: pattern.name,
        useWhen: pattern.use_cases[0],
        avoidWhen: pattern.disadvantages[0],
        complexity: this.assessComplexity(pattern)
      }))
    };
  }
  
  findPatternByCharacteristics(
    characteristics: PatternCharacteristics
  ): PatternRecommendation[] {
    const recommendations: PatternRecommendation[] = [];
    
    for (const [id, pattern] of this.patterns) {
      const score = this.calculateMatchScore(pattern, characteristics);
      
      if (score > 0.5) {
        recommendations.push({
          pattern,
          score,
          reasoning: this.generateRecommendationReasoning(
            pattern,
            characteristics
          )
        });
      }
    }
    
    return recommendations.sort((a, b) => b.score - a.score);
  }
}
```

ì´ì œ Task 3.16ë¶€í„° Task 3.20ê¹Œì§€ ì—ì´ì „íŠ¸ ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§ì— ëŒ€í•œ ì‘ì—…ì§€ì‹œì„œë¥¼ ì‘ì„±í•˜ê² ìŠµë‹ˆë‹¤.

### Task 3.16: ì—ì´ì „íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‹œìŠ¤í…œ

#### SubTask 3.16.1: ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì•„í‚¤í…ì²˜ êµ¬í˜„
**ë‹´ë‹¹ì**: ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/registry/agent_registry.py
from typing import Dict, List, Optional, Set, Type
import asyncio
from dataclasses import dataclass
from datetime import datetime

@dataclass
class AgentRegistration:
    agent_id: str
    agent_type: str
    agent_class: Type[BaseAgent]
    version: str
    capabilities: List[str]
    metadata: Dict[str, Any]
    registered_at: datetime
    status: str  # active, inactive, deprecated
    instances: Set[str]  # instance IDs

class AgentRegistry:
    """Central registry for all agents in the system"""
    
    _instance = None
    _lock = asyncio.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not hasattr(self, 'initialized'):
            self.registrations: Dict[str, AgentRegistration] = {}
            self.type_index: Dict[str, Set[str]] = {}  # type -> agent_ids
            self.capability_index: Dict[str, Set[str]] = {}  # capability -> agent_ids
            self.version_index: Dict[str, Dict[str, Set[str]]] = {}  # type -> version -> agent_ids
            self.storage = RegistryStorage()
            self.initialized = True
    
    async def register_agent(
        self,
        agent_type: str,
        agent_class: Type[BaseAgent],
        version: str,
        capabilities: List[str],
        metadata: Dict[str, Any] = None
    ) -> str:
        """Register a new agent type"""
        
        async with self._lock:
            # Generate agent ID
            agent_id = f"{agent_type}:{version}:{uuid.uuid4()}"
            
            # Create registration
            registration = AgentRegistration(
                agent_id=agent_id,
                agent_type=agent_type,
                agent_class=agent_class,
                version=version,
                capabilities=capabilities,
                metadata=metadata or {},
                registered_at=datetime.utcnow(),
                status="active",
                instances=set()
            )
            
            # Store registration
            self.registrations[agent_id] = registration
            
            # Update indices
            self._update_indices(registration)
            
            # Persist to storage
            await self.storage.save_registration(registration)
            
            # Emit registration event
            await self._emit_event('agent.registered', {
                'agent_id': agent_id,
                'agent_type': agent_type,
                'version': version
            })
            
            return agent_id
    
    async def get_agent_class(
        self,
        agent_type: str,
        version: Optional[str] = None,
        capability: Optional[str] = None
    ) -> Optional[Type[BaseAgent]]:
        """Get agent class by type and optional filters"""
        
        candidates = self.type_index.get(agent_type, set())
        
        if version:
            # Filter by version
            version_candidates = self.version_index.get(agent_type, {}).get(version, set())
            candidates = candidates.intersection(version_candidates)
        
        if capability:
            # Filter by capability
            capability_candidates = self.capability_index.get(capability, set())
            candidates = candidates.intersection(capability_candidates)
        
        if not candidates:
            return None
        
        # Get the most recent registration
        registrations = [self.registrations[aid] for aid in candidates]
        registrations.sort(key=lambda r: r.registered_at, reverse=True)
        
        return registrations[0].agent_class
    
    async def create_agent_instance(
        self,
        agent_type: str,
        config: Dict[str, Any],
        version: Optional[str] = None
    ) -> BaseAgent:
        """Create an instance of an agent"""
        
        agent_class = await self.get_agent_class(agent_type, version)
        
        if not agent_class:
            raise ValueError(f"Agent type {agent_type} not found")
        
        # Create instance
        instance = agent_class(config)
        
        # Register instance
        registration = self._find_registration(agent_type, version)
        if registration:
            registration.instances.add(instance.agent_id)
            await self.storage.update_registration(registration)
        
        return instance
    
    def _update_indices(self, registration: AgentRegistration) -> None:
        """Update internal indices"""
        
        # Type index
        if registration.agent_type not in self.type_index:
            self.type_index[registration.agent_type] = set()
        self.type_index[registration.agent_type].add(registration.agent_id)
        
        # Capability index
        for capability in registration.capabilities:
            if capability not in self.capability_index:
                self.capability_index[capability] = set()
            self.capability_index[capability].add(registration.agent_id)
        
        # Version index
        if registration.agent_type not in self.version_index:
            self.version_index[registration.agent_type] = {}
        if registration.version not in self.version_index[registration.agent_type]:
            self.version_index[registration.agent_type][registration.version] = set()
        self.version_index[registration.agent_type][registration.version].add(
            registration.agent_id
        )
```

#### SubTask 3.16.2: ì—ì´ì „íŠ¸ ë””ìŠ¤ì»¤ë²„ë¦¬ ì„œë¹„ìŠ¤
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/registry/discovery_service.ts
export class AgentDiscoveryService {
  private readonly registry: AgentRegistry;
  private readonly networkDiscovery: NetworkDiscovery;
  private readonly healthChecker: HealthChecker;
  private discoveryInterval: NodeJS.Timer;
  
  constructor() {
    this.registry = AgentRegistry.getInstance();
    this.networkDiscovery = new NetworkDiscovery();
    this.healthChecker = new HealthChecker();
  }
  
  async startDiscovery(): Promise<void> {
    // Start network discovery
    await this.networkDiscovery.start();
    
    // Start periodic discovery
    this.discoveryInterval = setInterval(
      () => this.discoverAgents(),
      30000 // 30 seconds
    );
    
    // Initial discovery
    await this.discoverAgents();
  }
  
  async discoverAgents(): Promise<DiscoveryResult> {
    const result = new DiscoveryResult();
    
    // Discover via multiple methods
    const discoveries = await Promise.all([
      this.discoverViaNetwork(),
      this.discoverViaConfiguration(),
      this.discoverViaServiceMesh(),
      this.discoverViaDNS()
    ]);
    
    // Merge discoveries
    for (const discovery of discoveries) {
      result.merge(discovery);
    }
    
    // Validate discovered agents
    await this.validateDiscoveredAgents(result);
    
    // Update registry
    await this.updateRegistry(result);
    
    return result;
  }
  
  private async discoverViaNetwork(): Promise<AgentInfo[]> {
    // Multicast discovery
    const multicastResults = await this.networkDiscovery.multicast({
      port: 5555,
      message: {
        type: 'agent_discovery',
        version: '1.0'
      }
    });
    
    // Parse responses
    const agents: AgentInfo[] = [];
    for (const response of multicastResults) {
      if (response.type === 'agent_announcement') {
        agents.push({
          id: response.agentId,
          type: response.agentType,
          address: response.address,
          port: response.port,
          capabilities: response.capabilities,
          metadata: response.metadata
        });
      }
    }
    
    return agents;
  }
  
  private async discoverViaServiceMesh(): Promise<AgentInfo[]> {
    // Query service mesh (e.g., Istio, Linkerd)
    const services = await this.queryServiceMesh();
    
    // Filter agent services
    const agentServices = services.filter(
      service => service.labels?.['agent-type']
    );
    
    // Convert to AgentInfo
    return agentServices.map(service => ({
      id: service.id,
      type: service.labels['agent-type'],
      address: service.address,
      port: service.port,
      capabilities: this.parseCapabilities(service.labels),
      metadata: {
        mesh: 'istio',
        namespace: service.namespace,
        version: service.version
      }
    }));
  }
  
  async findAgents(query: AgentQuery): Promise<AgentInfo[]> {
    const allAgents = await this.registry.getAllAgents();
    
    return allAgents.filter(agent => {
      // Filter by type
      if (query.type && agent.type !== query.type) {
        return false;
      }
      
      // Filter by capabilities
      if (query.capabilities) {
        const hasAllCapabilities = query.capabilities.every(
          cap => agent.capabilities.includes(cap)
        );
        if (!hasAllCapabilities) return false;
      }
      
      // Filter by status
      if (query.status && agent.status !== query.status) {
        return false;
      }
      
      // Filter by version
      if (query.version) {
        if (!this.matchesVersionConstraint(agent.version, query.version)) {
          return false;
        }
      }
      
      // Filter by metadata
      if (query.metadata) {
        for (const [key, value] of Object.entries(query.metadata)) {
          if (agent.metadata[key] !== value) {
            return false;
          }
        }
      }
      
      return true;
    });
  }
  
  private async validateDiscoveredAgents(
    result: DiscoveryResult
  ): Promise<void> {
    const validationTasks = result.agents.map(async agent => {
      try {
        // Health check
        const health = await this.healthChecker.check(agent);
        
        if (!health.healthy) {
          result.unhealthyAgents.push({
            agent,
            reason: health.reason
          });
          return false;
        }
        
        // Capability verification
        const verified = await this.verifyCapabilities(agent);
        
        if (!verified) {
          result.invalidAgents.push({
            agent,
            reason: 'Capability verification failed'
          });
          return false;
        }
        
        return true;
      } catch (error) {
        result.errors.push({
          agent,
          error: error.message
        });
        return false;
      }
    });
    
    const validationResults = await Promise.all(validationTasks);
    
    // Remove invalid agents
    result.agents = result.agents.filter((_, index) => validationResults[index]);
  }
}
```

#### SubTask 3.16.3: ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë™ê¸°í™”
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/registry/registry_sync.py
from typing import Dict, List, Set, Optional
import asyncio
from dataclasses import dataclass

@dataclass
class RegistryNode:
    node_id: str
    address: str
    last_seen: datetime
    is_primary: bool
    sync_status: str

class RegistrySynchronizer:
    """Synchronizes agent registry across multiple nodes"""
    
    def __init__(self, node_id: str, registry: AgentRegistry):
        self.node_id = node_id
        self.registry = registry
        self.peers: Dict[str, RegistryNode] = {}
        self.sync_queue = asyncio.Queue()
        self.consensus_manager = ConsensusManager()
        
    async def start_sync(self) -> None:
        """Start registry synchronization"""
        
        # Start peer discovery
        asyncio.create_task(self._discover_peers())
        
        # Start sync processor
        asyncio.create_task(self._process_sync_queue())
        
        # Start heartbeat
        asyncio.create_task(self._heartbeat_loop())
        
        # Initial sync
        await self._initial_sync()
    
    async def _initial_sync(self) -> None:
        """Perform initial synchronization with peers"""
        
        # Find primary node
        primary = await self._find_primary_node()
        
        if primary and primary.node_id != self.node_id:
            # Sync from primary
            await self._sync_from_node(primary)
        elif not primary:
            # No primary, initiate leader election
            await self._elect_primary()
    
    async def _sync_from_node(self, node: RegistryNode) -> None:
        """Sync registry from another node"""
        
        try:
            # Request full registry dump
            response = await self._request_registry_dump(node)
            
            # Validate response
            if not self._validate_registry_dump(response):
                raise ValueError("Invalid registry dump")
            
            # Apply changes
            await self._apply_registry_dump(response)
            
            # Update sync status
            node.sync_status = "synced"
            
        except Exception as e:
            print(f"Failed to sync from {node.node_id}: {e}")
            node.sync_status = "failed"
    
    async def handle_registration_change(
        self,
        change_type: str,
        registration: AgentRegistration
    ) -> None:
        """Handle local registry changes"""
        
        # Create sync event
        event = SyncEvent(
            event_id=str(uuid.uuid4()),
            node_id=self.node_id,
            event_type=change_type,
            registration=registration,
            timestamp=datetime.utcnow(),
            vector_clock=self._get_vector_clock()
        )
        
        # Add to sync queue
        await self.sync_queue.put(event)
        
        # Immediate broadcast for critical changes
        if change_type in ['register', 'deregister']:
            await self._broadcast_event(event)
    
    async def _process_sync_queue(self) -> None:
        """Process synchronization queue"""
        
        batch = []
        
        while True:
            try:
                # Collect events for batching
                while len(batch) < 100:
                    event = await asyncio.wait_for(
                        self.sync_queue.get(),
                        timeout=1.0
                    )
                    batch.append(event)
                
            except asyncio.TimeoutError:
                # Timeout reached, process batch if any
                pass
            
            if batch:
                await self._sync_batch(batch)
                batch = []
    
    async def _sync_batch(self, events: List[SyncEvent]) -> None:
        """Sync a batch of events to peers"""
        
        # Group by peer for efficiency
        peer_batches = self._group_events_by_peer(events)
        
        # Send to each peer
        tasks = []
        for peer_id, peer_events in peer_batches.items():
            if peer_id != self.node_id:
                peer = self.peers.get(peer_id)
                if peer and peer.sync_status == "synced":
                    task = self._send_sync_batch(peer, peer_events)
                    tasks.append(task)
        
        # Wait for all sends to complete
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Handle failures
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                # Mark peer as failed
                peer_id = list(peer_batches.keys())[i]
                if peer_id in self.peers:
                    self.peers[peer_id].sync_status = "failed"
```

#### SubTask 3.16.4: ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì¿¼ë¦¬ ìµœì í™”
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/registry/registry_query.ts
export class RegistryQueryOptimizer {
  private readonly cache: QueryCache;
  private readonly indexManager: IndexManager;
  private readonly queryPlanner: QueryPlanner;
  
  constructor(private readonly registry: AgentRegistry) {
    this.cache = new QueryCache();
    this.indexManager = new IndexManager(registry);
    this.queryPlanner = new QueryPlanner();
  }
  
  async optimizeQuery(query: RegistryQuery): Promise<QueryPlan> {
    // Check cache first
    const cacheKey = this.generateCacheKey(query);
    const cached = await this.cache.get(cacheKey);
    
    if (cached && !cached.isStale()) {
      return cached.plan;
    }
    
    // Analyze query
    const analysis = this.analyzeQuery(query);
    
    // Generate query plan
    const plan = await this.queryPlanner.createPlan(query, analysis);
    
    // Optimize plan
    const optimizedPlan = await this.optimizePlan(plan);
    
    // Cache the plan
    await this.cache.set(cacheKey, optimizedPlan);
    
    return optimizedPlan;
  }
  
  private analyzeQuery(query: RegistryQuery): QueryAnalysis {
    return {
      selectivity: this.estimateSelectivity(query),
      indexUsage: this.identifyUsableIndexes(query),
      joinComplexity: this.analyzeJoinComplexity(query),
      estimatedResultSize: this.estimateResultSize(query)
    };
  }
  
  private identifyUsableIndexes(query: RegistryQuery): IndexUsage[] {
    const usableIndexes: IndexUsage[] = [];
    
    // Check type index
    if (query.filters.type) {
      usableIndexes.push({
        index: 'type_index',
        field: 'type',
        selectivity: this.indexManager.getSelectivity('type', query.filters.type)
      });
    }
    
    // Check capability index
    if (query.filters.capabilities) {
      for (const capability of query.filters.capabilities) {
        usableIndexes.push({
          index: 'capability_index',
          field: 'capability',
          value: capability,
          selectivity: this.indexManager.getSelectivity('capability', capability)
        });
      }
    }
    
    // Check composite indexes
    const compositeIndexes = this.indexManager.getCompositeIndexes();
    for (const index of compositeIndexes) {
      if (this.canUseCompositeIndex(index, query)) {
        usableIndexes.push({
          index: index.name,
          fields: index.fields,
          selectivity: index.selectivity
        });
      }
    }
    
    return usableIndexes;
  }
  
  async executeOptimizedQuery(
    query: RegistryQuery
  ): Promise<QueryResult> {
    // Get optimized plan
    const plan = await this.optimizeQuery(query);
    
    // Execute based on plan type
    switch (plan.type) {
      case 'index_scan':
        return await this.executeIndexScan(plan);
        
      case 'full_scan':
        return await this.executeFullScan(plan);
        
      case 'multi_index':
        return await this.executeMultiIndexQuery(plan);
        
      case 'cached':
        return await this.executeCachedQuery(plan);
        
      default:
        throw new Error(`Unknown plan type: ${plan.type}`);
    }
  }
  
  private async executeMultiIndexQuery(
    plan: QueryPlan
  ): Promise<QueryResult> {
    // Execute sub-queries in parallel
    const subResults = await Promise.all(
      plan.subQueries.map(subQuery => 
        this.executeSubQuery(subQuery)
      )
    );
    
    // Merge results based on plan
    let result: Set<string>;
    
    if (plan.mergeStrategy === 'intersection') {
      // AND operation
      result = subResults.reduce((acc, curr) => 
        new Set([...acc].filter(x => curr.has(x)))
      );
    } else {
      // OR operation
      result = subResults.reduce((acc, curr) => 
        new Set([...acc, ...curr])
      );
    }
    
    // Apply remaining filters
    const finalResults = await this.applyFilters(
      Array.from(result),
      plan.remainingFilters
    );
    
    // Apply sorting and pagination
    return this.applyResultModifiers(finalResults, plan);
  }
}

export class QueryCache {
  private readonly cache: LRUCache<string, CachedQueryPlan>;
  private readonly ttl: number = 60000; // 1 minute
  
  constructor() {
    this.cache = new LRUCache({
      max: 1000,
      ttl: this.ttl
    });
  }
  
  async get(key: string): Promise<CachedQueryPlan | null> {
    const cached = this.cache.get(key);
    
    if (cached) {
      // Update access statistics
      cached.accessCount++;
      cached.lastAccessed = Date.now();
      
      return cached;
    }
    
    return null;
  }
  
  async set(key: string, plan: QueryPlan): Promise<void> {
    const cached: CachedQueryPlan = {
      plan,
      createdAt: Date.now(),
      lastAccessed: Date.now(),
      accessCount: 1,
      isStale: () => Date.now() - cached.createdAt > this.ttl
    };
    
    this.cache.set(key, cached);
  }
}
```

### Task 3.17: ì—ì´ì „íŠ¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

#### SubTask 3.17.1: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/monitoring/metrics_collector.py
from typing import Dict, List, Any, Optional
import asyncio
from dataclasses import dataclass
from datetime import datetime, timedelta
import statistics

@dataclass
class AgentMetric:
    agent_id: str
    metric_name: str
    value: float
    timestamp: datetime
    tags: Dict[str, str]
    metadata: Dict[str, Any]

class MetricsCollector:
    """Collects and aggregates agent performance metrics"""
    
    def __init__(self):
        self.metrics_buffer: List[AgentMetric] = []
        self.aggregators: Dict[str, MetricAggregator] = {}
        self.collectors: Dict[str, MetricCollector] = {}
        self.storage = MetricsStorage()
        self.flush_interval = 10  # seconds
        
    async def start_collection(self) -> None:
        """Start metrics collection"""
        
        # Register default collectors
        self.register_collector('cpu', CPUMetricCollector())
        self.register_collector('memory', MemoryMetricCollector())
        self.register_collector('latency', LatencyMetricCollector())
        self.register_collector('throughput', ThroughputMetricCollector())
        self.register_collector('errors', ErrorMetricCollector())
        
        # Start collection tasks
        for name, collector in self.collectors.items():
            asyncio.create_task(self._collect_metrics(name, collector))
        
        # Start flush task
        asyncio.create_task(self._flush_metrics())
    
    def register_collector(
        self,
        name: str,
        collector: MetricCollector
    ) -> None:
        """Register a metric collector"""
        self.collectors[name] = collector
    
    async def record_metric(
        self,
        agent_id: str,
        metric_name: str,
        value: float,
        tags: Dict[str, str] = None,
        metadata: Dict[str, Any] = None
    ) -> None:
        """Record a single metric"""
        
        metric = AgentMetric(
            agent_id=agent_id,
            metric_name=metric_name,
            value=value,
            timestamp=datetime.utcnow(),
            tags=tags or {},
            metadata=metadata or {}
        )
        
        # Add to buffer
        self.metrics_buffer.append(metric)
        
        # Update aggregators
        if metric_name in self.aggregators:
            self.aggregators[metric_name].add(metric)
    
    async def _collect_metrics(
        self,
        name: str,
        collector: MetricCollector
    ) -> None:
        """Collect metrics from a specific collector"""
        
        while True:
            try:
                # Get all active agents
                agents = await self._get_active_agents()
                
                # Collect metrics for each agent
                for agent in agents:
                    metrics = await collector.collect(agent)
                    
                    for metric in metrics:
                        await self.record_metric(
                            agent_id=agent.agent_id,
                            metric_name=f"{name}.{metric.name}",
                            value=metric.value,
                            tags=metric.tags,
                            metadata=metric.metadata
                        )
                
                # Wait for next collection interval
                await asyncio.sleep(collector.interval)
                
            except Exception as e:
                print(f"Error collecting {name} metrics: {e}")
                await asyncio.sleep(5)
    
    async def _flush_metrics(self) -> None:
        """Flush metrics to storage"""
        
        while True:
            await asyncio.sleep(self.flush_interval)
            
            if self.metrics_buffer:
                # Copy and clear buffer
                metrics_to_flush = self.metrics_buffer.copy()
                self.metrics_buffer.clear()
                
                # Store metrics
                try:
                    await self.storage.store_metrics(metrics_to_flush)
                except Exception as e:
                    print(f"Error flushing metrics: {e}")
                    # Re-add metrics to buffer
                    self.metrics_buffer.extend(metrics_to_flush)

class LatencyMetricCollector(MetricCollector):
    """Collects latency metrics for agent operations"""
    
    def __init__(self):
        self.interval = 5  # seconds
        self.latency_tracker = LatencyTracker()
        
    async def collect(self, agent: BaseAgent) -> List[Metric]:
        """Collect latency metrics"""
        
        metrics = []
        
        # Get latency statistics
        stats = self.latency_tracker.get_stats(agent.agent_id)
        
        if stats:
            metrics.extend([
                Metric(
                    name="p50",
                    value=stats.percentile(50),
                    tags={"agent_type": agent.agent_type}
                ),
                Metric(
                    name="p95",
                    value=stats.percentile(95),
                    tags={"agent_type": agent.agent_type}
                ),
                Metric(
                    name="p99",
                    value=stats.percentile(99),
                    tags={"agent_type": agent.agent_type}
                ),
                Metric(
                    name="mean",
                    value=stats.mean(),
                    tags={"agent_type": agent.agent_type}
                )
            ])
        
        return metrics
```

#### SubTask 3.17.2: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
**ë‹´ë‹¹ì**: í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/monitoring/realtime_dashboard.ts
export class RealtimeDashboard {
  private readonly websocket: WebSocketServer;
  private readonly metricsStore: MetricsStore;
  private readonly alertManager: AlertManager;
  private readonly subscribers: Map<string, DashboardSubscriber> = new Map();
  
  constructor() {
    this.websocket = new WebSocketServer({ port: 8080 });
    this.metricsStore = new MetricsStore();
    this.alertManager = new AlertManager();
    
    this.setupWebSocketHandlers();
    this.startMetricsBroadcast();
  }
  
  private setupWebSocketHandlers(): void {
    this.websocket.on('connection', (ws, req) => {
      const subscriberId = uuid();
      
      const subscriber = new DashboardSubscriber({
        id: subscriberId,
        ws,
        subscriptions: new Set(),
        filters: {}
      });
      
      this.subscribers.set(subscriberId, subscriber);
      
      ws.on('message', (message) => {
        this.handleSubscriberMessage(subscriber, message);
      });
      
      ws.on('close', () => {
        this.subscribers.delete(subscriberId);
      });
      
      // Send initial data
      this.sendInitialData(subscriber);
    });
  }
  
  private async handleSubscriberMessage(
    subscriber: DashboardSubscriber,
    message: any
  ): Promise<void> {
    const data = JSON.parse(message);
    
    switch (data.type) {
      case 'subscribe':
        await this.handleSubscribe(subscriber, data);
        break;
        
      case 'unsubscribe':
        await this.handleUnsubscribe(subscriber, data);
        break;
        
      case 'filter':
        await this.handleFilter(subscriber, data);
        break;
        
      case 'query':
        await this.handleQuery(subscriber, data);
        break;
    }
  }
  
  private async sendInitialData(
    subscriber: DashboardSubscriber
  ): Promise<void> {
    // Send system overview
    const overview = await this.getSystemOverview();
    subscriber.ws.send(JSON.stringify({
      type: 'overview',
      data: overview
    }));
    
    // Send active agents
    const agents = await this.getActiveAgents();
    subscriber.ws.send(JSON.stringify({
      type: 'agents',
      data: agents
    }));
    
    // Send recent alerts
    const alerts = await this.alertManager.getRecentAlerts();
    subscriber.ws.send(JSON.stringify({
      type: 'alerts',
      data: alerts
    }));
  }
  
  private async startMetricsBroadcast(): Promise<void> {
    setInterval(async () => {
      const metrics = await this.collectRealtimeMetrics();
      
      for (const [id, subscriber] of this.subscribers) {
        if (subscriber.subscriptions.has('metrics')) {
          const filtered = this.applyFilters(metrics, subscriber.filters);
          
          subscriber.ws.send(JSON.stringify({
            type: 'metrics',
            timestamp: Date.now(),
            data: filtered
          }));
        }
      }
    }, 1000); // Broadcast every second
  }
  
  private async collectRealtimeMetrics(): Promise<DashboardMetrics> {
    return {
      system: {
        cpu: await this.getSystemCPU(),
        memory: await this.getSystemMemory(),
        activeAgents: await this.getActiveAgentCount(),
        taskQueue: await this.getQueueDepth()
      },
      agents: await this.getAgentMetrics(),
      performance: {
        throughput: await this.getThroughput(),
        latency: await this.getLatencyStats(),
        errorRate: await this.getErrorRate()
      }
    };
  }
  
  async getAgentMetrics(): Promise<AgentMetrics[]> {
    const agents = await this.getActiveAgents();
    
    return Promise.all(agents.map(async agent => ({
      agentId: agent.id,
      agentType: agent.type,
      status: agent.status,
      metrics: {
        cpu: await this.metricsStore.getLatest(agent.id, 'cpu'),
        memory: await this.metricsStore.getLatest(agent.id, 'memory'),
        tasksProcessed: await this.metricsStore.getCount(agent.id, 'tasks'),
        averageLatency: await this.metricsStore.getAverage(agent.id, 'latency'),
        errorCount: await this.metricsStore.getCount(agent.id, 'errors')
      },
      trends: {
        cpu: await this.calculateTrend(agent.id, 'cpu'),
        memory: await this.calculateTrend(agent.id, 'memory'),
        latency: await this.calculateTrend(agent.id, 'latency')
      }
    })));
  }
}

// Dashboard React Component
export const AgentMonitoringDashboard: React.FC = () => {
  const [metrics, setMetrics] = useState<DashboardMetrics | null>(null);
  const [selectedAgent, setSelectedAgent] = useState<string | null>(null);
  const [timeRange, setTimeRange] = useState<TimeRange>('1h');
  
  useEffect(() => {
    const ws = new WebSocket('ws://localhost:8080');
    
    ws.onmessage = (event) => {
      const message = JSON.parse(event.data);
      
      switch (message.type) {
        case 'metrics':
          setMetrics(message.data);
          break;
      }
    };
    
    // Subscribe to metrics
    ws.onopen = () => {
      ws.send(JSON.stringify({
        type: 'subscribe',
        topics: ['metrics']
      }));
    };
    
    return () => ws.close();
  }, []);
  
  return (
    <div className="dashboard">
      <SystemOverview metrics={metrics?.system} />
      <AgentGrid 
        agents={metrics?.agents || []}
        onSelectAgent={setSelectedAgent}
      />
      {selectedAgent && (
        <AgentDetails 
          agentId={selectedAgent}
          timeRange={timeRange}
        />
      )}
      <PerformanceCharts metrics={metrics?.performance} />
    </div>
  );
};
```

#### SubTask 3.17.3: ì„±ëŠ¥ ë¶„ì„ ë„êµ¬
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/monitoring/performance_analyzer.py
from typing import Dict, List, Optional, Tuple
import numpy as np
from scipy import stats
from sklearn.ensemble import IsolationForest

class PerformanceAnalyzer:
    """Analyzes agent performance metrics"""
    
    def __init__(self):
        self.anomaly_detector = AnomalyDetector()
        self.trend_analyzer = TrendAnalyzer()
        self.bottleneck_detector = BottleneckDetector()
        self.correlation_analyzer = CorrelationAnalyzer()
        
    async def analyze_agent_performance(
        self,
        agent_id: str,
        time_range: TimeRange
    ) -> PerformanceAnalysis:
        """Comprehensive performance analysis for an agent"""
        
        # Get metrics data
        metrics = await self._get_metrics(agent_id, time_range)
        
        # Perform various analyses
        analysis = PerformanceAnalysis(
            agent_id=agent_id,
            time_range=time_range,
            summary=self._calculate_summary_statistics(metrics),
            anomalies=await self.anomaly_detector.detect(metrics),
            trends=await self.trend_analyzer.analyze(metrics),
            bottlenecks=await self.bottleneck_detector.detect(agent_id, metrics),
            correlations=await self.correlation_analyzer.analyze(metrics),
            recommendations=[]
        )
        
        # Generate recommendations
        analysis.recommendations = await self._generate_recommendations(analysis)
        
        return analysis
    
    def _calculate_summary_statistics(
        self,
        metrics: Dict[str, List[MetricPoint]]
    ) -> Dict[str, Statistics]:
        """Calculate summary statistics for each metric"""
        
        summary = {}
        
        for metric_name, points in metrics.items():
            values = [p.value for p in points]
            
            if values:
                summary[metric_name] = Statistics(
                    mean=np.mean(values),
                    median=np.median(values),
                    std=np.std(values),
                    min=np.min(values),
                    max=np.max(values),
                    p95=np.percentile(values, 95),
                    p99=np.percentile(values, 99)
                )
        
        return summary
    
    async def _generate_recommendations(
        self,
        analysis: PerformanceAnalysis
    ) -> List[Recommendation]:
        """Generate performance recommendations"""
        
        recommendations = []
        
        # Check for high CPU usage
        if analysis.summary.get('cpu', {}).get('mean', 0) > 80:
            recommendations.append(Recommendation(
                severity='high',
                category='resource',
                title='High CPU Usage',
                description='Average CPU usage exceeds 80%',
                actions=[
                    'Consider scaling horizontally',
                    'Optimize compute-intensive operations',
                    'Review agent workload distribution'
                ]
            ))
        
        # Check for memory leaks
        memory_trend = analysis.trends.get('memory')
        if memory_trend and memory_trend.slope > 0.1:
            recommendations.append(Recommendation(
                severity='medium',
                category='resource',
                title='Potential Memory Leak',
                description='Memory usage shows increasing trend',
                actions=[
                    'Review object lifecycle management',
                    'Check for circular references',
                    'Implement periodic memory cleanup'
                ]
            ))
        
        # Check for latency spikes
        latency_anomalies = [
            a for a in analysis.anomalies
            if a.metric == 'latency' and a.severity == 'high'
        ]
        if latency_anomalies:
            recommendations.append(Recommendation(
                severity='high',
                category='performance',
                title='Latency Spikes Detected',
                description=f'{len(latency_anomalies)} latency anomalies detected',
                actions=[
                    'Investigate external service dependencies',
                    'Check for resource contention',
                    'Review async operation handling'
                ]
            ))
        
        return recommendations

class AnomalyDetector:
    """Detects anomalies in metrics"""
    
    def __init__(self):
        self.isolation_forest = IsolationForest(
            contamination=0.1,
            random_state=42
        )
        
    async def detect(
        self,
        metrics: Dict[str, List[MetricPoint]]
    ) -> List[Anomaly]:
        """Detect anomalies in metrics"""
        
        anomalies = []
        
        for metric_name, points in metrics.items():
            if len(points) < 10:
                continue
            
            # Prepare data
            values = np.array([[p.value] for p in points])
            timestamps = [p.timestamp for p in points]
            
            # Detect anomalies
            predictions = self.isolation_forest.fit_predict(values)
            
            # Extract anomalies
            for i, pred in enumerate(predictions):
                if pred == -1:  # Anomaly
                    severity = self._calculate_severity(
                        values[i][0],
                        values
                    )
                    
                    anomalies.append(Anomaly(
                        metric=metric_name,
                        timestamp=timestamps[i],
                        value=values[i][0],
                        severity=severity,
                        score=self.isolation_forest.score_samples([values[i]])[0]
                    ))
        
        return anomalies
    
    def _calculate_severity(
        self,
        anomaly_value: float,
        all_values: np.ndarray
    ) -> str:
        """Calculate anomaly severity"""
        
        mean = np.mean(all_values)
        std = np.std(all_values)
        
        z_score = abs((anomaly_value - mean) / std)
        
        if z_score > 4:
            return 'critical'
        elif z_score > 3:
            return 'high'
        elif z_score > 2:
            return 'medium'
        else:
            return 'low'
```

#### SubTask 3.17.4: ì„±ëŠ¥ ìµœì í™” ê¶Œê³  ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/monitoring/optimization_advisor.ts
export class OptimizationAdvisor {
  private readonly analyzer: PerformanceAnalyzer;
  private readonly historicalData: HistoricalDataStore;
  private readonly mlPredictor: MLPredictor;
  
  constructor() {
    this.analyzer = new PerformanceAnalyzer();
    this.historicalData = new HistoricalDataStore();
    this.mlPredictor = new MLPredictor();
  }
  
  async generateOptimizationPlan(
    agentId: string,
    options: OptimizationOptions = {}
  ): Promise<OptimizationPlan> {
    // Analyze current performance
    const currentPerformance = await this.analyzer.analyze(agentId);
    
    // Get historical context
    const historicalContext = await this.historicalData.getContext(
      agentId,
      options.lookbackPeriod || '7d'
    );
    
    // Predict future performance
    const predictions = await this.mlPredictor.predictPerformance(
      agentId,
      currentPerformance,
      historicalContext
    );
    
    // Generate optimization recommendations
    const recommendations = await this.generateRecommendations(
      currentPerformance,
      predictions,
      options
    );
    
    // Create optimization plan
    return {
      agentId,
      currentState: currentPerformance,
      predictedImpact: predictions,
      recommendations,
      implementationSteps: this.createImplementationSteps(recommendations),
      estimatedROI: this.calculateROI(recommendations, predictions)
    };
  }
  
  private async generateRecommendations(
    performance: PerformanceAnalysis,
    predictions: PerformancePredictions,
    options: OptimizationOptions
  ): Promise<OptimizationRecommendation[]> {
    const recommendations: OptimizationRecommendation[] = [];
    
    // Resource optimization
    if (performance.resourceUtilization.cpu > 0.8) {
      recommendations.push({
        id: 'cpu_optimization',
        category: 'resource',
        priority: 'high',
        title: 'CPU Optimization Required',
        description: 'High CPU utilization detected',
        suggestions: [
          {
            action: 'enable_cpu_throttling',
            impact: 'Reduce CPU by 20%',
            effort: 'low',
            config: {
              maxCPU: 0.7,
              burstLimit: 0.9
            }
          },
          {
            action: 'optimize_algorithms',
            impact: 'Reduce CPU by 30%',
            effort: 'medium',
            details: this.identifyHotspots(performance)
          }
        ]
      });
    }
    
    // Concurrency optimization
    const concurrencyAnalysis = this.analyzeConcurrency(performance);
    if (concurrencyAnalysis.suboptimal) {
      recommendations.push({
        id: 'concurrency_optimization',
        category: 'performance',
        priority: 'medium',
        title: 'Suboptimal Concurrency Settings',
        description: concurrencyAnalysis.description,
        suggestions: [
          {
            action: 'adjust_thread_pool',
            impact: 'Improve throughput by 25%',
            effort: 'low',
            config: {
              minThreads: concurrencyAnalysis.optimal.min,
              maxThreads: concurrencyAnalysis.optimal.max
            }
          }
        ]
      });
    }
    
    // Caching optimization
    const cacheAnalysis = await this.analyzeCaching(performance);
    if (cacheAnalysis.hitRate < 0.7) {
      recommendations.push({
        id: 'cache_optimization',
        category: 'performance',
        priority: 'medium',
        title: 'Low Cache Hit Rate',
        description: `Cache hit rate is ${cacheAnalysis.hitRate * 100}%`,
        suggestions: [
          {
            action: 'increase_cache_size',
            impact: 'Reduce latency by 40%',
            effort: 'low',
            config: {
              cacheSize: cacheAnalysis.recommendedSize
            }
          },
          {
            action: 'implement_intelligent_caching',
            impact: 'Improve hit rate to 85%',
            effort: 'high',
            algorithm: 'LRU_with_frequency'
          }
        ]
      });
    }
    
    return recommendations;
  }
  
  private createImplementationSteps(
    recommendations: OptimizationRecommendation[]
  ): ImplementationStep[] {
    const steps: ImplementationStep[] = [];
    
    // Sort by priority and effort
    const sorted = recommendations
      .flatMap(r => r.suggestions.map(s => ({ rec: r, sug: s })))
      .sort((a, b) => {
        const priorityScore = { high: 3, medium: 2, low: 1 };
        const effortScore = { low: 1, medium: 2, high: 3 };
        
        const aScore = priorityScore[a.rec.priority] / effortScore[a.sug.effort];
        const bScore = priorityScore[b.rec.priority] / effortScore[b.sug.effort];
        
        return bScore - aScore;
      });
    
    // Create implementation plan
    for (const { rec, sug } of sorted) {
      steps.push({
        order: steps.length + 1,
        recommendationId: rec.id,
        action: sug.action,
        description: `${rec.title}: ${sug.action}`,
        prerequisites: this.identifyPrerequisites(sug),
        estimatedTime: this.estimateImplementationTime(sug),
        validation: this.createValidationCriteria(sug),
        rollback: this.createRollbackPlan(sug)
      });
    }
    
    return steps;
  }
}
```

### Task 3.18: ì—ì´ì „íŠ¸ ë¡œê¹… ë° ì¶”ì 

#### SubTask 3.18.1: êµ¬ì¡°í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/logging/structured_logger.py
from typing import Dict, Any, Optional, List
import json
import asyncio
from datetime import datetime
from contextvars import ContextVar

# Context variables for tracing
trace_id_var: ContextVar[str] = ContextVar('trace_id', default='')
span_id_var: ContextVar[str] = ContextVar('span_id', default='')

class StructuredLogger:
    """Structured logging for agents"""
    
    def __init__(self, agent_id: str, agent_type: str):
        self.agent_id = agent_id
        self.agent_type = agent_type
        self.log_buffer = []
        self.log_processors: List[LogProcessor] = []
        self.context_extractors: List[ContextExtractor] = []
        
        # Default processors
        self.add_processor(ConsoleLogProcessor())
        self.add_processor(FileLogProcessor(f"logs/{agent_id}.log"))
        self.add_processor(CloudWatchLogProcessor())
        
    def add_processor(self, processor: LogProcessor) -> None:
        """Add a log processor"""
        self.log_processors.append(processor)
    
    def add_context_extractor(self, extractor: ContextExtractor) -> None:
        """Add a context extractor"""
        self.context_extractors.append(extractor)
    
    async def log(
        self,
        level: str,
        message: str,
        **kwargs
    ) -> None:
        """Log a structured message"""
        
        # Create log entry
        entry = LogEntry(
            timestamp=datetime.utcnow(),
            level=level,
            message=message,
            agent_id=self.agent_id,
            agent_type=self.agent_type,
            trace_id=trace_id_var.get(),
            span_id=span_id_var.get(),
            fields=kwargs,
            context=await self._extract_context()
        )
        
        # Add to buffer
        self.log_buffer.append(entry)
        
        # Process immediately for error/critical
        if level in ['ERROR', 'CRITICAL']:
            await self._process_logs()
    
    async def _extract_context(self) -> Dict[str, Any]:
        """Extract context from various sources"""
        
        context = {}
        
        for extractor in self.context_extractors:
            try:
                extracted = await extractor.extract()
                context.update(extracted)
            except Exception as e:
                # Don't let context extraction fail logging
                context[f"{extractor.__class__.__name__}_error"] = str(e)
        
        return context
    
    async def _process_logs(self) -> None:
        """Process buffered logs"""
        
        if not self.log_buffer:
            return
        
        # Copy and clear buffer
        logs_to_process = self.log_buffer.copy()
        self.log_buffer.clear()
        
        # Process through each processor
        for processor in self.log_processors:
            try:
                await processor.process(logs_to_process)
            except Exception as e:
                # Log processor errors to console
                print(f"Log processor error: {e}")
    
    # Convenience methods
    async def debug(self, message: str, **kwargs) -> None:
        await self.log('DEBUG', message, **kwargs)
    
    async def info(self, message: str, **kwargs) -> None:
        await self.log('INFO', message, **kwargs)
    
    async def warning(self, message: str, **kwargs) -> None:
        await self.log('WARNING', message, **kwargs)
    
    async def error(self, message: str, **kwargs) -> None:
        await self.log('ERROR', message, **kwargs)
    
    async def critical(self, message: str, **kwargs) -> None:
        await self.log('CRITICAL', message, **kwargs)

class LogEntry:
    """Structured log entry"""
    
    def __init__(
        self,
        timestamp: datetime,
        level: str,
        message: str,
        agent_id: str,
        agent_type: str,
        trace_id: str,
        span_id: str,
        fields: Dict[str, Any],
        context: Dict[str, Any]
    ):
        self.timestamp = timestamp
        self.level = level
        self.message = message
        self.agent_id = agent_id
        self.agent_type = agent_type
        self.trace_id = trace_id
        self.span_id = span_id
        self.fields = fields
        self.context = context
    
    def to_json(self) -> str:
        """Convert to JSON format"""
        
        return json.dumps({
            '@timestamp': self.timestamp.isoformat(),
            'level': self.level,
            'message': self.message,
            'agent': {
                'id': self.agent_id,
                'type': self.agent_type
            },
            'trace': {
                'trace_id': self.trace_id,
                'span_id': self.span_id
            },
            'fields': self.fields,
            'context': self.context
        })

class CloudWatchLogProcessor(LogProcessor):
    """Process logs to AWS CloudWatch"""
    
    def __init__(self):
        self.client = boto3.client('logs')
        self.log_group = '/aws/agents'
        self.sequence_tokens = {}
        
    async def process(self, entries: List[LogEntry]) -> None:
        """Send logs to CloudWatch"""
        
        # Group by agent ID (stream)
        streams = {}
        for entry in entries:
            stream_name = f"agent/{entry.agent_id}"
            if stream_name not in streams:
                streams[stream_name] = []
            streams[stream_name].append(entry)
        
        # Send to CloudWatch
        for stream_name, stream_entries in streams.items():
            await self._send_to_stream(stream_name, stream_entries)
    
    async def _send_to_stream(
        self,
        stream_name: str,
        entries: List[LogEntry]
    ) -> None:
        """Send logs to a specific stream"""
        
        # Ensure stream exists
        await self._ensure_stream_exists(stream_name)
        
        # Prepare log events
        log_events = [
            {
                'timestamp': int(entry.timestamp.timestamp() * 1000),
                'message': entry.to_json()
            }
            for entry in entries
        ]
        
        # Send logs
        params = {
            'logGroupName': self.log_group,
            'logStreamName': stream_name,
            'logEvents': log_events
        }
        
        # Add sequence token if available
        if stream_name in self.sequence_tokens:
            params['sequenceToken'] = self.sequence_tokens[stream_name]
        
        response = await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: self.client.put_log_events(**params)
        )
        
        # Update sequence token
        self.sequence_tokens[stream_name] = response.get('nextSequenceToken')
```

#### SubTask 3.18.2: ë¶„ì‚° ì¶”ì  ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/tracing/distributed_tracer.ts
export class DistributedTracer {
  private readonly spans: Map<string, Span> = new Map();
  private readonly exporter: SpanExporter;
  private readonly sampler: Sampler;
  
  constructor(config: TracerConfig) {
    this.exporter = this.createExporter(config);
    this.sampler = new ProbabilitySampler(config.samplingRate || 0.1);
  }
  
  createSpan(options: SpanOptions): Span {
    // Check sampling decision
    const sampled = this.sampler.shouldSample(
      options.traceId || this.generateTraceId()
    );
    
    if (!sampled && !options.forceSample) {
      return new NoOpSpan();
    }
    
    const span = new Span({
      spanId: this.generateSpanId(),
      traceId: options.traceId || this.generateTraceId(),
      parentSpanId: options.parentSpanId,
      operationName: options.name,
      startTime: Date.now(),
      tags: {
        'agent.id': options.agentId,
        'agent.type': options.agentType,
        ...options.tags
      },
      baggage: options.baggage || {}
    });
    
    this.spans.set(span.spanId, span);
    
    return span;
  }
  
  async startSpan(
    name: string,
    options: SpanOptions = {}
  ): Promise<Span> {
    const span = this.createSpan({ name, ...options });
    
    // Set as current span in context
    setCurrentSpan(span);
    
    // Log span start
    span.log({
      event: 'span_started',
      timestamp: Date.now()
    });
    
    return span;
  }
  
  async finishSpan(span: Span): Promise<void> {
    if (span instanceof NoOpSpan) return;
    
    span.finish();
    
    // Export span
    await this.exporter.export([span]);
    
    // Remove from active spans
    this.spans.delete(span.spanId);
  }
  
  inject(span: Span, carrier: any): void {
    // Inject trace context into carrier (e.g., HTTP headers)
    carrier['X-Trace-Id'] = span.traceId;
    carrier['X-Span-Id'] = span.spanId;
    carrier['X-Sampled'] = span.sampled ? '1' : '0';
    
    // Inject baggage
    for (const [key, value] of Object.entries(span.baggage)) {
      carrier[`X-Baggage-${key}`] = value;
    }
  }
  
  extract(carrier: any): SpanContext | null {
    // Extract trace context from carrier
    const traceId = carrier['X-Trace-Id'];
    const spanId = carrier['X-Span-Id'];
    const sampled = carrier['X-Sampled'] === '1';
    
    if (!traceId || !spanId) {
      return null;
    }
    
    // Extract baggage
    const baggage: Record<string, string> = {};
    for (const [key, value] of Object.entries(carrier)) {
      if (key.startsWith('X-Baggage-')) {
        const baggageKey = key.substring('X-Baggage-'.length);
        baggage[baggageKey] = value as string;
      }
    }
    
    return {
      traceId,
      spanId,
      sampled,
      baggage
    };
  }
}

export class Span {
  public readonly spanId: string;
  public readonly traceId: string;
  public readonly parentSpanId?: string;
  public readonly operationName: string;
  public readonly startTime: number;
  public endTime?: number;
  public readonly tags: Record<string, any>;
  public readonly logs: LogEntry[] = [];
  public readonly baggage: Record<string, string>;
  public sampled: boolean = true;
  
  constructor(options: SpanData) {
    this.spanId = options.spanId;
    this.traceId = options.traceId;
    this.parentSpanId = options.parentSpanId;
    this.operationName = options.operationName;
    this.startTime = options.startTime;
    this.tags = options.tags;
    this.baggage = options.baggage;
  }
  
  setTag(key: string, value: any): void {
    this.tags[key] = value;
  }
  
  log(fields: Record<string, any>): void {
    this.logs.push({
      timestamp: Date.now(),
      fields
    });
  }
  
  setBaggageItem(key: string, value: string): void {
    this.baggage[key] = value;
  }
  
  finish(): void {
    this.endTime = Date.now();
    
    // Calculate duration
    this.setTag('duration', this.endTime - this.startTime);
  }
  
  toJSON(): SpanData {
    return {
      spanId: this.spanId,
      traceId: this.traceId,
      parentSpanId: this.parentSpanId,
      operationName: this.operationName,
      startTime: this.startTime,
      endTime: this.endTime,
      tags: this.tags,
      logs: this.logs,
      baggage: this.baggage
    };
  }
}

// Trace context propagation
export class TraceContextPropagator {
  async propagateToAgent(
    span: Span,
    targetAgent: string,
    message: any
  ): Promise<void> {
    // Create child span
    const childSpan = tracer.createSpan({
      name: `call_agent_${targetAgent}`,
      traceId: span.traceId,
      parentSpanId: span.spanId,
      tags: {
        'target.agent': targetAgent,
        'message.type': message.type
      }
    });
    
    // Inject trace context into message
    message._trace = {
      traceId: childSpan.traceId,
      spanId: childSpan.spanId,
      baggage: childSpan.baggage
    };
    
    // Set deadline if specified
    if (span.baggage.deadline) {
      message._deadline = span.baggage.deadline;
    }
  }
}
```

#### SubTask 3.18.3: ë¡œê·¸ ì§‘ê³„ ë° ë¶„ì„
**ë‹´ë‹¹ì**: ë°ì´í„° ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/logging/log_aggregator.py
from typing import Dict, List, Any, Optional
import asyncio
from datetime import datetime, timedelta
from collections import defaultdict

class LogAggregator:
    """Aggregates and analyzes logs from multiple agents"""
    
    def __init__(self):
        self.elasticsearch = AsyncElasticsearch()
        self.aggregation_rules: List[AggregationRule] = []
        self.alert_rules: List[AlertRule] = []
        self.pattern_matcher = LogPatternMatcher()
        
    async def aggregate_logs(
        self,
        time_range: TimeRange,
        filters: Optional[Dict[str, Any]] = None
    ) -> AggregationResult:
        """Aggregate logs for analysis"""
        
        # Build query
        query = self._build_query(time_range, filters)
        
        # Execute aggregation
        response = await self.elasticsearch.search(
            index="agent-logs-*",
            body={
                "size": 0,
                "query": query,
                "aggs": {
                    "by_agent": {
                        "terms": {
                            "field": "agent.id",
                            "size": 1000
                        },
                        "aggs": {
                            "by_level": {
                                "terms": {
                                    "field": "level"
                                }
                            },
                            "over_time": {
                                "date_histogram": {
                                    "field": "@timestamp",
                                    "fixed_interval": "1m"
                                }
                            },
                            "error_messages": {
                                "terms": {
                                    "field": "message",
                                    "size": 100,
                                    "include": {
                                        "partition": 0,
                                        "num_partitions": 1
                                    }
                                }
                            }
                        }
                    },
                    "by_pattern": {
                        "significant_text": {
                            "field": "message",
                            "size": 50
                        }
                    }
                }
            }
        )
        
        # Process results
        return await self._process_aggregation_results(response)
    
    async def analyze_error_patterns(
        self,
        agent_id: Optional[str] = None,
        time_range: Optional[TimeRange] = None
    ) -> ErrorPatternAnalysis:
        """Analyze error patterns in logs"""
        
        # Get error logs
        error_logs = await self._get_error_logs(agent_id, time_range)
        
        # Extract patterns
        patterns = await self.pattern_matcher.extract_patterns(error_logs)
        
        # Cluster similar errors
        clusters = await self._cluster_errors(patterns)
        
        # Analyze trends
        trends = await self._analyze_error_trends(clusters, time_range)
        
        return ErrorPatternAnalysis(
            total_errors=len(error_logs),
            unique_patterns=len(patterns),
            error_clusters=clusters,
            trends=trends,
            top_errors=self._get_top_errors(clusters),
            recommendations=await self._generate_error_recommendations(clusters)
        )
    
    async def _cluster_errors(
        self,
        patterns: List[LogPattern]
    ) -> List[ErrorCluster]:
        """Cluster similar error patterns"""
        
        clusters = []
        clustered_patterns = set()
        
        for pattern in patterns:
            if pattern.id in clustered_patterns:
                continue
            
            # Find similar patterns
            cluster = ErrorCluster(
                id=f"cluster_{len(clusters)}",
                representative_pattern=pattern,
                patterns=[pattern],
                count=pattern.count
            )
            
            clustered_patterns.add(pattern.id)
            
            # Add similar patterns to cluster
            for other in patterns:
                if other.id not in clustered_patterns:
                    similarity = self._calculate_similarity(pattern, other)
                    if similarity > 0.8:
                        cluster.patterns.append(other)
                        cluster.count += other.count
                        clustered_patterns.add(other.id)
            
            clusters.append(cluster)
        
        # Sort by frequency
        clusters.sort(key=lambda c: c.count, reverse=True)
        
        return clusters
    
    async def create_log_dashboard(
        self,
        dashboard_config: DashboardConfig
    ) -> LogDashboard:
        """Create a log analysis dashboard"""
        
        dashboard = LogDashboard(
            id=dashboard_config.id,
            name=dashboard_config.name,
            refresh_interval=dashboard_config.refresh_interval
        )
        
        # Add widgets based on configuration
        for widget_config in dashboard_config.widgets:
            widget = await self._create_widget(widget_config)
            dashboard.add_widget(widget)
        
        # Set up real-time updates
        if dashboard_config.realtime:
            await self._setup_realtime_updates(dashboard)
        
        return dashboard
    
    async def _create_widget(
        self,
        config: WidgetConfig
    ) -> DashboardWidget:
        """Create a dashboard widget"""
        
        if config.type == 'log_stream':
            return LogStreamWidget(
                title=config.title,
                filters=config.filters,
                max_entries=config.get('max_entries', 100)
            )
            
        elif config.type == 'error_rate':
            return ErrorRateWidget(
                title=config.title,
                agent_filter=config.get('agent_filter'),
                time_window=config.get('time_window', '5m')
            )
            
        elif config.type == 'log_volume':
            return LogVolumeWidget(
                title=config.title,
                group_by=config.get('group_by', 'agent'),
                interval=config.get('interval', '1m')
            )
            
        elif config.type == 'pattern_analysis':
            return PatternAnalysisWidget(
                title=config.title,
                pattern_type=config.get('pattern_type', 'error'),
                min_occurrences=config.get('min_occurrences', 5)
            )
        
        else:
            raise ValueError(f"Unknown widget type: {config.type}")
```

#### SubTask 3.18.4: ë¡œê·¸ ë³´ì¡´ ë° ì•„ì¹´ì´ë¹™
**ë‹´ë‹¹ì**: ë°ì´í„° ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/logging/log_archiver.ts
export class LogArchiver {
  private readonly storage: ArchiveStorage;
  private readonly compressionService: CompressionService;
  private readonly retentionPolicy: RetentionPolicy;
  
  constructor(config: ArchiverConfig) {
    this.storage = new S3ArchiveStorage(config.s3Config);
    this.compressionService = new CompressionService();
    this.retentionPolicy = new RetentionPolicy(config.retention);
  }
  
  async archiveLogs(
    startDate: Date,
    endDate: Date,
    options: ArchiveOptions = {}
  ): Promise<ArchiveResult> {
    const result = new ArchiveResult();
    
    // Get logs to archive
    const logs = await this.getLogsForArchiving(startDate, endDate);
    
    // Group logs by agent and date
    const grouped = this.groupLogs(logs);
    
    // Archive each group
    for (const [key, groupLogs] of grouped) {
      try {
        const archiveInfo = await this.archiveLogGroup(
          key,
          groupLogs,
          options
        );
        result.archived.push(archiveInfo);
      } catch (error) {
        result.errors.push({
          group: key,
          error: error.message
        });
      }
    }
    
    // Apply retention policy
    if (options.applyRetention) {
      await this.applyRetentionPolicy();
    }
    
    return result;
  }
  
  private async archiveLogGroup(
    groupKey: string,
    logs: LogEntry[],
    options: ArchiveOptions
  ): Promise<ArchiveInfo> {
    // Prepare archive data
    const archiveData = {
      metadata: {
        groupKey,
        logCount: logs.length,
        dateRange: {
          start: logs[0].timestamp,
          end: logs[logs.length - 1].timestamp
        },
        agentInfo: this.extractAgentInfo(logs),
        statistics: this.calculateStatistics(logs)
      },
      logs: logs
    };
    
    // Convert to format
    let data: Buffer;
    switch (options.format || 'json') {
      case 'json':
        data = Buffer.from(JSON.stringify(archiveData));
        break;
        
      case 'parquet':
        data = await this.convertToParquet(archiveData);
        break;
        
      case 'avro':
        data = await this.convertToAvro(archiveData);
        break;
        
      default:
        throw new Error(`Unsupported format: ${options.format}`);
    }
    
    // Compress if requested
    if (options.compress) {
      data = await this.compressionService.compress(data, {
        algorithm: options.compressionAlgorithm || 'gzip',
        level: options.compressionLevel || 6
      });
    }
    
    // Generate archive path
    const archivePath = this.generateArchivePath(groupKey, options);
    
    // Upload to storage
    await this.storage.upload(archivePath, data, {
      metadata: archiveData.metadata,
      encryption: options.encryption
    });
    
    // Create index entry
    await this.createIndexEntry(archivePath, archiveData.metadata);
    
    return {
      path: archivePath,
      size: data.length,
      logCount: logs.length,
      compressed: options.compress || false,
      format: options.format || 'json',
      metadata: archiveData.metadata
    };
  }
  
  async applyRetentionPolicy(): Promise<RetentionResult> {
    const result = new RetentionResult();
    
    // Get all archived files
    const files = await this.storage.listFiles();
    
    for (const file of files) {
      const shouldDelete = await this.retentionPolicy.shouldDelete(file);
      
      if (shouldDelete) {
        try {
          // Delete from storage
          await this.storage.delete(file.path);
          
          // Remove index entry
          await this.removeIndexEntry(file.path);
          
          result.deleted.push(file.path);
          result.freedSpace += file.size;
        } catch (error) {
          result.errors.push({
            path: file.path,
            error: error.message
          });
        }
      }
    }
    
    return result;
  }
  
  async searchArchives(
    query: ArchiveQuery
  ): Promise<ArchiveSearchResult> {
    // Search in index first
    const indexResults = await this.searchIndex(query);
    
    if (!query.deepSearch || indexResults.length === 0) {
      return {
        results: indexResults,
        searched: 'index_only'
      };
    }
    
    // Deep search in archived files
    const deepResults: ArchiveSearchHit[] = [];
    
    for (const indexResult of indexResults) {
      // Download and search archive
      const hits = await this.searchInArchive(
        indexResult.path,
        query
      );
      deepResults.push(...hits);
    }
    
    return {
      results: deepResults,
      searched: 'deep',
      archivesSearched: indexResults.length
    };
  }
  
  private async searchInArchive(
    archivePath: string,
    query: ArchiveQuery
  ): Promise<ArchiveSearchHit[]> {
    // Download archive
    const data = await this.storage.download(archivePath);
    
    // Decompress if needed
    const decompressed = await this.compressionService.decompress(data);
    
    // Parse archive
    const archive = JSON.parse(decompressed.toString());
    
    // Search logs
    const hits: ArchiveSearchHit[] = [];
    
    for (const log of archive.logs) {
      if (this.matchesQuery(log, query)) {
        hits.push({
          archivePath,
          log,
          score: this.calculateRelevance(log, query)
        });
      }
    }
    
    return hits;
  }
}
```

### Task 3.19: ì—ì´ì „íŠ¸ ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ

#### SubTask 3.19.1: ë²„ì „ ê´€ë¦¬ í”„ë ˆì„ì›Œí¬
**ë‹´ë‹¹ì**: ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/versioning/version_manager.py
from typing import Dict, List, Optional, Tuple
import semver
from dataclasses import dataclass
from datetime import datetime

@dataclass
class AgentVersion:
    version: str
    agent_type: str
    release_date: datetime
    changelog: List[str]
    breaking_changes: List[str]
    dependencies: Dict[str, str]
    compatibility: Dict[str, str]
    deprecated_features: List[str]
    checksum: str

class VersionManager:
    """Manages agent versions and compatibility"""
    
    def __init__(self):
        self.versions: Dict[str, List[AgentVersion]] = {}
        self.compatibility_matrix = CompatibilityMatrix()
        self.migration_manager = MigrationManager()
        self.version_validator = VersionValidator()
        
    async def register_version(
        self,
        agent_type: str,
        version: str,
        metadata: VersionMetadata
    ) -> AgentVersion:
        """Register a new agent version"""
        
        # Validate version format
        if not self.version_validator.is_valid(version):
            raise ValueError(f"Invalid version format: {version}")
        
        # Check version conflicts
        if await self._version_exists(agent_type, version):
            raise ValueError(f"Version {version} already exists for {agent_type}")
        
        # Create version object
        agent_version = AgentVersion(
            version=version,
            agent_type=agent_type,
            release_date=datetime.utcnow(),
            changelog=metadata.changelog,
            breaking_changes=metadata.breaking_changes,
            dependencies=metadata.dependencies,
            compatibility=metadata.compatibility,
            deprecated_features=metadata.deprecated_features,
            checksum=await self._calculate_checksum(agent_type, version)
        )
        
        # Validate compatibility
        await self._validate_compatibility(agent_version)
        
        # Store version
        if agent_type not in self.versions:
            self.versions[agent_type] = []
        
        self.versions[agent_type].append(agent_version)
        self.versions[agent_type].sort(
            key=lambda v: semver.VersionInfo.parse(v.version),
            reverse=True
        )
        
        # Update compatibility matrix
        await self.compatibility_matrix.update(agent_version)
        
        # Generate migration if needed
        if metadata.breaking_changes:
            await self.migration_manager.generate_migration(
                agent_type,
                self._get_previous_version(agent_type, version),
                version,
                metadata.breaking_changes
            )
        
        return agent_version
    
    async def get_compatible_versions(
        self,
        agent_type: str,
        target_version: str,
        dependency_type: str
    ) -> List[str]:
        """Get compatible versions for a dependency"""
        
        return await self.compatibility_matrix.get_compatible_versions(
            agent_type,
            target_version,
            dependency_type
        )
    
    async def check_compatibility(
        self,
        agents: List[Tuple[str, str]]
    ) -> CompatibilityReport:
        """Check compatibility between agent versions"""
        
        report = CompatibilityReport()
        
        # Check each pair
        for i, (type1, ver1) in enumerate(agents):
            for j, (type2, ver2) in enumerate(agents[i+1:], i+1):
                result = await self.compatibility_matrix.check_compatibility(
                    type1, ver1, type2, ver2
                )
                
                if not result.compatible:
                    report.add_issue(
                        f"{type1}@{ver1} incompatible with {type2}@{ver2}",
                        result.reason,
                        result.severity
                    )
        
        return report
    
    async def upgrade_agent(
        self,
        agent_type: str,
        current_version: str,
        target_version: str
    ) -> UpgradePlan:
        """Create an upgrade plan for an agent"""
        
        # Validate versions
        current = semver.VersionInfo.parse(current_version)
        target = semver.VersionInfo.parse(target_version)
        
        if current >= target:
            raise ValueError(f"Target version must be newer than current")
        
        # Find upgrade path
        path = await self._find_upgrade_path(
            agent_type,
            current_version,
            target_version
        )
        
        # Create upgrade plan
        plan = UpgradePlan(
            agent_type=agent_type,
            current_version=current_version,
            target_version=target_version,
            steps=[]
        )
        
        # Add upgrade steps
        for i in range(len(path) - 1):
            from_ver = path[i]
            to_ver = path[i + 1]
            
            # Get version info
            version_info = self._get_version_info(agent_type, to_ver)
            
            # Check for breaking changes
            if version_info.breaking_changes:
                # Add migration step
                migration = await self.migration_manager.get_migration(
                    agent_type,
                    from_ver,
                    to_ver
                )
                
                plan.steps.append(UpgradeStep(
                    from_version=from_ver,
                    to_version=to_ver,
                    type='migration',
                    migration=migration,
                    breaking_changes=version_info.breaking_changes
                ))
            
            # Add upgrade step
            plan.steps.append(UpgradeStep(
                from_version=from_ver,
                to_version=to_ver,
                type='upgrade',
                changelog=version_info.changelog,
                validation_tests=await self._get_validation_tests(
                    agent_type,
                    to_ver
                )
            ))
        
        return plan
```

#### SubTask 3.19.2: ë²„ì „ ë°°í¬ ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: DevOps ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/versioning/deployment_system.ts
export class VersionDeploymentSystem {
  private readonly deploymentStrategies: Map<string, DeploymentStrategy>;
  private readonly versionRegistry: VersionRegistry;
  private readonly healthChecker: HealthChecker;
  private readonly rollbackManager: RollbackManager;
  
  constructor() {
    this.deploymentStrategies = new Map([
      ['blue_green', new BlueGreenDeployment()],
      ['canary', new CanaryDeployment()],
      ['rolling', new RollingDeployment()],
      ['shadow', new ShadowDeployment()]
    ]);
    
    this.versionRegistry = new VersionRegistry();
    this.healthChecker = new HealthChecker();
    this.rollbackManager = new RollbackManager();
  }
  
  async deployVersion(
    agentType: string,
    version: string,
    options: DeploymentOptions
  ): Promise<DeploymentResult> {
    // Validate deployment
    await this.validateDeployment(agentType, version, options);
    
    // Get deployment strategy
    const strategy = this.deploymentStrategies.get(
      options.strategy || 'rolling'
    );
    
    if (!strategy) {
      throw new Error(`Unknown deployment strategy: ${options.strategy}`);
    }
    
    // Create deployment plan
    const plan = await this.createDeploymentPlan(
      agentType,
      version,
      options
    );
    
    // Execute deployment
    const deployment = new Deployment({
      id: uuid(),
      agentType,
      version,
      strategy: options.strategy,
      startTime: new Date(),
      plan
    });
    
    try {
      // Pre-deployment checks
      await this.runPreDeploymentChecks(deployment);
      
      // Execute strategy
      await strategy.deploy(deployment, {
        onProgress: (progress) => this.updateProgress(deployment, progress),
        onHealthCheck: (instance) => this.healthChecker.check(instance),
        onError: (error) => this.handleDeploymentError(deployment, error)
      });
      
      // Post-deployment validation
      await this.runPostDeploymentValidation(deployment);
      
      // Finalize deployment
      deployment.status = 'completed';
      deployment.endTime = new Date();
      
      return {
        success: true,
        deployment,
        metrics: await this.collectDeploymentMetrics(deployment)
      };
      
    } catch (error) {
      // Rollback on failure
      await this.rollbackDeployment(deployment, error);
      
      throw error;
    }
  }
  
  private async createDeploymentPlan(
    agentType: string,
    version: string,
    options: DeploymentOptions
  ): Promise<DeploymentPlan> {
    // Get current instances
    const instances = await this.getAgentInstances(agentType);
    
    // Create deployment groups
    const groups = this.createDeploymentGroups(instances, options);
    
    // Create plan
    return {
      totalInstances: instances.length,
      groups,
      phases: this.createDeploymentPhases(groups, options),
      healthChecks: this.createHealthCheckPlan(options),
      rollbackTriggers: this.createRollbackTriggers(options)
    };
  }
  
  async rollbackDeployment(
    deployment: Deployment,
    reason: any
  ): Promise<void> {
    deployment.status = 'rolling_back';
    deployment.rollbackReason = reason;
    
    try {
      // Execute rollback
      await this.rollbackManager.rollback(deployment);
      
      deployment.status = 'rolled_back';
    } catch (rollbackError) {
      deployment.status = 'rollback_failed';
      deployment.rollbackError = rollbackError;
      
      // Alert on rollback failure
      await this.alertOnRollbackFailure(deployment, rollbackError);
    }
  }
}

export class CanaryDeployment implements DeploymentStrategy {
  async deploy(
    deployment: Deployment,
    callbacks: DeploymentCallbacks
  ): Promise<void> {
    const plan = deployment.plan;
    
    // Phase 1: Deploy to canary instances (e.g., 5%)
    const canaryGroup = plan.groups[0];
    await this.deployToGroup(canaryGroup, deployment.version, callbacks);
    
    // Monitor canary
    const canaryMetrics = await this.monitorCanary(
      canaryGroup,
      deployment.canaryDuration || 300000 // 5 minutes
    );
    
    // Analyze canary results
    const analysis = await this.analyzeCanaryMetrics(canaryMetrics);
    
    if (!analysis.healthy) {
      throw new CanaryFailureError(
        'Canary deployment failed health checks',
        analysis
      );
    }
    
    // Phase 2: Gradual rollout
    for (let i = 1; i < plan.groups.length; i++) {
      const group = plan.groups[i];
      
      await this.deployToGroup(group, deployment.version, callbacks);
      
      // Wait and monitor
      await this.waitAndMonitor(group, deployment.monitoringInterval);
      
      // Update progress
      await callbacks.onProgress({
        phase: `group_${i}`,
        completed: (i + 1) / plan.groups.length,
        instances: group.instances.length
      });
    }
  }
  
  private async monitorCanary(
    group: DeploymentGroup,
    duration: number
  ): Promise<CanaryMetrics> {
    const startTime = Date.now();
    const metrics: CanaryMetrics = {
      requests: 0,
      errors: 0,
      latencies: [],
      healthChecks: []
    };
    
    while (Date.now() - startTime < duration) {
      // Collect metrics
      const instanceMetrics = await Promise.all(
        group.instances.map(inst => this.collectInstanceMetrics(inst))
      );
      
      // Aggregate
      for (const im of instanceMetrics) {
        metrics.requests += im.requests;
        metrics.errors += im.errors;
        metrics.latencies.push(...im.latencies);
      }
      
      // Health check
      const healthResults = await Promise.all(
        group.instances.map(inst => this.healthCheck(inst))
      );
      
      metrics.healthChecks.push({
        timestamp: Date.now(),
        results: healthResults
      });
      
      await new Promise(resolve => setTimeout(resolve, 5000)); // 5s interval
    }
    
    return metrics;
  }
}
```

#### SubTask 3.19.3: ë²„ì „ í˜¸í™˜ì„± ê´€ë¦¬
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/versioning/compatibility_manager.py
from typing import Dict, List, Set, Optional, Tuple
import networkx as nx
from dataclasses import dataclass

@dataclass
class CompatibilityRule:
    source_type: str
    source_version_pattern: str
    target_type: str
    target_version_pattern: str
    compatible: bool
    reason: Optional[str] = None
    migration_required: bool = False

class CompatibilityMatrix:
    """Manages version compatibility between agents"""
    
    def __init__(self):
        self.compatibility_graph = nx.DiGraph()
        self.rules: List[CompatibilityRule] = []
        self.compatibility_cache: Dict[str, bool] = {}
        
    def add_compatibility_rule(self, rule: CompatibilityRule) -> None:
        """Add a compatibility rule"""
        self.rules.append(rule)
        
        # Clear cache as rules have changed
        self.compatibility_cache.clear()
    
    async def check_compatibility(
        self,
        agent1_type: str,
        agent1_version: str,
        agent2_type: str,
        agent2_version: str
    ) -> CompatibilityResult:
        """Check if two agent versions are compatible"""
        
        # Check cache
        cache_key = f"{agent1_type}@{agent1_version}<->{agent2_type}@{agent2_version}"
        if cache_key in self.compatibility_cache:
            return self.compatibility_cache[cache_key]
        
        # Apply rules
        for rule in self.rules:
            if self._matches_rule(
                rule,
                agent1_type,
                agent1_version,
                agent2_type,
                agent2_version
            ):
                result = CompatibilityResult(
                    compatible=rule.compatible,
                    reason=rule.reason,
                    migration_required=rule.migration_required
                )
                
                # Cache result
                self.compatibility_cache[cache_key] = result
                
                return result
        
        # No specific rule found, check general compatibility
        result = await self._check_general_compatibility(
            agent1_type,
            agent1_version,
            agent2_type,
            agent2_version
        )
        
        # Cache result
        self.compatibility_cache[cache_key] = result
        
        return result
    
    async def find_compatible_versions(
        self,
        target_agents: List[Tuple[str, str]],
        candidate_type: str
    ) -> List[str]:
        """Find compatible versions of a candidate agent"""
        
        # Get all versions of candidate
        all_versions = await self._get_all_versions(candidate_type)
        compatible_versions = []
        
        for version in all_versions:
            # Check compatibility with all targets
            compatible_with_all = True
            
            for target_type, target_version in target_agents:
                result = await self.check_compatibility(
                    candidate_type,
                    version,
                    target_type,
                    target_version
                )
                
                if not result.compatible:
                    compatible_with_all = False
                    break
            
            if compatible_with_all:
                compatible_versions.append(version)
        
        return compatible_versions
    
    async def build_compatibility_graph(
        self,
        agents: List[Tuple[str, str]]
    ) -> nx.DiGraph:
        """Build a compatibility graph for visualization"""
        
        graph = nx.DiGraph()
        
        # Add nodes
        for agent_type, version in agents:
            node_id = f"{agent_type}@{version}"
            graph.add_node(
                node_id,
                agent_type=agent_type,
                version=version
            )
        
        # Add edges for compatibility
        for i, (type1, ver1) in enumerate(agents):
            for j, (type2, ver2) in enumerate(agents):
                if i != j:
                    result = await self.check_compatibility(
                        type1, ver1, type2, ver2
                    )
                    
                    if result.compatible:
                        graph.add_edge(
                            f"{type1}@{ver1}",
                            f"{type2}@{ver2}",
                            compatible=True,
                            migration_required=result.migration_required
                        )
        
        return graph
    
    def _matches_rule(
        self,
        rule: CompatibilityRule,
        agent1_type: str,
        agent1_version: str,
        agent2_type: str,
        agent2_version: str
    ) -> bool:
        """Check if a rule matches the agent pair"""
        
        # Check agent types
        if rule.source_type != agent1_type or rule.target_type != agent2_type:
            # Also check reverse
            if rule.source_type != agent2_type or rule.target_type != agent1_type:
                return False
        
        # Check version patterns
        if not self._matches_version_pattern(
            agent1_version,
            rule.source_version_pattern
        ):
            return False
        
        if not self._matches_version_pattern(
            agent2_version,
            rule.target_version_pattern
        ):
            return False
        
        return True
```

#### SubTask 3.19.4: ë²„ì „ ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/versioning/migration_tools.ts
export class MigrationManager {
  private readonly migrations: Map<string, Migration> = new Map();
  private readonly validator: MigrationValidator;
  private readonly executor: MigrationExecutor;
  
  constructor() {
    this.validator = new MigrationValidator();
    this.executor = new MigrationExecutor();
  }
  
  async createMigration(
    agentType: string,
    fromVersion: string,
    toVersion: string,
    changes: BreakingChange[]
  ): Promise<Migration> {
    const migration = new Migration({
      id: `${agentType}_${fromVersion}_to_${toVersion}`,
      agentType,
      fromVersion,
      toVersion,
      steps: await this.generateMigrationSteps(changes),
      validators: await this.createValidators(changes),
      rollbackSteps: await this.generateRollbackSteps(changes)
    });
    
    // Validate migration
    await this.validator.validate(migration);
    
    // Store migration
    this.migrations.set(migration.id, migration);
    
    return migration;
  }
  
  private async generateMigrationSteps(
    changes: BreakingChange[]
  ): Promise<MigrationStep[]> {
    const steps: MigrationStep[] = [];
    
    for (const change of changes) {
      switch (change.type) {
        case 'schema_change':
          steps.push(await this.createSchemaStep(change));
          break;
          
        case 'api_change':
          steps.push(await this.createAPIStep(change));
          break;
          
        case 'config_change':
          steps.push(await this.createConfigStep(change));
          break;
          
        case 'behavior_change':
          steps.push(await this.createBehaviorStep(change));
          break;
      }
    }
    
    return steps;
  }
  
  async executeMigration(
    migrationId: string,
    target: MigrationTarget,
    options: MigrationOptions = {}
  ): Promise<MigrationResult> {
    const migration = this.migrations.get(migrationId);
    if (!migration) {
      throw new Error(`Migration ${migrationId} not found`);
    }
    
    const execution = new MigrationExecution({
      migration,
      target,
      options,
      startTime: new Date()
    });
    
    try {
      // Pre-migration backup
      if (options.backup) {
        await this.createBackup(target, execution);
      }
      
      // Execute migration steps
      for (const step of migration.steps) {
        await this.executeStep(step, target, execution);
      }
      
      // Validate migration
      await this.validateMigration(migration, target);
      
      execution.status = 'completed';
      execution.endTime = new Date();
      
      return {
        success: true,
        execution,
        validation: await this.generateValidationReport(migration, target)
      };
      
    } catch (error) {
      // Rollback on failure
      if (options.autoRollback) {
        await this.rollbackMigration(execution, error);
      }
      
      throw error;
    }
  }
  
  private async executeStep(
    step: MigrationStep,
    target: MigrationTarget,
    execution: MigrationExecution
  ): Promise<void> {
    execution.currentStep = step.id;
    
    // Log step start
    await this.logStepStart(step, execution);
    
    try {
      // Execute based on step type
      switch (step.type) {
        case 'data_transformation':
          await this.executeDataTransformation(step, target);
          break;
          
        case 'schema_migration':
          await this.executeSchemaMigration(step, target);
          break;
          
        case 'code_update':
          await this.executeCodeUpdate(step, target);
          break;
          
        case 'config_update':
          await this.executeConfigUpdate(step, target);
          break;
      }
      
      // Verify step
      if (step.verification) {
        await step.verification(target);
      }
      
      // Log step completion
      await this.logStepComplete(step, execution);
      
    } catch (error) {
      await this.logStepError(step, execution, error);
      throw error;
    }
  }
}

export class MigrationValidator {
  async validate(migration: Migration): Promise<ValidationResult> {
    const result = new ValidationResult();
    
    // Validate migration structure
    this.validateStructure(migration, result);
    
    // Validate steps
    for (const step of migration.steps) {
      await this.validateStep(step, result);
    }
    
    // Validate rollback capability
    this.validateRollback(migration, result);
    
    // Test migration in sandbox
    if (result.isValid()) {
      const sandboxResult = await this.testInSandbox(migration);
      if (!sandboxResult.success) {
        result.addError('Sandbox test failed', sandboxResult.error);
      }
    }
    
    return result;
  }
  
  private async testInSandbox(
    migration: Migration
  ): Promise<SandboxTestResult> {
    // Create sandbox environment
    const sandbox = await this.createSandbox(migration);
    
    try {
      // Set up test data
      await sandbox.setupTestData();
      
      // Execute migration
      await sandbox.executeMigration(migration);
      
      // Verify results
      const verification = await sandbox.verifyResults();
      
      return {
        success: verification.passed,
        error: verification.error,
        metrics: sandbox.getMetrics()
      };
      
    } finally {
      // Clean up sandbox
      await sandbox.cleanup();
    }
  }
}
```

### Task 3.20: ì—ì´ì „íŠ¸ ì„±ëŠ¥ ìµœì í™”

#### SubTask 3.20.1: ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ë„êµ¬
**ë‹´ë‹¹ì**: ì„±ëŠ¥ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/optimization/performance_profiler.py
from typing import Dict, List, Any, Optional
import cProfile
import pstats
import asyncio
import tracemalloc
from dataclasses import dataclass

@dataclass
class ProfilingResult:
    agent_id: str
    profile_type: str
    duration: float
    cpu_time: float
    memory_peak: int
    function_stats: List[FunctionStat]
    memory_allocations: List[MemoryAllocation]
    bottlenecks: List[Bottleneck]

class PerformanceProfiler:
    """Profile agent performance"""
    
    def __init__(self):
        self.cpu_profiler = CPUProfiler()
        self.memory_profiler = MemoryProfiler()
        self.async_profiler = AsyncProfiler()
        self.io_profiler = IOProfiler()
        
    async def profile_agent(
        self,
        agent: BaseAgent,
        workload: Workload,
        options: ProfilingOptions = None
    ) -> ProfilingResult:
        """Profile agent performance under workload"""
        
        options = options or ProfilingOptions()
        
        # Start profiling
        profiling_session = ProfilingSession(
            agent_id=agent.agent_id,
            start_time=datetime.utcnow()
        )
        
        # CPU profiling
        if options.profile_cpu:
            cpu_stats = await self.cpu_profiler.profile(
                agent,
                workload,
                profiling_session
            )
        
        # Memory profiling
        if options.profile_memory:
            memory_stats = await self.memory_profiler.profile(
                agent,
                workload,
                profiling_session
            )
        
        # Async profiling
        if options.profile_async:
            async_stats = await self.async_profiler.profile(
                agent,
                workload,
                profiling_session
            )
        
        # I/O profiling
        if options.profile_io:
            io_stats = await self.io_profiler.profile(
                agent,
                workload,
                profiling_session
            )
        
        # Analyze results
        analysis = await self._analyze_profiling_data(profiling_session)
        
        return ProfilingResult(
            agent_id=agent.agent_id,
            profile_type='comprehensive',
            duration=profiling_session.duration,
            cpu_time=cpu_stats.total_time if options.profile_cpu else 0,
            memory_peak=memory_stats.peak_memory if options.profile_memory else 0,
            function_stats=analysis.function_stats,
            memory_allocations=analysis.memory_allocations,
            bottlenecks=analysis.bottlenecks
        )
    
    async def _analyze_profiling_data(
        self,
        session: ProfilingSession
    ) -> ProfilingAnalysis:
        """Analyze profiling data to identify issues"""
        
        analysis = ProfilingAnalysis()
        
        # Analyze CPU hotspots
        if session.cpu_data:
            hotspots = self._find_cpu_hotspots(session.cpu_data)
            analysis.bottlenecks.extend(hotspots)
        
        # Analyze memory patterns
        if session.memory_data:
            leaks = self._detect_memory_leaks(session.memory_data)
            analysis.bottlenecks.extend(leaks)
        
        # Analyze async patterns
        if session.async_data:
            async_issues = self._analyze_async_patterns(session.async_data)
            analysis.bottlenecks.extend(async_issues)
        
        return analysis

class CPUProfiler:
    """CPU profiling for agents"""
    
    async def profile(
        self,
        agent: BaseAgent,
        workload: Workload,
        session: ProfilingSession
    ) -> CPUStats:
        """Profile CPU usage"""
        
        profiler = cProfile.Profile()
        
        # Start profiling
        profiler.enable()
        
        # Execute workload
        start_time = asyncio.get_event_loop().time()
        await workload.execute(agent)
        end_time = asyncio.get_event_loop().time()
        
        # Stop profiling
        profiler.disable()
        
        # Analyze stats
        stats = pstats.Stats(profiler)
        stats.sort_stats('cumulative')
        
        # Extract function statistics
        function_stats = []
        for func, (cc, nc, tt, ct, callers) in stats.stats.items():
            function_stats.append(FunctionStat(
                name=f"{func[0]}:{func[1]}:{func[2]}",
                calls=nc,
                total_time=tt,
                cumulative_time=ct,
                average_time=tt/nc if nc > 0 else 0
            ))
        
        # Store in session
        session.cpu_data = CPUData(
            profile=profiler,
            stats=stats,
            function_stats=function_stats,
            total_time=end_time - start_time
        )
        
        return CPUStats(
            total_time=end_time - start_time,
            cpu_time=sum(f.total_time for f in function_stats),
            top_functions=sorted(
                function_stats,
                key=lambda f: f.cumulative_time,
                reverse=True
            )[:10]
        )

class MemoryProfiler:
    """Memory profiling for agents"""
    
    async def profile(
        self,
        agent: BaseAgent,
        workload: Workload,
        session: ProfilingSession
    ) -> MemoryStats:
        """Profile memory usage"""
        
        # Start memory tracking
        tracemalloc.start()
        
        # Take initial snapshot
        snapshot1 = tracemalloc.take_snapshot()
        
        # Execute workload
        await workload.execute(agent)
        
        # Take final snapshot
        snapshot2 = tracemalloc.take_snapshot()
        
        # Get peak memory
        current, peak = tracemalloc.get_traced_memory()
        
        # Stop tracking
        tracemalloc.stop()
        
        # Analyze differences
        top_stats = snapshot2.compare_to(snapshot1, 'lineno')
        
        # Extract allocation statistics
        allocations = []
        for stat in top_stats[:50]:  # Top 50 allocations
            allocations.append(MemoryAllocation(
                file=stat.traceback[0].filename,
                line=stat.traceback[0].lineno,
                size=stat.size,
                count=stat.count,
                size_diff=stat.size_diff,
                count_diff=stat.count_diff
            ))
        
        # Store in session
        session.memory_data = MemoryData(
            initial_snapshot=snapshot1,
            final_snapshot=snapshot2,
            peak_memory=peak,
            allocations=allocations
        )
        
        return MemoryStats(
            peak_memory=peak,
            current_memory=current,
            top_allocations=allocations[:10]
        )
```

#### SubTask 3.20.2: ìë™ ìµœì í™” ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 16ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/optimization/auto_optimizer.ts
export class AutoOptimizer {
  private readonly profiler: PerformanceProfiler;
  private readonly optimizer: OptimizationEngine;
  private readonly validator: OptimizationValidator;
  
  constructor() {
    this.profiler = new PerformanceProfiler();
    this.optimizer = new OptimizationEngine();
    this.validator = new OptimizationValidator();
  }
  
  async optimizeAgent(
    agent: BaseAgent,
    options: OptimizationOptions = {}
  ): Promise<OptimizationResult> {
    // Profile current performance
    const baseline = await this.profileBaseline(agent);
    
    // Identify optimization opportunities
    const opportunities = await this.identifyOpportunities(
      agent,
      baseline
    );
    
    // Apply optimizations
    const optimizations: AppliedOptimization[] = [];
    
    for (const opportunity of opportunities) {
      if (opportunity.risk <= options.maxRisk || 'medium') {
        const result = await this.applyOptimization(
          agent,
          opportunity,
          options
        );
        
        if (result.successful) {
          optimizations.push(result);
        }
      }
    }
    
    // Measure improvement
    const optimized = await this.profileBaseline(agent);
    const improvement = this.calculateImprovement(baseline, optimized);
    
    return {
      baseline,
      optimized,
      improvement,
      optimizations,
      recommendations: await this.generateRecommendations(
        agent,
        improvement
      )
    };
  }
  
  private async identifyOpportunities(
    agent: BaseAgent,
    profile: PerformanceProfile
  ): Promise<OptimizationOpportunity[]> {
    const opportunities: OptimizationOpportunity[] = [];
    
    // Check for caching opportunities
    const cacheAnalysis = await this.analyzeCachingPotential(profile);
    if (cacheAnalysis.potential > 0.3) {
      opportunities.push({
        type: 'caching',
        description: 'Add result caching for repeated operations',
        estimatedImprovement: cacheAnalysis.potential,
        risk: 'low',
        implementation: new CachingOptimization(cacheAnalysis)
      });
    }
    
    // Check for parallelization opportunities
    const parallelAnalysis = await this.analyzeParallelization(profile);
    if (parallelAnalysis.opportunities.length > 0) {
      opportunities.push({
        type: 'parallelization',
        description: 'Parallelize independent operations',
        estimatedImprovement: parallelAnalysis.speedup,
        risk: 'medium',
        implementation: new ParallelizationOptimization(parallelAnalysis)
      });
    }
    
    // Check for algorithm optimization
    const algorithmAnalysis = await this.analyzeAlgorithms(profile);
    for (const suboptimal of algorithmAnalysis.suboptimalAlgorithms) {
      opportunities.push({
        type: 'algorithm',
        description: `Optimize ${suboptimal.function} algorithm`,
        estimatedImprovement: suboptimal.improvementPotential,
        risk: 'high',
        implementation: new AlgorithmOptimization(suboptimal)
      });
    }
    
    // Check for resource optimization
    const resourceAnalysis = await this.analyzeResourceUsage(profile);
    if (resourceAnalysis.wastefulOperations.length > 0) {
      opportunities.push({
        type: 'resource',
        description: 'Optimize resource usage',
        estimatedImprovement: resourceAnalysis.savingsPotential,
        risk: 'low',
        implementation: new ResourceOptimization(resourceAnalysis)
      });
    }
    
    return opportunities;
  }
  
  private async applyOptimization(
    agent: BaseAgent,
    opportunity: OptimizationOpportunity,
    options: OptimizationOptions
  ): Promise<AppliedOptimization> {
    // Create optimization context
    const context = new OptimizationContext(agent, opportunity);
    
    // Take snapshot for rollback
    const snapshot = await agent.createSnapshot();
    
    try {
      // Apply optimization
      await opportunity.implementation.apply(context);
      
      // Validate optimization
      const validation = await this.validator.validate(
        agent,
        opportunity
      );
      
      if (!validation.passed) {
        // Rollback
        await agent.restoreSnapshot(snapshot);
        
        return {
          opportunity,
          successful: false,
          reason: validation.reason
        };
      }
      
      // Measure impact
      const impact = await this.measureImpact(
        agent,
        opportunity
      );
      
      return {
        opportunity,
        successful: true,
        impact,
        appliedAt: new Date()
      };
      
    } catch (error) {
      // Rollback on error
      await agent.restoreSnapshot(snapshot);
      
      return {
        opportunity,
        successful: false,
        reason: error.message,
        error
      };
    }
  }
}

export class CachingOptimization implements OptimizationImplementation {
  constructor(private analysis: CacheAnalysis) {}
  
  async apply(context: OptimizationContext): Promise<void> {
    const agent = context.agent;
    
    // Identify cacheable methods
    const cacheableMethods = this.analysis.cacheableMethods;
    
    for (const method of cacheableMethods) {
      // Wrap method with caching
      const original = agent[method.name];
      
      agent[method.name] = this.createCachedMethod(
        original,
        method.cacheConfig
      );
    }
    
    // Add cache management
    agent.cache = new AgentCache({
      maxSize: this.analysis.recommendedCacheSize,
      ttl: this.analysis.recommendedTTL,
      evictionPolicy: 'lru'
    });
  }
  
  private createCachedMethod(
    original: Function,
    config: CacheConfig
  ): Function {
    const cache = new Map();
    
    return async function(...args: any[]) {
      const key = config.keyGenerator
        ? config.keyGenerator(...args)
        : JSON.stringify(args);
      
      // Check cache
      if (cache.has(key)) {
        const cached = cache.get(key);
        if (Date.now() - cached.timestamp < config.ttl) {
          return cached.value;
        }
      }
      
      // Execute original
      const result = await original.apply(this, args);
      
      // Cache result
      cache.set(key, {
        value: result,
        timestamp: Date.now()
      });
      
      return result;
    };
  }
}
```

#### SubTask 3.20.3: ë¦¬ì†ŒìŠ¤ ìµœì í™”
**ë‹´ë‹¹ì**: ë°±ì—”ë“œ ê°œë°œì  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 14ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```python
# backend/src/agents/framework/optimization/resource_optimizer.py
from typing import Dict, List, Any, Optional
import asyncio
import psutil
from dataclasses import dataclass

@dataclass
class ResourceUsage:
    cpu_percent: float
    memory_mb: float
    io_read_bytes: int
    io_write_bytes: int
    network_sent_bytes: int
    network_recv_bytes: int
    thread_count: int
    file_descriptors: int

class ResourceOptimizer:
    """Optimizes agent resource usage"""
    
    def __init__(self):
        self.resource_monitor = ResourceMonitor()
        self.throttler = ResourceThrottler()
        self.pool_manager = ResourcePoolManager()
        
    async def optimize_resources(
        self,
        agent: BaseAgent,
        constraints: ResourceConstraints
    ) -> ResourceOptimizationResult:
        """Optimize agent resource usage"""
        
        # Monitor current usage
        current_usage = await self.resource_monitor.get_usage(agent)
        
        # Identify optimization areas
        optimizations = []
        
        # CPU optimization
        if current_usage.cpu_percent > constraints.max_cpu_percent:
            cpu_opt = await self._optimize_cpu(agent, current_usage, constraints)
            optimizations.append(cpu_opt)
        
        # Memory optimization
        if current_usage.memory_mb > constraints.max_memory_mb:
            mem_opt = await self._optimize_memory(agent, current_usage, constraints)
            optimizations.append(mem_opt)
        
        # I/O optimization
        if self._is_io_intensive(current_usage):
            io_opt = await self._optimize_io(agent, current_usage)
            optimizations.append(io_opt)
        
        # Thread pool optimization
        thread_opt = await self._optimize_thread_pool(agent, current_usage)
        if thread_opt:
            optimizations.append(thread_opt)
        
        # Apply optimizations
        for opt in optimizations:
            await opt.apply(agent)
        
        # Measure impact
        optimized_usage = await self.resource_monitor.get_usage(agent)
        
        return ResourceOptimizationResult(
            before=current_usage,
            after=optimized_usage,
            optimizations=optimizations,
            savings=self._calculate_savings(current_usage, optimized_usage)
        )
    
    async def _optimize_cpu(
        self,
        agent: BaseAgent,
        usage: ResourceUsage,
        constraints: ResourceConstraints
    ) -> CPUOptimization:
        """Optimize CPU usage"""
        
        optimization = CPUOptimization()
        
        # Enable CPU throttling
        if hasattr(agent, 'cpu_throttle'):
            optimization.add_action(
                'enable_throttling',
                lambda: setattr(agent, 'cpu_throttle', constraints.max_cpu_percent)
            )
        
        # Adjust computation batch size
        if hasattr(agent, 'batch_size'):
            optimal_batch = self._calculate_optimal_batch_size(usage)
            optimization.add_action(
                'adjust_batch_size',
                lambda: setattr(agent, 'batch_size', optimal_batch)
            )
        
        # Enable lazy evaluation
        optimization.add_action(
            'enable_lazy_evaluation',
            lambda: self._enable_lazy_evaluation(agent)
        )
        
        return optimization
    
    async def _optimize_memory(
        self,
        agent: BaseAgent,
        usage: ResourceUsage,
        constraints: ResourceConstraints
    ) -> MemoryOptimization:
        """Optimize memory usage"""
        
        optimization = MemoryOptimization()
        
        # Analyze memory allocations
        allocations = await self._analyze_memory_allocations(agent)
        
        # Reduce cache sizes
        if allocations.cache_memory > constraints.max_cache_mb:
            optimization.add_action(
                'reduce_cache_size',
                lambda: self._reduce_cache_sizes(agent, constraints.max_cache_mb)
            )
        
        # Enable object pooling
        if allocations.frequent_allocations:
            optimization.add_action(
                'enable_object_pooling',
                lambda: self._enable_object_pooling(agent, allocations)
            )
        
        # Optimize data structures
        if allocations.inefficient_structures:
            optimization.add_action(
                'optimize_data_structures',
                lambda: self._optimize_data_structures(agent, allocations)
            )
        
        return optimization
    
    async def _optimize_thread_pool(
        self,
        agent: BaseAgent,
        usage: ResourceUsage
    ) -> Optional[ThreadPoolOptimization]:
        """Optimize thread pool configuration"""
        
        if not hasattr(agent, 'thread_pool'):
            return None
        
        # Analyze thread utilization
        utilization = await self._analyze_thread_utilization(agent)
        
        if utilization.average < 0.3:
            # Underutilized, reduce threads
            optimal_size = max(1, int(agent.thread_pool.size * 0.7))
            
            return ThreadPoolOptimization(
                current_size=agent.thread_pool.size,
                optimal_size=optimal_size,
                reason='Low thread utilization'
            )
        
        elif utilization.average > 0.9 and utilization.queue_length > 0:
            # Overutilized, increase threads
            optimal_size = min(
                usage.thread_count * 2,
                agent.thread_pool.size + 2
            )
            
            return ThreadPoolOptimization(
                current_size=agent.thread_pool.size,
                optimal_size=optimal_size,
                reason='High thread utilization with queuing'
            )
        
        return None

class ResourcePoolManager:
    """Manages resource pools for agents"""
    
    def __init__(self):
        self.connection_pools: Dict[str, ConnectionPool] = {}
        self.object_pools: Dict[str, ObjectPool] = {}
        self.buffer_pools: Dict[str, BufferPool] = {}
        
    def create_connection_pool(
        self,
        name: str,
        factory: Callable,
        config: PoolConfig
    ) -> ConnectionPool:
        """Create a connection pool"""
        
        pool = ConnectionPool(
            name=name,
            factory=factory,
            min_size=config.min_size,
            max_size=config.max_size,
            max_idle_time=config.max_idle_time,
            validation_interval=config.validation_interval
        )
        
        self.connection_pools[name] = pool
        return pool
    
    def create_object_pool(
        self,
        cls: Type,
        config: PoolConfig
    ) -> ObjectPool:
        """Create an object pool"""
        
        pool = ObjectPool(
            object_class=cls,
            min_size=config.min_size,
            max_size=config.max_size,
            reset_on_return=config.reset_on_return
        )
        
        key = f"{cls.__module__}.{cls.__name__}"
        self.object_pools[key] = pool
        
        return pool
```

#### SubTask 3.20.4: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ
**ë‹´ë‹¹ì**: ì„±ëŠ¥ ì—”ì§€ë‹ˆì–´  
**ì˜ˆìƒ ì†Œìš”ì‹œê°„**: 12ì‹œê°„

**ì‘ì—… ë‚´ìš©**:
```typescript
// backend/src/agents/framework/optimization/benchmark_system.ts
export class BenchmarkSystem {
  private readonly benchmarks: Map<string, Benchmark> = new Map();
  private readonly runner: BenchmarkRunner;
  private readonly reporter: BenchmarkReporter;
  
  constructor() {
    this.runner = new BenchmarkRunner();
    this.reporter = new BenchmarkReporter();
    
    // Register standard benchmarks
    this.registerStandardBenchmarks();
  }
  
  private registerStandardBenchmarks(): void {
    // Throughput benchmark
    this.register(new ThroughputBenchmark({
      name: 'agent_throughput',
      description: 'Measures agent request processing throughput',
      workloads: [
        { name: 'light', requestsPerSecond: 10 },
        { name: 'medium', requestsPerSecond: 100 },
        { name: 'heavy', requestsPerSecond: 1000 }
      ]
    }));
    
    // Latency benchmark
    this.register(new LatencyBenchmark({
      name: 'agent_latency',
      description: 'Measures agent response latency',
      percentiles: [50, 95, 99, 99.9]
    }));
    
    // Memory benchmark
    this.register(new MemoryBenchmark({
      name: 'agent_memory',
      description: 'Measures agent memory usage patterns',
      scenarios: ['idle', 'normal_load', 'peak_load']
    }));
    
    // Concurrency benchmark
    this.register(new ConcurrencyBenchmark({
      name: 'agent_concurrency',
      description: 'Tests agent performance under concurrent load',
      concurrencyLevels: [1, 10, 50, 100, 500]
    }));
  }
  
  async runBenchmark(
    agentType: string,
    benchmarkName: string,
    options: BenchmarkOptions = {}
  ): Promise<BenchmarkResult> {
    const benchmark = this.benchmarks.get(benchmarkName);
    if (!benchmark) {
      throw new Error(`Benchmark ${benchmarkName} not found`);
    }
    
    // Create agent instance for benchmarking
    const agent = await this.createBenchmarkAgent(agentType);
    
    // Warm up
    if (options.warmup) {
      await this.warmup(agent, benchmark, options.warmupDuration);
    }
    
    // Run benchmark
    const result = await this.runner.run(
      agent,
      benchmark,
      options
    );
    
    // Generate report
    const report = await this.reporter.generate(result);
    
    // Store results
    await this.storeResults(agentType, benchmarkName, result);
    
    return result;
  }
  
  async compareBenchmarks(
    results: BenchmarkResult[]
  ): Promise<ComparisonReport> {
    const comparison = new ComparisonReport();
    
    // Group by benchmark type
    const grouped = this.groupByBenchmark(results);
    
    for (const [benchmarkName, benchmarkResults] of grouped) {
      const analysis = await this.analyzeBenchmarkGroup(
        benchmarkName,
        benchmarkResults
      );
      
      comparison.addAnalysis(benchmarkName, analysis);
    }
    
    // Generate visualizations
    comparison.charts = await this.generateCharts(comparison);
    return comparison;
  }
  
  private async generateCharts(
    comparison: ComparisonReport
  ): Promise<Chart[]> {
    const charts: Chart[] = [];
    
    // Throughput comparison chart
    charts.push(new BarChart({
      title: 'Throughput Comparison',
      data: comparison.getThroughputData(),
      xAxis: 'Agent Type',
      yAxis: 'Requests/Second'
    }));
    
    // Latency distribution chart
    charts.push(new BoxPlot({
      title: 'Latency Distribution',
      data: comparison.getLatencyData(),
      xAxis: 'Agent Type',
      yAxis: 'Latency (ms)'
    }));
    
    // Memory usage over time
    charts.push(new LineChart({
      title: 'Memory Usage Over Time',
      data: comparison.getMemoryTimeSeriesData(),
      xAxis: 'Time (seconds)',
      yAxis: 'Memory (MB)'
    }));
    
    // Concurrency scaling
    charts.push(new LineChart({
      title: 'Concurrency Scaling',
      data: comparison.getConcurrencyScalingData(),
      xAxis: 'Concurrent Requests',
      yAxis: 'Throughput (req/s)',
      series: comparison.getAgentTypes()
    }));
    
    return charts;
  }
}

export class BenchmarkRunner {
  async run(
    agent: BaseAgent,
    benchmark: Benchmark,
    options: BenchmarkOptions
  ): Promise<BenchmarkResult> {
    const result = new BenchmarkResult({
      benchmarkName: benchmark.name,
      agentId: agent.agentId,
      agentType: agent.agentType,
      startTime: new Date(),
      options
    });
    
    try {
      // Prepare benchmark environment
      await this.prepareBenchmarkEnvironment(agent, benchmark);
      
      // Run benchmark iterations
      for (let i = 0; i < options.iterations || 1; i++) {
        const iteration = await this.runIteration(
          agent,
          benchmark,
          i,
          options
        );
        
        result.addIteration(iteration);
        
        // Cool down between iterations
        if (i < (options.iterations || 1) - 1) {
          await this.coolDown(options.coolDownDuration || 1000);
        }
      }
      
      // Calculate aggregated metrics
      result.aggregate();
      
    } catch (error) {
      result.error = error;
    } finally {
      // Cleanup
      await this.cleanup(agent, benchmark);
    }
    
    result.endTime = new Date();
    return result;
  }
  
  private async runIteration(
    agent: BaseAgent,
    benchmark: Benchmark,
    iteration: number,
    options: BenchmarkOptions
  ): Promise<BenchmarkIteration> {
    const iterationResult = new BenchmarkIteration(iteration);
    
    // Start monitoring
    const monitor = await this.startMonitoring(agent);
    
    // Execute benchmark workload
    const workloadResult = await benchmark.execute(agent, {
      duration: options.duration || 60000,
      ...benchmark.getIterationConfig(iteration)
    });
    
    // Stop monitoring
    const monitoringData = await monitor.stop();
    
    // Collect metrics
    iterationResult.metrics = {
      ...workloadResult.metrics,
      ...monitoringData
    };
    
    return iterationResult;
  }
}

// Benchmark definitions
export class ThroughputBenchmark extends Benchmark {
  async execute(
    agent: BaseAgent,
    config: WorkloadConfig
  ): Promise<WorkloadResult> {
    const result = new WorkloadResult();
    const startTime = Date.now();
    let requestCount = 0;
    let errorCount = 0;
    
    // Generate load
    const loadGenerator = new LoadGenerator({
      targetRPS: config.requestsPerSecond,
      duration: config.duration
    });
    
    await loadGenerator.generate(async (request) => {
      try {
        const response = await agent.execute(request);
        requestCount++;
        result.recordResponse(response);
      } catch (error) {
        errorCount++;
        result.recordError(error);
      }
    });
    
    const duration = Date.now() - startTime;
    
    result.metrics = {
      throughput: (requestCount * 1000) / duration,
      errorRate: errorCount / (requestCount + errorCount),
      totalRequests: requestCount,
      totalErrors: errorCount,
      duration
    };
    
    return result;
  }
}

export class LatencyBenchmark extends Benchmark {
  async execute(
    agent: BaseAgent,
    config: WorkloadConfig
  ): Promise<WorkloadResult> {
    const result = new WorkloadResult();
    const latencies: number[] = [];
    
    // Generate requests with controlled rate
    const requestGenerator = new RequestGenerator({
      rate: config.requestRate || 10,
      duration: config.duration
    });
    
    await requestGenerator.generate(async (request) => {
      const startTime = process.hrtime.bigint();
      
      try {
        await agent.execute(request);
        const endTime = process.hrtime.bigint();
        const latency = Number(endTime - startTime) / 1_000_000; // Convert to ms
        
        latencies.push(latency);
        result.recordLatency(latency);
      } catch (error) {
        result.recordError(error);
      }
    });
    
    // Calculate percentiles
    latencies.sort((a, b) => a - b);
    
    result.metrics = {
      count: latencies.length,
      min: latencies[0],
      max: latencies[latencies.length - 1],
      mean: latencies.reduce((a, b) => a + b, 0) / latencies.length,
      p50: this.percentile(latencies, 50),
      p95: this.percentile(latencies, 95),
      p99: this.percentile(latencies, 99),
      p999: this.percentile(latencies, 99.9)
    };
    
    return result;
  }
  
  private percentile(sorted: number[], p: number): number {
    const index = Math.ceil((sorted.length * p) / 100) - 1;
    return sorted[Math.max(0, index)];
  }
}
```

---

## ğŸ“ Phase 3 ì™„ë£Œ ìš”ì•½

### ì™„ë£Œëœ ì‘ì—… ê°œìš”
Phase 3ì—ì„œëŠ” T-Developerì˜ 9ê°œ í•µì‹¬ ì—ì´ì „íŠ¸ë¥¼ ìœ„í•œ í¬ê´„ì ì¸ í”„ë ˆì„ì›Œí¬ë¥¼ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.

### ì£¼ìš” êµ¬í˜„ ë‚´ìš©

#### 1. **ì—ì´ì „íŠ¸ ê¸°ë³¸ í”„ë ˆì„ì›Œí¬ (Tasks 3.1-3.5)**
- âœ… ì¶”ìƒ ë² ì´ìŠ¤ ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ êµ¬í˜„
- âœ… ì—ì´ì „íŠ¸ ìƒëª…ì£¼ê¸° ê´€ë¦¬ ì‹œìŠ¤í…œ
- âœ… ìƒíƒœ ê´€ë¦¬ ë° ë™ê¸°í™” ë©”ì»¤ë‹ˆì¦˜
- âœ… ì„¤ì • ê´€ë¦¬ ë° ë™ì  ì—…ë°ì´íŠ¸
- âœ… í¬ê´„ì ì¸ ì—ëŸ¬ ì²˜ë¦¬ í”„ë ˆì„ì›Œí¬

#### 2. **ì—ì´ì „íŠ¸ í†µì‹  ë° ë©”ì‹œì§• (Tasks 3.6-3.10)**
- âœ… ì—ì´ì „íŠ¸ ê°„ í†µì‹  í”„ë¡œí† ì½œ ì •ì˜
- âœ… ë¶„ì‚° ë©”ì‹œì§€ í ì‹œìŠ¤í…œ
- âœ… ì´ë²¤íŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜
- âœ… ë™ê¸°/ë¹„ë™ê¸° í†µì‹  ë ˆì´ì–´
- âœ… ê³µìœ  ë©”ëª¨ë¦¬ ë° ë°ì´í„° ë™ê¸°í™”

#### 3. **ì—ì´ì „íŠ¸ í˜‘ì—… ë° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (Tasks 3.11-3.15)**
- âœ… ì›Œí¬í”Œë¡œìš° ì—”ì§„ êµ¬ì¶•
- âœ… ì—ì´ì „íŠ¸ ì²´ì¸ ê´€ë¦¬ ì‹œìŠ¤í…œ
- âœ… ë³‘ë ¬ ì²˜ë¦¬ ë° ì¡°ì • ë©”ì»¤ë‹ˆì¦˜
- âœ… ì˜ì¡´ì„± ê´€ë¦¬ ë° í•´ê²°
- âœ… í˜‘ì—… íŒ¨í„´ ë¼ì´ë¸ŒëŸ¬ë¦¬

#### 4. **ì—ì´ì „íŠ¸ ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§ (Tasks 3.16-3.20)**
- âœ… ì—ì´ì „íŠ¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‹œìŠ¤í…œ
- âœ… ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- âœ… êµ¬ì¡°í™”ëœ ë¡œê¹… ë° ë¶„ì‚° ì¶”ì 
- âœ… ë²„ì „ ê´€ë¦¬ ë° ë°°í¬ ì‹œìŠ¤í…œ
- âœ… ìë™ ì„±ëŠ¥ ìµœì í™”

### ê¸°ìˆ ì  ì„±ê³¼

1. **í™•ì¥ì„±**
   - ìˆ˜ì²œ ê°œì˜ ì—ì´ì „íŠ¸ ë™ì‹œ ì‹¤í–‰ ì§€ì›
   - ìë™ ìŠ¤ì¼€ì¼ë§ ë° ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
   - ë¶„ì‚° ì‹œìŠ¤í…œ ì§€ì›

2. **ì„±ëŠ¥**
   - 3Î¼s ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤í™” (Agno í†µí•©)
   - íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš© (ì—ì´ì „íŠ¸ë‹¹ ~6.5KB)
   - ì§€ëŠ¥ì ì¸ ìºì‹± ë° ìµœì í™”

3. **ì•ˆì •ì„±**
   - í¬ê´„ì ì¸ ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬
   - ìë™ ë¡¤ë°± ë©”ì»¤ë‹ˆì¦˜
   - í—¬ìŠ¤ ì²´í¬ ë° ìê°€ ì¹˜ìœ 

4. **ê°œë°œì ê²½í—˜**
   - ëª…í™•í•œ API ë° ì¸í„°í˜ì´ìŠ¤
   - í’ë¶€í•œ ë¬¸ì„œí™”
   - ë””ë²„ê¹… ë° í”„ë¡œíŒŒì¼ë§ ë„êµ¬

### ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„

Phase 3ì˜ í”„ë ˆì„ì›Œí¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Phase 4ì—ì„œëŠ” 9ê°œì˜ í•µì‹¬ ì—ì´ì „íŠ¸ë¥¼ ì‹¤ì œë¡œ êµ¬í˜„í•  ì˜ˆì •ì…ë‹ˆë‹¤:

9ê°œ í•µì‹¬ ì—ì´ì „íŠ¸ (ê° ì—ì´ì „íŠ¸ë‹¹ 10 Tasks)

1. NL Input Agent (Tasks 4.1-4.10): ìì—°ì–´ ì…ë ¥ ì²˜ë¦¬
2. UI Selection Agent (Tasks 4.11-4.20): UI í”„ë ˆì„ì›Œí¬ ì„ íƒ
3. Parser Agent (Tasks 4.21-4.30): ìš”êµ¬ì‚¬í•­ íŒŒì‹±
4. Component Decision Agent (Tasks 4.31-4.40): ì»´í¬ë„ŒíŠ¸ ê²°ì •
5. Match Rate Agent (Tasks 4.41-4.50): ë§¤ì¹­ë¥  ê³„ì‚°
6. Search Agent (Tasks 4.51-4.60): ì»´í¬ë„ŒíŠ¸ ê²€ìƒ‰
7. Generation Agent (Tasks 4.61-4.70): ì½”ë“œ ìƒì„±
8. Assembly Agent (Tasks 4.71-4.80): ì„œë¹„ìŠ¤ ì¡°ë¦½
9. Download Agent (Tasks 4.81-4.90): íŒ¨í‚¤ì§• ë° ë‹¤ìš´ë¡œë“œ

### í”„ë¡œì íŠ¸ í†µê³„

- **ì´ ì‘ì—… ì™„ë£Œ**: 20 Tasks Ã— 4 SubTasks = 80ê°œ ì‘ì—… ë‹¨ìœ„
- **ì½”ë“œ ë¼ì¸ ìˆ˜**: ì•½ 15,000+ ë¼ì¸
- **í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€**: ëª©í‘œ 80% ì´ìƒ
- **ë¬¸ì„œí™”**: ê° ì»´í¬ë„ŒíŠ¸ë³„ ìƒì„¸ ë¬¸ì„œ ì™„ì„±
